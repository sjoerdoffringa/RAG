[
    {
        "vector_id": 0,
        "text": "AI is transforming industries."
    },
    {
        "vector_id": 1,
        "text": "Machine learning is evolving."
    },
    {
        "vector_id": 2,
        "text": "Natural language processing is powerful."
    },
    {
        "vector_id": 3,
        "text": "AI is transforming industries."
    },
    {
        "vector_id": 4,
        "text": "Machine learning is evolving."
    },
    {
        "vector_id": 5,
        "text": "Natural language processing is powerful."
    },
    {
        "vector_id": 6,
        "text": "Computer vision is advancing."
    },
    {
        "vector_id": 7,
        "text": "AI is changing the world."
    },
    {
        "vector_id": 8,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 9,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 10,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 11,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 12,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 13,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 14,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 15,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 16,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 17,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 18,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 19,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 20,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 21,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 22,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 23,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 24,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 25,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 26,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 27,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 28,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 29,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 30,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 31,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 32,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 33,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 34,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 35,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 36,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 37,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 38,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 39,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 40,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 41,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 42,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 43,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 44,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 45,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 46,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 47,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 48,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 49,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 50,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 51,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 52,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 53,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 54,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 55,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 56,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 57,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 58,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 59,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 60,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 61,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 62,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 63,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 64,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 65,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 66,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 67,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 68,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 69,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 70,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 71,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 72,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 73,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 74,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 75,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 76,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 77,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 78,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 79,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 80,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 81,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 82,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 83,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 84,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 85,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 86,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 87,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 88,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 89,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 90,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 91,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 92,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 93,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 94,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 95,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 96,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 97,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 98,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 99,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 100,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 101,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 102,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 103,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 104,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 105,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 106,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 107,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 108,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 109,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 110,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 111,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 112,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 113,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 114,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 115,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 116,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 117,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 118,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 119,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 120,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 121,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 122,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 123,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 124,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 125,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 126,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 127,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 128,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 129,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 130,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 131,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 132,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 133,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 134,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 135,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 136,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 137,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 138,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 139,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 140,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 141,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 142,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 143,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 144,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 145,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 146,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 147,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 148,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 149,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 150,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 151,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 152,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 153,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 154,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 155,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 156,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 157,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 158,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 159,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 160,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 161,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 162,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 163,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 164,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 165,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 166,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 167,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 168,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 169,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 170,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 171,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 172,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 173,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 174,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 175,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 176,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 177,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 178,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 179,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 180,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 181,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 182,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 183,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 184,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 185,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 186,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 187,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 188,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 189,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 190,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 191,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 192,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 193,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 194,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 195,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 196,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 197,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 198,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 199,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 200,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 201,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 202,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 203,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 204,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 205,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 206,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 207,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 208,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 209,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 210,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 211,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 212,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 213,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 214,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 215,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 216,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 217,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 218,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 219,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 220,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 221,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 222,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 223,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 224,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 225,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 226,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 227,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 228,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 229,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 230,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 231,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 232,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 233,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 234,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 235,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 236,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 237,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 238,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 239,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 240,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 241,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 242,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 243,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 244,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 245,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 246,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 247,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 248,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 249,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 250,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 251,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 252,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 253,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 254,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 255,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 256,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 257,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 258,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 259,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 260,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 261,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 262,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 263,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 264,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 265,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 266,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 267,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 268,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 269,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 270,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 271,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 272,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 273,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 274,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 275,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 276,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 277,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 278,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 279,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 280,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 281,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 282,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 283,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 284,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 285,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 286,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 287,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 288,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 289,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 290,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 291,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 292,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 293,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 294,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 295,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 296,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 297,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 298,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 299,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 300,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 301,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 302,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 303,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 304,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 305,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 306,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 307,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 308,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 309,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 310,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 311,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 312,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 313,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 314,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 315,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 316,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 317,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 318,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 319,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 320,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 321,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 322,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 323,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 324,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 325,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 326,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 327,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 328,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 329,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 330,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 331,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 332,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 333,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 334,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 335,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 336,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 337,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 338,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 339,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 340,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 341,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 342,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 343,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 344,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 345,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 346,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 347,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 348,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 349,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 350,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 351,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 352,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 353,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 354,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 355,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 356,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 357,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 358,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 359,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 360,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 361,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 362,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 363,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 364,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 365,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 366,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 367,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 368,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 369,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 370,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 371,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 372,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 373,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 374,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 375,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 376,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 377,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 378,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 379,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 380,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 381,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 382,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 383,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 384,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 385,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 386,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 387,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 388,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 389,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 390,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 391,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 392,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 393,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 394,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 395,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 396,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 397,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 398,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 399,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 400,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 401,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 402,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 403,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 404,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 405,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 406,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 407,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 408,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 409,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 410,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 411,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 412,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 413,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 414,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 415,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 416,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 417,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 418,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 419,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 420,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 421,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 422,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 423,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 424,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 425,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 426,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 427,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 428,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 429,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 430,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 431,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 432,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 433,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 434,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 435,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 436,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 437,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 438,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 439,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 440,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 441,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 442,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 443,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 444,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 445,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 446,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 447,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 448,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 449,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 450,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 451,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 452,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 453,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 454,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 455,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 456,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 457,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 458,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 459,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 460,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 461,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 462,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 463,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 464,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 465,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 466,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 467,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 468,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 469,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 470,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 471,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 472,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 473,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 474,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 475,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 476,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 477,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 478,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 479,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 480,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 481,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 482,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 483,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 484,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 485,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 486,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 487,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 488,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 489,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 490,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 491,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 492,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 493,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 494,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 495,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 496,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 497,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 498,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 499,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 500,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 501,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 502,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 503,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 504,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 505,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 506,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 507,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 508,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 509,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 510,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 511,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 512,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 513,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 514,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 515,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 516,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 517,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 518,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 519,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 520,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 521,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 522,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 523,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 524,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 525,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 526,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 527,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 528,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 529,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 530,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 531,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 532,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 533,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 534,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 535,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 536,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 537,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 538,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 539,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 540,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 541,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 542,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 543,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 544,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 545,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 546,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 547,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 548,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 549,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 550,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 551,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 552,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 553,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 554,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 555,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 556,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 557,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 558,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 559,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 560,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 561,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 562,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 563,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 564,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 565,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 566,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 567,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 568,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 569,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 570,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 571,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 572,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 573,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 574,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 575,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 576,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 577,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 578,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 579,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 580,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 581,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 582,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 583,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 584,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 585,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 586,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 587,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 588,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 589,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 590,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 591,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 592,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 593,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 594,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 595,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 596,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 597,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 598,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 599,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 600,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 601,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 602,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 603,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 604,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 605,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 606,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 607,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 608,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 609,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 610,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 611,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 612,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 613,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 614,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 615,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 616,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 617,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 618,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 619,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 620,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 621,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 622,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 623,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 624,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 625,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 626,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 627,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 628,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 629,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 630,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 631,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 632,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 633,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 634,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 635,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 636,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 637,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 638,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 639,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 640,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 641,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 642,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 643,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 644,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 645,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 646,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 647,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 648,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 649,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 650,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 651,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 652,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 653,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 654,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 655,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 656,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 657,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 658,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 659,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 660,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 661,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 662,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 663,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 664,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 665,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 666,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 667,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 668,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 669,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 670,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 671,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 672,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 673,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 674,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 675,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 676,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 677,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 678,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 679,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 680,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 681,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 682,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 683,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 684,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 685,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 686,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 687,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 688,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 689,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 690,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 691,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 692,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 693,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 694,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 695,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 696,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 697,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 698,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 699,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 700,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 701,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 702,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 703,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 704,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 705,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 706,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 707,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 708,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 709,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 710,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 711,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 712,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 713,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 714,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 715,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 716,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 717,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 718,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 719,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 720,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 721,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 722,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 723,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 724,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 725,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 726,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 727,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 728,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 729,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 730,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 731,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 732,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 733,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 734,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 735,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 736,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 737,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 738,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 739,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 740,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 741,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 742,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 743,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 744,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 745,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 746,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 747,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 748,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 749,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 750,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 751,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 752,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 753,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 754,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 755,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 756,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 757,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 758,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 759,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 760,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 761,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 762,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 763,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 764,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 765,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 766,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 767,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 768,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 769,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 770,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 771,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 772,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 773,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 774,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 775,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 776,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 777,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 778,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 779,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 780,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 781,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 782,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 783,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 784,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 785,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 786,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 787,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 788,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 789,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 790,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 791,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 792,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 793,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 794,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 795,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 796,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 797,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 798,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 799,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 800,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 801,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 802,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 803,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 804,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 805,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 806,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 807,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 808,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 809,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 810,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 811,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 812,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 813,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 814,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 815,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 816,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 817,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 818,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 819,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 820,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 821,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 822,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 823,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 824,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 825,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 826,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 827,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 828,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 829,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 830,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 831,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 832,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 833,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 834,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 835,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 836,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 837,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 838,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 839,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 840,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 841,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 842,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 843,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 844,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 845,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 846,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 847,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 848,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 849,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 850,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 851,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 852,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 853,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 854,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 855,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 856,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 857,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 858,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 859,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 860,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 861,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 862,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 863,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 864,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 865,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 866,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 867,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 868,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 869,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 870,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 871,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 872,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 873,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 874,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 875,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 876,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 877,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 878,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 879,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 880,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 881,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 882,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 883,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 884,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 885,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 886,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 887,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 888,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 889,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 890,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 891,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 892,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 893,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 894,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 895,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 896,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 897,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 898,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 899,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 900,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 901,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 902,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 903,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 904,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 905,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 906,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 907,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 908,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 909,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 910,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 911,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 912,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 913,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 914,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 915,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 916,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 917,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 918,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 919,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 920,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 921,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 922,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 923,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 924,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 925,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 926,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 927,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 928,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 929,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 930,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 931,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 932,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 933,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 934,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 935,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 936,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 937,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 938,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 939,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 940,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 941,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 942,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 943,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 944,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 945,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 946,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 947,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 948,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 949,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 950,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 951,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 952,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 953,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 954,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 955,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 956,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 957,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 958,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 959,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 960,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 961,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 962,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 963,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 964,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 965,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 966,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 967,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 968,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 969,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 970,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 971,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 972,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 973,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 974,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 975,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 976,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 977,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 978,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 979,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 980,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 981,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 982,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 983,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 984,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 985,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 986,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 987,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 988,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 989,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 990,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 991,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 992,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 993,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 994,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 995,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 996,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 997,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 998,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 999,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 1000,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 1001,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 1002,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 1003,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 1004,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 1005,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 1006,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 1007,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 1008,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 1009,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 1010,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 1011,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 1012,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 1013,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 1014,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 1015,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 1016,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 1017,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 1018,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 1019,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 1020,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 1021,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 1022,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 1023,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 1024,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 1025,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 1026,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 1027,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 1028,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 1029,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 1030,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 1031,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 1032,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 1033,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 1034,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 1035,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 1036,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 1037,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 1038,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 1039,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 1040,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 1041,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 1042,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 1043,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 1044,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 1045,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 1046,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 1047,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 1048,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 1049,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 1050,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 1051,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 1052,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 1053,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 1054,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 1055,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 1056,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 1057,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 1058,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 1059,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 1060,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 1061,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 1062,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 1063,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 1064,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 1065,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 1066,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 1067,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 1068,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 1069,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 1070,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 1071,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 1072,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 1073,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 1074,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 1075,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 1076,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 1077,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 1078,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 1079,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 1080,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 1081,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 1082,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 1083,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 1084,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 1085,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 1086,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 1087,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 1088,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 1089,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 1090,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 1091,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 1092,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 1093,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 1094,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 1095,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 1096,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 1097,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 1098,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 1099,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 1100,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 1101,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 1102,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 1103,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 1104,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 1105,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 1106,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 1107,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 1108,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 1109,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 1110,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 1111,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 1112,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 1113,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 1114,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 1115,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 1116,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 1117,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 1118,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 1119,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 1120,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 1121,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 1122,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 1123,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 1124,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 1125,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 1126,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 1127,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 1128,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 1129,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 1130,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 1131,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 1132,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 1133,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 1134,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 1135,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 1136,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 1137,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 1138,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 1139,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 1140,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 1141,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 1142,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 1143,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 1144,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 1145,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 1146,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 1147,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 1148,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 1149,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 1150,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 1151,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 1152,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 1153,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 1154,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 1155,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 1156,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 1157,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 1158,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 1159,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 1160,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 1161,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 1162,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 1163,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 1164,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 1165,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 1166,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 1167,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 1168,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 1169,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 1170,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 1171,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 1172,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 1173,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 1174,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 1175,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 1176,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 1177,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 1178,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 1179,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 1180,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 1181,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 1182,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 1183,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 1184,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 1185,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 1186,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 1187,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 1188,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 1189,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 1190,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 1191,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 1192,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 1193,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 1194,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 1195,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 1196,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 1197,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 1198,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 1199,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 1200,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 1201,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 1202,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 1203,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 1204,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 1205,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 1206,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 1207,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 1208,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 1209,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 1210,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 1211,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 1212,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 1213,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 1214,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 1215,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 1216,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 1217,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 1218,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 1219,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 1220,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 1221,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 1222,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 1223,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 1224,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 1225,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 1226,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 1227,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 1228,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 1229,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 1230,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 1231,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 1232,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 1233,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 1234,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 1235,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 1236,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 1237,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 1238,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 1239,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 1240,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 1241,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 1242,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 1243,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 1244,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 1245,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 1246,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 1247,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 1248,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 1249,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 1250,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 1251,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 1252,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 1253,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 1254,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 1255,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 1256,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 1257,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 1258,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 1259,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 1260,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 1261,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 1262,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 1263,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 1264,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 1265,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 1266,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 1267,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 1268,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 1269,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 1270,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 1271,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 1272,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 1273,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 1274,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 1275,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 1276,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 1277,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 1278,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 1279,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 1280,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 1281,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 1282,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 1283,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 1284,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 1285,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 1286,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 1287,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 1288,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 1289,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 1290,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 1291,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 1292,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 1293,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 1294,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 1295,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 1296,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 1297,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 1298,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 1299,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 1300,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 1301,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 1302,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 1303,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 1304,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 1305,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 1306,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 1307,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 1308,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 1309,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 1310,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 1311,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 1312,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 1313,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 1314,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 1315,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 1316,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 1317,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 1318,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 1319,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 1320,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 1321,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 1322,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 1323,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 1324,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 1325,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 1326,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 1327,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 1328,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 1329,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 1330,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 1331,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 1332,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 1333,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 1334,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 1335,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 1336,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 1337,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 1338,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 1339,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 1340,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 1341,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 1342,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 1343,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 1344,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 1345,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 1346,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 1347,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 1348,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 1349,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 1350,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 1351,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 1352,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 1353,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 1354,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 1355,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 1356,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 1357,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 1358,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 1359,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 1360,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 1361,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 1362,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 1363,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 1364,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 1365,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 1366,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 1367,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 1368,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 1369,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 1370,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 1371,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 1372,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 1373,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 1374,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 1375,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 1376,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 1377,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 1378,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 1379,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 1380,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 1381,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 1382,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 1383,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 1384,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 1385,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 1386,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 1387,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 1388,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 1389,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 1390,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 1391,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 1392,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 1393,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 1394,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 1395,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 1396,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 1397,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 1398,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 1399,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 1400,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 1401,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 1402,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 1403,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 1404,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 1405,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 1406,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 1407,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 1408,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 1409,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 1410,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 1411,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 1412,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 1413,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 1414,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 1415,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 1416,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 1417,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 1418,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 1419,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 1420,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 1421,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 1422,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 1423,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 1424,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 1425,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 1426,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 1427,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 1428,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 1429,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 1430,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 1431,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 1432,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 1433,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 1434,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 1435,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 1436,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 1437,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 1438,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 1439,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 1440,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 1441,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 1442,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 1443,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 1444,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 1445,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 1446,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 1447,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 1448,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 1449,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 1450,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 1451,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 1452,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 1453,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 1454,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 1455,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 1456,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 1457,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 1458,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 1459,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 1460,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 1461,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 1462,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 1463,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 1464,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 1465,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 1466,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 1467,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 1468,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 1469,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 1470,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 1471,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 1472,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 1473,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 1474,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 1475,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 1476,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 1477,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 1478,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 1479,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 1480,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 1481,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 1482,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 1483,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 1484,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 1485,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 1486,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 1487,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 1488,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 1489,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 1490,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 1491,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 1492,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 1493,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 1494,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 1495,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 1496,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 1497,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 1498,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 1499,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 1500,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 1501,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 1502,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 1503,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 1504,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 1505,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 1506,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 1507,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 1508,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 1509,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 1510,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 1511,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 1512,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 1513,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 1514,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 1515,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 1516,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 1517,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 1518,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 1519,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 1520,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 1521,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 1522,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 1523,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 1524,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 1525,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 1526,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 1527,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 1528,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 1529,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 1530,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 1531,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 1532,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 1533,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 1534,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 1535,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 1536,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 1537,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 1538,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 1539,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 1540,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 1541,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 1542,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 1543,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 1544,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 1545,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 1546,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 1547,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 1548,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 1549,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 1550,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 1551,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 1552,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 1553,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 1554,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 1555,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 1556,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 1557,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 1558,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 1559,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 1560,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 1561,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 1562,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 1563,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 1564,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 1565,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 1566,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 1567,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 1568,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 1569,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 1570,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 1571,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 1572,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 1573,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 1574,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 1575,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 1576,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 1577,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 1578,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 1579,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 1580,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 1581,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 1582,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 1583,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 1584,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 1585,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 1586,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 1587,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 1588,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 1589,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 1590,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 1591,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 1592,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 1593,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 1594,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 1595,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 1596,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 1597,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 1598,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 1599,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 1600,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 1601,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 1602,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 1603,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 1604,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 1605,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 1606,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 1607,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 1608,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 1609,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 1610,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 1611,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 1612,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 1613,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 1614,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 1615,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 1616,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 1617,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 1618,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 1619,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 1620,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 1621,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 1622,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 1623,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 1624,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 1625,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 1626,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 1627,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 1628,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 1629,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 1630,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 1631,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 1632,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 1633,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 1634,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 1635,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 1636,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 1637,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 1638,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 1639,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 1640,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 1641,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 1642,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 1643,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 1644,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 1645,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 1646,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 1647,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 1648,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 1649,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 1650,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 1651,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 1652,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 1653,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 1654,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 1655,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 1656,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 1657,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 1658,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 1659,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 1660,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 1661,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 1662,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 1663,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 1664,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 1665,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 1666,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 1667,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 1668,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 1669,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 1670,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 1671,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 1672,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 1673,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 1674,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 1675,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 1676,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 1677,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 1678,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 1679,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 1680,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 1681,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 1682,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 1683,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 1684,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 1685,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 1686,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 1687,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 1688,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 1689,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 1690,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 1691,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 1692,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 1693,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 1694,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 1695,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 1696,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 1697,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 1698,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 1699,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 1700,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 1701,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 1702,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 1703,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 1704,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 1705,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 1706,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 1707,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 1708,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 1709,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 1710,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 1711,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 1712,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 1713,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 1714,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 1715,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 1716,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 1717,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 1718,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 1719,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 1720,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 1721,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 1722,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 1723,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 1724,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 1725,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 1726,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 1727,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 1728,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 1729,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 1730,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 1731,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 1732,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 1733,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 1734,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 1735,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 1736,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 1737,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 1738,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 1739,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 1740,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 1741,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 1742,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 1743,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 1744,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 1745,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 1746,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 1747,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 1748,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 1749,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 1750,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 1751,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 1752,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 1753,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 1754,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 1755,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 1756,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 1757,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 1758,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 1759,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 1760,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 1761,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 1762,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 1763,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 1764,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 1765,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 1766,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 1767,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 1768,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 1769,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 1770,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 1771,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 1772,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 1773,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 1774,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 1775,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 1776,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 1777,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 1778,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 1779,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 1780,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 1781,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 1782,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 1783,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 1784,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 1785,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 1786,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 1787,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 1788,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 1789,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 1790,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 1791,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 1792,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 1793,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 1794,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 1795,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 1796,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 1797,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 1798,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 1799,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 1800,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 1801,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 1802,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 1803,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 1804,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 1805,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 1806,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 1807,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 1808,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 1809,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 1810,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 1811,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 1812,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 1813,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 1814,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 1815,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 1816,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 1817,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 1818,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 1819,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 1820,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 1821,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 1822,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 1823,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 1824,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 1825,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 1826,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 1827,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 1828,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 1829,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 1830,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 1831,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 1832,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 1833,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 1834,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 1835,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 1836,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 1837,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 1838,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 1839,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 1840,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 1841,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 1842,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 1843,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 1844,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 1845,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 1846,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 1847,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 1848,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 1849,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 1850,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 1851,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 1852,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 1853,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 1854,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 1855,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 1856,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 1857,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 1858,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 1859,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 1860,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 1861,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 1862,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 1863,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 1864,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 1865,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 1866,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 1867,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 1868,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 1869,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 1870,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 1871,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 1872,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 1873,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 1874,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 1875,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 1876,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 1877,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 1878,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 1879,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 1880,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 1881,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 1882,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 1883,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 1884,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 1885,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 1886,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 1887,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 1888,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 1889,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 1890,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 1891,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 1892,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 1893,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 1894,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 1895,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 1896,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 1897,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 1898,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 1899,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 1900,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 1901,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 1902,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 1903,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 1904,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 1905,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 1906,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 1907,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 1908,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 1909,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 1910,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 1911,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 1912,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 1913,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 1914,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 1915,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 1916,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 1917,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 1918,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 1919,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 1920,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 1921,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 1922,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 1923,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 1924,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 1925,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 1926,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 1927,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 1928,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 1929,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 1930,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 1931,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 1932,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 1933,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 1934,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 1935,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 1936,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 1937,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 1938,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 1939,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 1940,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 1941,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 1942,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 1943,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 1944,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 1945,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 1946,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 1947,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 1948,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 1949,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 1950,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 1951,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 1952,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 1953,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 1954,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 1955,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 1956,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 1957,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 1958,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 1959,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 1960,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 1961,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 1962,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 1963,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 1964,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 1965,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 1966,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 1967,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 1968,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 1969,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 1970,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 1971,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 1972,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 1973,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 1974,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 1975,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 1976,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 1977,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 1978,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 1979,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 1980,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 1981,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 1982,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 1983,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 1984,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 1985,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 1986,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 1987,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 1988,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 1989,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 1990,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 1991,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 1992,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 1993,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 1994,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 1995,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 1996,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 1997,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 1998,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 1999,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 2000,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 2001,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 2002,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 2003,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 2004,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 2005,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 2006,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 2007,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 2008,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 2009,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 2010,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 2011,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 2012,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 2013,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 2014,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 2015,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 2016,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 2017,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 2018,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 2019,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 2020,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 2021,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 2022,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 2023,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 2024,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 2025,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 2026,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 2027,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 2028,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 2029,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 2030,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 2031,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 2032,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 2033,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 2034,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 2035,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 2036,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 2037,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 2038,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 2039,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 2040,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 2041,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 2042,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 2043,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 2044,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 2045,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 2046,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 2047,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 2048,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 2049,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 2050,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 2051,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 2052,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 2053,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 2054,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 2055,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 2056,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 2057,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 2058,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 2059,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 2060,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 2061,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 2062,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 2063,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 2064,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 2065,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 2066,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 2067,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 2068,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 2069,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 2070,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 2071,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 2072,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 2073,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 2074,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 2075,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 2076,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 2077,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 2078,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 2079,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 2080,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 2081,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 2082,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 2083,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 2084,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 2085,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 2086,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 2087,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 2088,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 2089,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 2090,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 2091,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 2092,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 2093,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 2094,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 2095,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 2096,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 2097,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 2098,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 2099,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 2100,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 2101,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 2102,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 2103,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 2104,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 2105,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 2106,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 2107,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 2108,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 2109,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 2110,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 2111,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 2112,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 2113,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 2114,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 2115,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 2116,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 2117,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 2118,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 2119,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 2120,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 2121,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 2122,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 2123,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 2124,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 2125,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 2126,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 2127,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 2128,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 2129,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 2130,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 2131,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 2132,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 2133,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 2134,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 2135,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 2136,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 2137,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 2138,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 2139,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 2140,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 2141,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 2142,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 2143,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 2144,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 2145,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 2146,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 2147,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 2148,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 2149,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 2150,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 2151,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 2152,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 2153,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 2154,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 2155,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 2156,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 2157,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 2158,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 2159,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 2160,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 2161,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 2162,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 2163,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 2164,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 2165,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 2166,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 2167,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 2168,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 2169,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 2170,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 2171,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 2172,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 2173,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 2174,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 2175,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 2176,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 2177,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 2178,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 2179,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 2180,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 2181,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 2182,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 2183,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 2184,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 2185,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 2186,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 2187,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 2188,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 2189,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 2190,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 2191,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 2192,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 2193,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 2194,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 2195,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 2196,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 2197,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 2198,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 2199,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 2200,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 2201,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 2202,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 2203,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 2204,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 2205,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 2206,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 2207,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 2208,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 2209,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 2210,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 2211,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 2212,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 2213,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 2214,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 2215,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 2216,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 2217,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 2218,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 2219,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 2220,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 2221,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 2222,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 2223,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 2224,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 2225,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 2226,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 2227,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 2228,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 2229,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 2230,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 2231,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 2232,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 2233,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 2234,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 2235,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 2236,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 2237,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 2238,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 2239,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 2240,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 2241,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 2242,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 2243,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 2244,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 2245,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 2246,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 2247,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 2248,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 2249,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 2250,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 2251,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 2252,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 2253,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 2254,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 2255,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 2256,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 2257,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 2258,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 2259,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 2260,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 2261,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 2262,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 2263,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 2264,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 2265,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 2266,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 2267,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 2268,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 2269,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 2270,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 2271,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 2272,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 2273,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 2274,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 2275,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 2276,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 2277,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 2278,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 2279,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 2280,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 2281,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 2282,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 2283,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 2284,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 2285,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 2286,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 2287,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 2288,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 2289,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 2290,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 2291,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 2292,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 2293,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 2294,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 2295,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 2296,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 2297,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 2298,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 2299,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 2300,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 2301,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 2302,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 2303,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 2304,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 2305,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 2306,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 2307,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 2308,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 2309,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 2310,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 2311,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 2312,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 2313,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 2314,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 2315,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 2316,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 2317,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 2318,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 2319,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 2320,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 2321,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 2322,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 2323,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 2324,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 2325,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 2326,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 2327,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 2328,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 2329,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 2330,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 2331,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 2332,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 2333,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 2334,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 2335,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 2336,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 2337,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 2338,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 2339,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 2340,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 2341,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 2342,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 2343,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 2344,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 2345,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 2346,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 2347,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 2348,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 2349,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 2350,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 2351,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 2352,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 2353,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 2354,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 2355,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 2356,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 2357,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 2358,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 2359,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 2360,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 2361,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 2362,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 2363,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 2364,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 2365,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 2366,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 2367,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 2368,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 2369,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 2370,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 2371,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 2372,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 2373,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 2374,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 2375,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 2376,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 2377,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 2378,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 2379,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 2380,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 2381,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 2382,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 2383,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 2384,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 2385,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 2386,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 2387,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 2388,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 2389,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 2390,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 2391,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 2392,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 2393,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 2394,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 2395,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 2396,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 2397,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 2398,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 2399,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 2400,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 2401,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 2402,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 2403,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 2404,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 2405,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 2406,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 2407,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 2408,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 2409,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 2410,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 2411,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 2412,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 2413,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 2414,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 2415,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 2416,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 2417,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 2418,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 2419,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 2420,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 2421,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 2422,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 2423,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 2424,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 2425,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 2426,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 2427,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 2428,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 2429,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 2430,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 2431,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 2432,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 2433,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 2434,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 2435,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 2436,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 2437,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 2438,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 2439,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 2440,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 2441,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 2442,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 2443,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 2444,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 2445,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 2446,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 2447,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 2448,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 2449,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 2450,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 2451,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 2452,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 2453,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 2454,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 2455,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 2456,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 2457,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 2458,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 2459,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 2460,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 2461,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 2462,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 2463,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 2464,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 2465,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 2466,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 2467,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 2468,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 2469,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 2470,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 2471,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 2472,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 2473,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 2474,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 2475,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 2476,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 2477,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 2478,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 2479,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 2480,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 2481,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 2482,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 2483,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 2484,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 2485,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 2486,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 2487,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 2488,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 2489,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 2490,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 2491,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 2492,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 2493,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 2494,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 2495,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 2496,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 2497,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 2498,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 2499,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 2500,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 2501,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 2502,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 2503,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 2504,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 2505,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 2506,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 2507,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 2508,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 2509,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 2510,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 2511,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 2512,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 2513,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 2514,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 2515,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 2516,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 2517,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 2518,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 2519,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 2520,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 2521,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 2522,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 2523,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 2524,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 2525,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 2526,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 2527,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 2528,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 2529,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 2530,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 2531,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 2532,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 2533,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 2534,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 2535,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 2536,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 2537,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 2538,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 2539,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 2540,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 2541,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 2542,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 2543,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 2544,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 2545,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 2546,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 2547,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 2548,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 2549,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 2550,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 2551,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 2552,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 2553,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 2554,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 2555,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 2556,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 2557,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 2558,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 2559,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 2560,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 2561,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 2562,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 2563,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 2564,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 2565,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 2566,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 2567,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 2568,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 2569,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 2570,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 2571,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 2572,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 2573,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 2574,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 2575,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 2576,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 2577,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 2578,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 2579,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 2580,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 2581,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 2582,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 2583,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 2584,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 2585,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 2586,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 2587,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 2588,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 2589,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 2590,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 2591,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 2592,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 2593,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 2594,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 2595,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 2596,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 2597,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 2598,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 2599,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 2600,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 2601,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 2602,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 2603,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 2604,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 2605,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 2606,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 2607,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 2608,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 2609,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 2610,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 2611,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 2612,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 2613,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 2614,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 2615,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 2616,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 2617,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 2618,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 2619,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 2620,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 2621,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 2622,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 2623,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 2624,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 2625,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 2626,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 2627,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 2628,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 2629,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 2630,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 2631,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 2632,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 2633,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 2634,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 2635,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 2636,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 2637,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 2638,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 2639,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 2640,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 2641,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 2642,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 2643,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 2644,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 2645,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 2646,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 2647,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 2648,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 2649,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 2650,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 2651,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 2652,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 2653,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 2654,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 2655,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 2656,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 2657,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 2658,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 2659,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 2660,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 2661,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 2662,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 2663,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 2664,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 2665,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 2666,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 2667,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 2668,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 2669,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 2670,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 2671,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 2672,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 2673,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 2674,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 2675,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 2676,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 2677,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 2678,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 2679,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 2680,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 2681,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 2682,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 2683,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 2684,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 2685,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 2686,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 2687,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 2688,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 2689,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 2690,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 2691,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 2692,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 2693,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 2694,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 2695,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 2696,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 2697,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 2698,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 2699,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 2700,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 2701,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 2702,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 2703,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 2704,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 2705,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 2706,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 2707,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 2708,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 2709,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 2710,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 2711,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 2712,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 2713,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 2714,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 2715,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 2716,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 2717,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 2718,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 2719,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 2720,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 2721,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 2722,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 2723,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 2724,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 2725,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 2726,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 2727,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 2728,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 2729,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 2730,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 2731,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 2732,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 2733,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 2734,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 2735,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 2736,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 2737,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 2738,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 2739,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 2740,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 2741,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 2742,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 2743,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 2744,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 2745,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 2746,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 2747,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 2748,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 2749,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 2750,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 2751,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 2752,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 2753,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 2754,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 2755,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 2756,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 2757,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 2758,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 2759,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 2760,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 2761,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 2762,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 2763,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 2764,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 2765,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 2766,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 2767,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 2768,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 2769,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 2770,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 2771,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 2772,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 2773,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 2774,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 2775,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 2776,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 2777,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 2778,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 2779,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 2780,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 2781,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 2782,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 2783,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 2784,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 2785,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 2786,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 2787,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 2788,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 2789,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 2790,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 2791,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 2792,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 2793,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 2794,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 2795,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 2796,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 2797,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 2798,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 2799,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 2800,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 2801,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 2802,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 2803,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 2804,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 2805,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 2806,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 2807,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 2808,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 2809,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 2810,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 2811,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 2812,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 2813,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 2814,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 2815,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 2816,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 2817,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 2818,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 2819,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 2820,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 2821,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 2822,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 2823,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 2824,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 2825,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 2826,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 2827,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 2828,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 2829,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 2830,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 2831,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 2832,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 2833,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 2834,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 2835,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 2836,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 2837,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 2838,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 2839,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 2840,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 2841,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 2842,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 2843,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 2844,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 2845,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 2846,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 2847,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 2848,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 2849,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 2850,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 2851,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 2852,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 2853,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 2854,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 2855,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 2856,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 2857,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 2858,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 2859,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 2860,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 2861,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 2862,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 2863,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 2864,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 2865,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 2866,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 2867,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 2868,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 2869,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 2870,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 2871,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 2872,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 2873,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 2874,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 2875,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 2876,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 2877,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 2878,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 2879,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 2880,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 2881,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 2882,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 2883,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 2884,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 2885,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 2886,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 2887,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 2888,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 2889,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 2890,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 2891,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 2892,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 2893,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 2894,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 2895,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 2896,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 2897,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 2898,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 2899,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 2900,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 2901,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 2902,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 2903,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 2904,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 2905,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 2906,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 2907,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 2908,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 2909,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 2910,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 2911,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 2912,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 2913,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 2914,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 2915,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 2916,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 2917,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 2918,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 2919,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 2920,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 2921,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 2922,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 2923,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 2924,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 2925,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 2926,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 2927,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 2928,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 2929,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 2930,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 2931,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 2932,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 2933,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 2934,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 2935,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 2936,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 2937,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 2938,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 2939,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 2940,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 2941,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 2942,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 2943,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 2944,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 2945,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 2946,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 2947,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 2948,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 2949,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 2950,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 2951,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 2952,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 2953,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 2954,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 2955,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 2956,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 2957,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 2958,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 2959,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 2960,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 2961,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 2962,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 2963,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 2964,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 2965,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 2966,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 2967,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 2968,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 2969,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 2970,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 2971,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 2972,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 2973,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 2974,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 2975,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 2976,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 2977,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 2978,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 2979,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 2980,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 2981,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 2982,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 2983,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 2984,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 2985,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 2986,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 2987,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 2988,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 2989,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 2990,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 2991,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 2992,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 2993,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 2994,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 2995,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 2996,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 2997,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 2998,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 2999,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 3000,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 3001,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 3002,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 3003,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 3004,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 3005,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 3006,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 3007,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 3008,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 3009,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 3010,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 3011,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 3012,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 3013,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 3014,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 3015,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 3016,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 3017,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 3018,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 3019,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 3020,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 3021,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 3022,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 3023,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 3024,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 3025,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 3026,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 3027,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 3028,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 3029,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 3030,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 3031,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 3032,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 3033,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 3034,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 3035,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 3036,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 3037,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 3038,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 3039,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 3040,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 3041,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 3042,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 3043,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 3044,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 3045,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 3046,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 3047,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 3048,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 3049,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 3050,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 3051,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 3052,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 3053,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 3054,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 3055,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 3056,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 3057,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 3058,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 3059,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 3060,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 3061,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 3062,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 3063,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 3064,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 3065,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 3066,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 3067,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 3068,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 3069,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 3070,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 3071,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 3072,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 3073,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 3074,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 3075,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 3076,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 3077,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 3078,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 3079,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 3080,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 3081,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 3082,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 3083,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 3084,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 3085,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 3086,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 3087,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 3088,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 3089,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 3090,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 3091,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 3092,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 3093,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 3094,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 3095,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 3096,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 3097,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 3098,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 3099,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 3100,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 3101,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 3102,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 3103,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 3104,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 3105,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 3106,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 3107,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 3108,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 3109,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 3110,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 3111,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 3112,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 3113,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 3114,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 3115,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 3116,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 3117,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 3118,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 3119,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 3120,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 3121,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 3122,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 3123,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 3124,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 3125,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 3126,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 3127,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 3128,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 3129,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 3130,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 3131,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 3132,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 3133,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 3134,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 3135,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 3136,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 3137,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 3138,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 3139,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 3140,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 3141,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 3142,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 3143,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 3144,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 3145,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 3146,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 3147,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 3148,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 3149,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 3150,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 3151,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 3152,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 3153,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 3154,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 3155,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 3156,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 3157,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 3158,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 3159,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 3160,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 3161,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 3162,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 3163,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 3164,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 3165,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 3166,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 3167,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 3168,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 3169,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 3170,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 3171,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 3172,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 3173,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 3174,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 3175,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 3176,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 3177,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 3178,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 3179,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 3180,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 3181,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 3182,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 3183,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 3184,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 3185,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 3186,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 3187,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 3188,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 3189,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 3190,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 3191,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 3192,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 3193,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 3194,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 3195,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 3196,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 3197,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 3198,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 3199,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 3200,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 3201,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 3202,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 3203,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 3204,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 3205,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 3206,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 3207,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 3208,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 3209,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 3210,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 3211,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 3212,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 3213,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 3214,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 3215,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 3216,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 3217,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 3218,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 3219,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 3220,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 3221,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 3222,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 3223,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 3224,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 3225,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 3226,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 3227,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 3228,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 3229,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 3230,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 3231,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 3232,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 3233,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 3234,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 3235,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 3236,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 3237,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 3238,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 3239,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 3240,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 3241,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 3242,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 3243,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 3244,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 3245,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 3246,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 3247,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 3248,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 3249,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 3250,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 3251,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 3252,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 3253,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 3254,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 3255,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 3256,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 3257,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 3258,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 3259,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 3260,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 3261,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 3262,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 3263,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 3264,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 3265,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 3266,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 3267,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 3268,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 3269,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 3270,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 3271,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 3272,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 3273,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 3274,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 3275,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 3276,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 3277,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 3278,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 3279,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 3280,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 3281,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 3282,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 3283,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 3284,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 3285,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 3286,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 3287,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 3288,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 3289,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 3290,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 3291,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 3292,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 3293,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 3294,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 3295,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 3296,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 3297,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 3298,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 3299,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 3300,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 3301,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 3302,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 3303,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 3304,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 3305,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 3306,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 3307,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 3308,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 3309,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 3310,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 3311,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 3312,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 3313,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 3314,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 3315,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 3316,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 3317,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 3318,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 3319,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 3320,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 3321,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 3322,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 3323,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 3324,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 3325,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 3326,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 3327,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 3328,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 3329,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 3330,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 3331,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 3332,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 3333,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 3334,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 3335,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 3336,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 3337,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 3338,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 3339,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 3340,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 3341,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 3342,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 3343,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 3344,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 3345,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 3346,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 3347,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 3348,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 3349,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 3350,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 3351,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 3352,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 3353,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 3354,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 3355,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 3356,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 3357,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 3358,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 3359,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 3360,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 3361,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 3362,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 3363,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 3364,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 3365,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 3366,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 3367,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 3368,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 3369,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 3370,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 3371,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 3372,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 3373,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 3374,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 3375,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 3376,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 3377,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 3378,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 3379,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 3380,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 3381,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 3382,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 3383,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 3384,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 3385,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 3386,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 3387,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 3388,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 3389,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 3390,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 3391,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 3392,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 3393,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 3394,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 3395,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 3396,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 3397,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 3398,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 3399,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 3400,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 3401,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 3402,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 3403,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 3404,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 3405,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 3406,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 3407,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 3408,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 3409,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 3410,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 3411,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 3412,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 3413,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 3414,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 3415,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 3416,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 3417,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 3418,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 3419,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 3420,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 3421,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 3422,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 3423,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 3424,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 3425,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 3426,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 3427,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 3428,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 3429,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 3430,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 3431,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 3432,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 3433,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 3434,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 3435,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 3436,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 3437,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 3438,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 3439,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 3440,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 3441,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 3442,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 3443,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 3444,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 3445,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 3446,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 3447,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 3448,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 3449,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 3450,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 3451,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 3452,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 3453,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 3454,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 3455,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 3456,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 3457,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 3458,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 3459,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 3460,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 3461,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 3462,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 3463,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 3464,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 3465,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 3466,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 3467,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 3468,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 3469,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 3470,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 3471,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 3472,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 3473,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 3474,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 3475,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 3476,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 3477,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 3478,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 3479,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 3480,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 3481,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 3482,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 3483,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 3484,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 3485,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 3486,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 3487,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 3488,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 3489,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 3490,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 3491,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 3492,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 3493,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 3494,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 3495,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 3496,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 3497,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 3498,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 3499,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 3500,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 3501,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 3502,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 3503,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 3504,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 3505,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 3506,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 3507,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 3508,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 3509,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 3510,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 3511,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 3512,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 3513,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 3514,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 3515,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 3516,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 3517,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 3518,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 3519,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 3520,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 3521,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 3522,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 3523,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 3524,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 3525,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 3526,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 3527,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 3528,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 3529,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 3530,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 3531,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 3532,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 3533,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 3534,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 3535,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 3536,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 3537,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 3538,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 3539,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 3540,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 3541,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 3542,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 3543,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 3544,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 3545,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 3546,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 3547,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 3548,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 3549,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 3550,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 3551,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 3552,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 3553,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 3554,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 3555,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 3556,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 3557,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 3558,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 3559,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 3560,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 3561,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 3562,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 3563,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 3564,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 3565,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 3566,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 3567,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 3568,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 3569,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 3570,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 3571,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 3572,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 3573,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 3574,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 3575,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 3576,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 3577,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 3578,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 3579,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 3580,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 3581,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 3582,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 3583,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 3584,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 3585,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 3586,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 3587,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 3588,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 3589,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 3590,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 3591,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 3592,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 3593,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 3594,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 3595,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 3596,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 3597,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 3598,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 3599,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 3600,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 3601,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 3602,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 3603,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 3604,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 3605,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 3606,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 3607,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 3608,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 3609,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 3610,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 3611,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 3612,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 3613,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 3614,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 3615,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 3616,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 3617,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 3618,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 3619,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 3620,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 3621,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 3622,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 3623,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 3624,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 3625,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 3626,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 3627,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 3628,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 3629,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 3630,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 3631,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 3632,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 3633,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 3634,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 3635,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 3636,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 3637,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 3638,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 3639,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 3640,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 3641,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 3642,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 3643,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 3644,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 3645,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 3646,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 3647,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 3648,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 3649,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 3650,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 3651,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 3652,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 3653,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 3654,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 3655,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 3656,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 3657,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 3658,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 3659,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 3660,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 3661,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 3662,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 3663,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 3664,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 3665,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 3666,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 3667,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 3668,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 3669,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 3670,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 3671,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 3672,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 3673,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 3674,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 3675,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 3676,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 3677,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 3678,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 3679,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 3680,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 3681,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 3682,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 3683,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 3684,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 3685,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 3686,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 3687,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 3688,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 3689,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 3690,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 3691,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 3692,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 3693,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 3694,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 3695,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 3696,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 3697,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 3698,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 3699,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 3700,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 3701,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 3702,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 3703,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 3704,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 3705,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 3706,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 3707,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 3708,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 3709,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 3710,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 3711,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 3712,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 3713,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 3714,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 3715,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 3716,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 3717,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 3718,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 3719,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 3720,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 3721,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 3722,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 3723,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 3724,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 3725,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 3726,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 3727,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 3728,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 3729,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 3730,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 3731,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 3732,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 3733,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 3734,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 3735,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 3736,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 3737,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 3738,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 3739,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 3740,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 3741,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 3742,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 3743,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 3744,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 3745,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 3746,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 3747,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 3748,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 3749,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 3750,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 3751,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 3752,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 3753,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 3754,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 3755,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 3756,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 3757,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 3758,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 3759,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 3760,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 3761,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 3762,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 3763,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 3764,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 3765,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 3766,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 3767,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 3768,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 3769,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 3770,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 3771,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 3772,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 3773,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 3774,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 3775,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 3776,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 3777,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 3778,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 3779,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 3780,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 3781,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 3782,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 3783,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 3784,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 3785,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 3786,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 3787,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 3788,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 3789,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 3790,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 3791,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 3792,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 3793,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 3794,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 3795,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 3796,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 3797,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 3798,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 3799,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 3800,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 3801,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 3802,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 3803,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 3804,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 3805,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 3806,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 3807,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 3808,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 3809,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 3810,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 3811,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 3812,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 3813,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 3814,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 3815,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 3816,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 3817,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 3818,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 3819,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 3820,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 3821,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 3822,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 3823,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 3824,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 3825,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 3826,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 3827,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 3828,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 3829,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 3830,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 3831,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 3832,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 3833,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 3834,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 3835,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 3836,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 3837,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 3838,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 3839,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 3840,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 3841,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 3842,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 3843,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 3844,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 3845,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 3846,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 3847,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 3848,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 3849,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 3850,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 3851,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 3852,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 3853,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 3854,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 3855,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 3856,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 3857,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 3858,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 3859,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 3860,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 3861,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 3862,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 3863,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 3864,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 3865,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 3866,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 3867,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 3868,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 3869,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 3870,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 3871,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 3872,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 3873,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 3874,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 3875,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 3876,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 3877,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 3878,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 3879,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 3880,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 3881,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 3882,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 3883,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 3884,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 3885,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 3886,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 3887,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 3888,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 3889,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 3890,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 3891,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 3892,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 3893,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 3894,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 3895,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 3896,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 3897,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 3898,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 3899,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 3900,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 3901,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 3902,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 3903,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 3904,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 3905,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 3906,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 3907,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 3908,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 3909,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 3910,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 3911,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 3912,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 3913,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 3914,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 3915,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 3916,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 3917,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 3918,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 3919,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 3920,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 3921,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 3922,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 3923,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 3924,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 3925,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 3926,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 3927,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 3928,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 3929,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 3930,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 3931,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 3932,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 3933,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 3934,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 3935,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 3936,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 3937,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 3938,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 3939,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 3940,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 3941,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 3942,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 3943,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 3944,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 3945,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 3946,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 3947,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 3948,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 3949,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 3950,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 3951,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 3952,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 3953,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 3954,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 3955,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 3956,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 3957,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 3958,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 3959,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 3960,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 3961,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 3962,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 3963,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 3964,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 3965,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 3966,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 3967,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 3968,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 3969,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 3970,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 3971,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 3972,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 3973,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 3974,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 3975,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 3976,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 3977,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 3978,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 3979,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 3980,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 3981,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 3982,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 3983,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 3984,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 3985,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 3986,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 3987,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 3988,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 3989,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 3990,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 3991,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 3992,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 3993,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 3994,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 3995,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 3996,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 3997,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 3998,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 3999,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 4000,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 4001,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 4002,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 4003,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 4004,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 4005,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 4006,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 4007,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 4008,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 4009,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 4010,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 4011,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 4012,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 4013,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 4014,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 4015,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 4016,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 4017,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 4018,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 4019,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 4020,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 4021,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 4022,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 4023,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 4024,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 4025,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 4026,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 4027,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 4028,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 4029,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 4030,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 4031,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 4032,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 4033,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 4034,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 4035,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 4036,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 4037,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 4038,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 4039,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 4040,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 4041,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 4042,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 4043,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 4044,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 4045,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 4046,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 4047,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 4048,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 4049,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 4050,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 4051,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 4052,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 4053,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 4054,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 4055,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 4056,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 4057,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 4058,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 4059,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 4060,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 4061,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 4062,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 4063,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 4064,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 4065,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 4066,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 4067,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 4068,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 4069,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 4070,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 4071,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 4072,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 4073,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 4074,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 4075,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 4076,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 4077,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 4078,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 4079,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 4080,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 4081,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 4082,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 4083,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 4084,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 4085,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 4086,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 4087,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 4088,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 4089,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 4090,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 4091,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 4092,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 4093,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 4094,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 4095,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 4096,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 4097,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 4098,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 4099,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 4100,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 4101,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 4102,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 4103,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 4104,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 4105,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 4106,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 4107,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 4108,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 4109,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 4110,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 4111,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 4112,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 4113,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 4114,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 4115,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 4116,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 4117,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 4118,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 4119,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 4120,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 4121,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 4122,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 4123,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 4124,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 4125,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 4126,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 4127,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 4128,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 4129,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 4130,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 4131,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 4132,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 4133,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 4134,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 4135,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 4136,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 4137,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 4138,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 4139,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 4140,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 4141,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 4142,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 4143,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 4144,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 4145,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 4146,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 4147,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 4148,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 4149,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 4150,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 4151,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 4152,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 4153,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 4154,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 4155,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 4156,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 4157,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 4158,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 4159,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 4160,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 4161,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 4162,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 4163,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 4164,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 4165,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 4166,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 4167,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 4168,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 4169,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 4170,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 4171,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 4172,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 4173,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 4174,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 4175,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 4176,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 4177,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 4178,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 4179,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 4180,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 4181,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 4182,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 4183,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 4184,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 4185,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 4186,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 4187,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 4188,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 4189,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 4190,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 4191,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 4192,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 4193,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 4194,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 4195,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 4196,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 4197,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 4198,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 4199,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 4200,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 4201,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 4202,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 4203,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 4204,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 4205,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 4206,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 4207,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 4208,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 4209,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 4210,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 4211,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 4212,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 4213,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 4214,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 4215,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 4216,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 4217,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 4218,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 4219,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 4220,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 4221,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 4222,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 4223,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 4224,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 4225,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 4226,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 4227,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 4228,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 4229,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 4230,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 4231,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 4232,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 4233,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 4234,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 4235,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 4236,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 4237,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 4238,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 4239,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 4240,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 4241,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 4242,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 4243,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 4244,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 4245,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 4246,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 4247,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 4248,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 4249,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 4250,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 4251,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 4252,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 4253,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 4254,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 4255,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 4256,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 4257,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 4258,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 4259,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 4260,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 4261,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 4262,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 4263,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 4264,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 4265,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 4266,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 4267,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 4268,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 4269,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 4270,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 4271,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 4272,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 4273,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 4274,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 4275,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 4276,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 4277,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 4278,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 4279,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 4280,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 4281,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 4282,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 4283,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 4284,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 4285,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 4286,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 4287,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 4288,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 4289,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 4290,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 4291,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 4292,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 4293,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 4294,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 4295,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 4296,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 4297,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 4298,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 4299,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 4300,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 4301,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 4302,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 4303,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 4304,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 4305,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 4306,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 4307,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 4308,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 4309,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 4310,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 4311,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 4312,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 4313,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 4314,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 4315,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 4316,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 4317,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 4318,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 4319,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 4320,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 4321,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 4322,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 4323,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 4324,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 4325,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 4326,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 4327,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 4328,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 4329,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 4330,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 4331,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 4332,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 4333,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 4334,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 4335,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 4336,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 4337,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 4338,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 4339,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 4340,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 4341,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 4342,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 4343,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 4344,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 4345,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 4346,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 4347,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 4348,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 4349,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 4350,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 4351,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 4352,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 4353,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 4354,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 4355,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 4356,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 4357,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 4358,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 4359,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 4360,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 4361,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 4362,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 4363,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 4364,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 4365,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 4366,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 4367,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 4368,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 4369,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 4370,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 4371,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 4372,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 4373,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 4374,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 4375,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 4376,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 4377,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 4378,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 4379,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 4380,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 4381,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 4382,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 4383,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 4384,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 4385,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 4386,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 4387,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 4388,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 4389,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 4390,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 4391,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 4392,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 4393,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 4394,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 4395,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 4396,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 4397,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 4398,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 4399,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 4400,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 4401,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 4402,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 4403,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,"
    },
    {
        "vector_id": 4404,
        "text": "explainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of"
    },
    {
        "vector_id": 4405,
        "text": "evaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding"
    },
    {
        "vector_id": 4406,
        "text": "on the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR)."
    },
    {
        "vector_id": 4407,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style"
    },
    {
        "vector_id": 4408,
        "text": "over-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 4409,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval"
    },
    {
        "vector_id": 4410,
        "text": "Although interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,"
    },
    {
        "vector_id": 4411,
        "text": "P.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@\nl3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75"
    },
    {
        "vector_id": 4412,
        "text": "Free-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches"
    },
    {
        "vector_id": 4413,
        "text": "(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from\npost-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR"
    },
    {
        "vector_id": 4414,
        "text": "and delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative"
    },
    {
        "vector_id": 4415,
        "text": "window into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),\nInternational Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68"
    },
    {
        "vector_id": 4416,
        "text": "papers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus"
    },
    {
        "vector_id": 4417,
        "text": "on core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated\nin NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145]."
    },
    {
        "vector_id": 4418,
        "text": "We only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data."
    },
    {
        "vector_id": 4419,
        "text": "Pre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used\nin ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these"
    },
    {
        "vector_id": 4420,
        "text": "areas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability"
    },
    {
        "vector_id": 4421,
        "text": "method. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or\nform of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability."
    },
    {
        "vector_id": 4422,
        "text": "2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the"
    },
    {
        "vector_id": 4423,
        "text": "locality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given\nquery. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a"
    },
    {
        "vector_id": 4424,
        "text": "candidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading"
    },
    {
        "vector_id": 4425,
        "text": "to a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only\nif the feature space is understandable but also if the explanation is small. An attribution over a"
    },
    {
        "vector_id": 4426,
        "text": "feature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of"
    },
    {
        "vector_id": 4427,
        "text": "input instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of\ngaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of"
    },
    {
        "vector_id": 4428,
        "text": "the prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 4429,
        "text": "Chair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]\nand MS MARCO [86], respectively."
    },
    {
        "vector_id": 4430,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has"
    },
    {
        "vector_id": 4431,
        "text": "no access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the"
    },
    {
        "vector_id": 4432,
        "text": "work in the existing literature only considers our definition of a weakly agnostic model.\n2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and"
    },
    {
        "vector_id": 4433,
        "text": "numeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)"
    },
    {
        "vector_id": 4434,
        "text": "models as much as possible specifically for high-stakes decision-making. However, building an\nIBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,"
    },
    {
        "vector_id": 4435,
        "text": "when in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,"
    },
    {
        "vector_id": 4436,
        "text": "IBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods.\nPre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to"
    },
    {
        "vector_id": 4437,
        "text": "improve the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented"
    },
    {
        "vector_id": 4438,
        "text": "in both post-hoc and IBD manner.\n2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and"
    },
    {
        "vector_id": 4439,
        "text": "functionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR."
    },
    {
        "vector_id": 4440,
        "text": "employed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do\nnot differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability"
    },
    {
        "vector_id": 4441,
        "text": "of their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate"
    },
    {
        "vector_id": 4442,
        "text": "the underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could\nbe derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when"
    },
    {
        "vector_id": 4443,
        "text": "we come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference"
    },
    {
        "vector_id": 4444,
        "text": "model. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically\ngenerate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 4445,
        "text": "features. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and"
    },
    {
        "vector_id": 4446,
        "text": "\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange."
    },
    {
        "vector_id": 4447,
        "text": "are used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we"
    },
    {
        "vector_id": 4448,
        "text": "differentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated\nas a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing"
    },
    {
        "vector_id": 4449,
        "text": "importance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly"
    },
    {
        "vector_id": 4450,
        "text": "chosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the\nimportant tokens, the authors find that they often correspond to exact match terms, i.e., terms that"
    },
    {
        "vector_id": 4451,
        "text": "also appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-"
    },
    {
        "vector_id": 4452,
        "text": "terpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 4453,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification"
    },
    {
        "vector_id": 4454,
        "text": "problem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 4455,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model"
    },
    {
        "vector_id": 4456,
        "text": "explanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term"
    },
    {
        "vector_id": 4457,
        "text": "replacement strategies produce better explanations than replacement strategies that use term\nposition information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on"
    },
    {
        "vector_id": 4458,
        "text": "the interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query"
    },
    {
        "vector_id": 4459,
        "text": "localities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous.\n3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose"
    },
    {
        "vector_id": 4460,
        "text": "a simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced"
    },
    {
        "vector_id": 4461,
        "text": "by DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation\nmeasures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang"
    },
    {
        "vector_id": 4462,
        "text": "3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small"
    },
    {
        "vector_id": 4463,
        "text": "change in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between\na baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each"
    },
    {
        "vector_id": 4464,
        "text": "baseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to"
    },
    {
        "vector_id": 4465,
        "text": "neural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find\nthat the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high"
    },
    {
        "vector_id": 4466,
        "text": "overlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps."
    },
    {
        "vector_id": 4467,
        "text": "extracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature\nattributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]"
    },
    {
        "vector_id": 4468,
        "text": "Fig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of"
    },
    {
        "vector_id": 4469,
        "text": "transformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 4470,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed"
    },
    {
        "vector_id": 4471,
        "text": "by observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance"
    },
    {
        "vector_id": 4472,
        "text": "Stopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens.\nIn addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and"
    },
    {
        "vector_id": 4473,
        "text": "\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document"
    },
    {
        "vector_id": 4474,
        "text": "frequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained"
    },
    {
        "vector_id": 4475,
        "text": "can also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature"
    },
    {
        "vector_id": 4476,
        "text": "attribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the\nsurrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to"
    },
    {
        "vector_id": 4477,
        "text": "a reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems"
    },
    {
        "vector_id": 4478,
        "text": "advantageous, but does not provide any guarantees of finding a good explanation. Based on the\nlimited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation"
    },
    {
        "vector_id": 4479,
        "text": "Approach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness"
    },
    {
        "vector_id": 4480,
        "text": "Intent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,"
    },
    {
        "vector_id": 4481,
        "text": "visualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can"
    },
    {
        "vector_id": 4482,
        "text": "be more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words.\nThis form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style"
    },
    {
        "vector_id": 4483,
        "text": "is commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98]."
    },
    {
        "vector_id": 4484,
        "text": "Approaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary\nto explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex"
    },
    {
        "vector_id": 4485,
        "text": "ranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise"
    },
    {
        "vector_id": 4486,
        "text": "preferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation\nfor the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29"
    },
    {
        "vector_id": 4487,
        "text": "1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}"
    },
    {
        "vector_id": 4488,
        "text": "+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity."
    },
    {
        "vector_id": 4489,
        "text": "with high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting"
    },
    {
        "vector_id": 4490,
        "text": "the relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model.\nHowever, it has not yet been examined whether the generations of CtrsGen explain the underlying"
    },
    {
        "vector_id": 4491,
        "text": "ranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]"
    },
    {
        "vector_id": 4492,
        "text": "suggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 4493,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets"
    },
    {
        "vector_id": 4494,
        "text": "and is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 4495,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,"
    },
    {
        "vector_id": 4496,
        "text": "where the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 4497,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to"
    },
    {
        "vector_id": 4498,
        "text": "measure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 4499,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine"
    },
    {
        "vector_id": 4500,
        "text": "learning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 4501,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]"
    },
    {
        "vector_id": 4502,
        "text": "argues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank"
    },
    {
        "vector_id": 4503,
        "text": "demotion of a document. For example, a company aiming to optimize search engines could leverage\nadversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-"
    },
    {
        "vector_id": 4504,
        "text": "differentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,"
    },
    {
        "vector_id": 4505,
        "text": "the adversarial perturbations are restricted by semantic similarity to the original document. The\nauthors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents."
    },
    {
        "vector_id": 4506,
        "text": "Wang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model"
    },
    {
        "vector_id": 4507,
        "text": "making a specific prediction whenever the trigger is concatenated to any input. Starting from\nan initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns."
    },
    {
        "vector_id": 4508,
        "text": "5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on"
    },
    {
        "vector_id": 4509,
        "text": "ClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets\nand expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which"
    },
    {
        "vector_id": 4510,
        "text": "highly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of"
    },
    {
        "vector_id": 4511,
        "text": "interpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,\nAxiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences"
    },
    {
        "vector_id": 4512,
        "text": "Given \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms"
    },
    {
        "vector_id": 4513,
        "text": "Prefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14].\nwas first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?"
    },
    {
        "vector_id": 4514,
        "text": "\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms"
    },
    {
        "vector_id": 4515,
        "text": "occurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity\n[38], or term proximity [47] among others (see Table 2). For a more detailed description of the"
    },
    {
        "vector_id": 4516,
        "text": "various axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to"
    },
    {
        "vector_id": 4517,
        "text": "attacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers\n(Section 6.3). An overview of this classification and papers in this section can be found in Table 3."
    },
    {
        "vector_id": 4518,
        "text": "Pre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -"
    },
    {
        "vector_id": 4519,
        "text": "Chen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking.\nBy learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings."
    },
    {
        "vector_id": 4520,
        "text": "Despite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms."
    },
    {
        "vector_id": 4521,
        "text": "This library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic\ndatasets based on existing axioms and checked whether classical neural ranking models are in"
    },
    {
        "vector_id": 4522,
        "text": "agreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical"
    },
    {
        "vector_id": 4523,
        "text": "approaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can\nbe explained by the combination of existing axioms. To do so, they train a small random forest"
    },
    {
        "vector_id": 4524,
        "text": "explanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers"
    },
    {
        "vector_id": 4525,
        "text": "6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 4526,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments"
    },
    {
        "vector_id": 4527,
        "text": "coincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 4528,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal"
    },
    {
        "vector_id": 4529,
        "text": "examples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 4530,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology"
    },
    {
        "vector_id": 4531,
        "text": "7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 4532,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have"
    },
    {
        "vector_id": 4533,
        "text": "Pre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 4534,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models"
    },
    {
        "vector_id": 4535,
        "text": "trained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the"
    },
    {
        "vector_id": 4536,
        "text": "embeddings [127]. For a more comprehensive overview of the initial probing paradigm and the\nproposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning"
    },
    {
        "vector_id": 4537,
        "text": "on the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models."
    },
    {
        "vector_id": 4538,
        "text": "Therefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By\nstratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their"
    },
    {
        "vector_id": 4539,
        "text": "inability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model."
    },
    {
        "vector_id": 4540,
        "text": "The resulting coefficients are then used to understand the importance of the corresponding aspects.\nThe resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability"
    },
    {
        "vector_id": 4541,
        "text": "to three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward"
    },
    {
        "vector_id": 4542,
        "text": "factually correct articles and that appending irrelevant text can improve the relevance scores.\nSimilarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small"
    },
    {
        "vector_id": 4543,
        "text": "parts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models"
    },
    {
        "vector_id": 4544,
        "text": "7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed.\nvan Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such"
    },
    {
        "vector_id": 4545,
        "text": "as question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include"
    },
    {
        "vector_id": 4546,
        "text": "tasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different\nfrom the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least"
    },
    {
        "vector_id": 4547,
        "text": "amount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a"
    },
    {
        "vector_id": 4548,
        "text": "benchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case"
    },
    {
        "vector_id": 4549,
        "text": "in the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the"
    },
    {
        "vector_id": 4550,
        "text": "Pre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation"
    },
    {
        "vector_id": 4551,
        "text": "Feature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of"
    },
    {
        "vector_id": 4552,
        "text": "What information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually\nbeing used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to"
    },
    {
        "vector_id": 4553,
        "text": "have used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels."
    },
    {
        "vector_id": 4554,
        "text": "models.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can\nbe viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,"
    },
    {
        "vector_id": 4555,
        "text": "not all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers"
    },
    {
        "vector_id": 4556,
        "text": "8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of\ncontextual models with transformers, where the self-attention mechanism essentially considers all"
    },
    {
        "vector_id": 4557,
        "text": "pairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque"
    },
    {
        "vector_id": 4558,
        "text": "for both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing\nthe large input space, which we refer to as rationale-based methods. The idea is to use a small set"
    },
    {
        "vector_id": 4559,
        "text": "of explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually."
    },
    {
        "vector_id": 4560,
        "text": "In the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum\nsimilarity score. The final document relevance is computed by simply summing up the maximum"
    },
    {
        "vector_id": 4561,
        "text": "scores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are"
    },
    {
        "vector_id": 4562,
        "text": "learned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting\nin lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1"
    },
    {
        "vector_id": 4563,
        "text": "achieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic"
    },
    {
        "vector_id": 4564,
        "text": "matrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,\nthe interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the"
    },
    {
        "vector_id": 4565,
        "text": "Pre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The"
    },
    {
        "vector_id": 4566,
        "text": "goal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space.\n8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution"
    },
    {
        "vector_id": 4567,
        "text": "methods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated"
    },
    {
        "vector_id": 4568,
        "text": "by simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 4569,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only."
    },
    {
        "vector_id": 4570,
        "text": "This step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation"
    },
    {
        "vector_id": 4571,
        "text": "the input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input\nsnippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of"
    },
    {
        "vector_id": 4572,
        "text": "explanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores."
    },
    {
        "vector_id": 4573,
        "text": "Except for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies\nthe faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation"
    },
    {
        "vector_id": 4574,
        "text": "Colbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-"
    },
    {
        "vector_id": 4575,
        "text": "tecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating\nrationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based"
    },
    {
        "vector_id": 4576,
        "text": "method performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in"
    },
    {
        "vector_id": 4577,
        "text": "Figure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nFig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is"
    },
    {
        "vector_id": 4578,
        "text": "made based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on"
    },
    {
        "vector_id": 4579,
        "text": "the optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was\nproposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial"
    },
    {
        "vector_id": 4580,
        "text": "reports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation"
    },
    {
        "vector_id": 4581,
        "text": "Method Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 4582,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-"
    },
    {
        "vector_id": 4583,
        "text": "intensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main"
    },
    {
        "vector_id": 4584,
        "text": "idea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask\nfashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively"
    },
    {
        "vector_id": 4585,
        "text": "for multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading"
    },
    {
        "vector_id": 4586,
        "text": "Model) approach to overcome the input length limitations of modern transformer-based rankers.\nIDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current"
    },
    {
        "vector_id": 4587,
        "text": "query using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory."
    },
    {
        "vector_id": 4588,
        "text": "The intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy\nmatrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether"
    },
    {
        "vector_id": 4589,
        "text": "the two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is"
    },
    {
        "vector_id": 4590,
        "text": "that of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution\nof relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,"
    },
    {
        "vector_id": 4591,
        "text": "they also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called"
    },
    {
        "vector_id": 4592,
        "text": "select and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of\nthe document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination"
    },
    {
        "vector_id": 4593,
        "text": "of the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the"
    },
    {
        "vector_id": 4594,
        "text": "information bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms.\nSpecifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58"
    },
    {
        "vector_id": 4595,
        "text": "tokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach"
    },
    {
        "vector_id": 4596,
        "text": "should provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the\nrationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple"
    },
    {
        "vector_id": 4597,
        "text": "human-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection."
    },
    {
        "vector_id": 4598,
        "text": "rationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 4599,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)"
    },
    {
        "vector_id": 4600,
        "text": "Furthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of"
    },
    {
        "vector_id": 4601,
        "text": "explainable information retrieval. We have reviewed many interpretability methods and approaches\nthat cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013"
    },
    {
        "vector_id": 4602,
        "text": "most prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches"
    },
    {
        "vector_id": 4603,
        "text": "for information retrieval tasks. For the common task of document retrieval, we discussed early\nheard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document"
    },
    {
        "vector_id": 4604,
        "text": "in the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-"
    },
    {
        "vector_id": 4605,
        "text": "tion, but complex machine learning models learn multiple features for the same behavior, which\nare also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing"
    },
    {
        "vector_id": 4606,
        "text": "approaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,"
    },
    {
        "vector_id": 4607,
        "text": "evaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been\nlimited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of"
    },
    {
        "vector_id": 4608,
        "text": "information retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution"
    },
    {
        "vector_id": 4609,
        "text": "method methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision.\nFor a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query"
    },
    {
        "vector_id": 4610,
        "text": "representation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according"
    },
    {
        "vector_id": 4611,
        "text": "to a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically"
    },
    {
        "vector_id": 4612,
        "text": "involves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective"
    },
    {
        "vector_id": 4613,
        "text": "representation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need\nto help us better understand what a search engine infers about its users. Specifically, an interesting"
    },
    {
        "vector_id": 4614,
        "text": "interpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting"
    },
    {
        "vector_id": 4615,
        "text": "from functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,\ncompleteness, and human congruence. We refer to these methods as intrinsic methods."
    },
    {
        "vector_id": 4616,
        "text": "Pre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure"
    },
    {
        "vector_id": 4617,
        "text": "to create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are\nreasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what"
    },
    {
        "vector_id": 4618,
        "text": "extent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search"
    },
    {
        "vector_id": 4619,
        "text": "applications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the"
    },
    {
        "vector_id": 4620,
        "text": "IR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-"
    },
    {
        "vector_id": 4621,
        "text": "text generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR\napproaches. In the end, we present some limitations and open questions that we foresee as the next"
    },
    {
        "vector_id": 4622,
        "text": "steps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information"
    },
    {
        "vector_id": 4623,
        "text": "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 4624,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416"
    },
    {
        "vector_id": 4625,
        "text": "582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 4626,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for"
    },
    {
        "vector_id": 4627,
        "text": "Computational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367"
    },
    {
        "vector_id": 4628,
        "text": "[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation\nwhen we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),"
    },
    {
        "vector_id": 4629,
        "text": "207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,"
    },
    {
        "vector_id": 4630,
        "text": "Jos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing\nMachinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022."
    },
    {
        "vector_id": 4631,
        "text": "Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in"
    },
    {
        "vector_id": 4632,
        "text": "Information Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022.\nAxiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference"
    },
    {
        "vector_id": 4633,
        "text": "on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378."
    },
    {
        "vector_id": 4634,
        "text": "63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and\ndevelopment in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi"
    },
    {
        "vector_id": 4635,
        "text": "Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618."
    },
    {
        "vector_id": 4636,
        "text": "Vol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 4637,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29"
    },
    {
        "vector_id": 4638,
        "text": "[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf"
    },
    {
        "vector_id": 4639,
        "text": "[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-\ntext contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR"
    },
    {
        "vector_id": 4640,
        "text": "Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,"
    },
    {
        "vector_id": 4641,
        "text": "ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html\n[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828"
    },
    {
        "vector_id": 4642,
        "text": "[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in"
    },
    {
        "vector_id": 4643,
        "text": "Information Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368.\n[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659"
    },
    {
        "vector_id": 4644,
        "text": "[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:"
    },
    {
        "vector_id": 4645,
        "text": "//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text\nClassification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006"
    },
    {
        "vector_id": 4646,
        "text": "[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,"
    },
    {
        "vector_id": 4647,
        "text": "Slovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval."
    },
    {
        "vector_id": 4648,
        "text": "In Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312"
    },
    {
        "vector_id": 4649,
        "text": "3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320."
    },
    {
        "vector_id": 4650,
        "text": "Public opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,"
    },
    {
        "vector_id": 4651,
        "text": "Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR\nResearch, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,"
    },
    {
        "vector_id": 4652,
        "text": "Vol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving"
    },
    {
        "vector_id": 4653,
        "text": "Content Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc\nRetrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM"
    },
    {
        "vector_id": 4654,
        "text": "2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704"
    },
    {
        "vector_id": 4655,
        "text": "[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941.\n[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and"
    },
    {
        "vector_id": 4656,
        "text": "Christo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for"
    },
    {
        "vector_id": 4657,
        "text": "personalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 4658,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,"
    },
    {
        "vector_id": 4659,
        "text": "1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 4660,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500"
    },
    {
        "vector_id": 4661,
        "text": "[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html"
    },
    {
        "vector_id": 4662,
        "text": "[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of\nExplanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of"
    },
    {
        "vector_id": 4663,
        "text": "the Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509."
    },
    {
        "vector_id": 4664,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in"
    },
    {
        "vector_id": 4665,
        "text": "Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 4666,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage"
    },
    {
        "vector_id": 4667,
        "text": "of Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual"
    },
    {
        "vector_id": 4668,
        "text": "International ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,\nUSA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association"
    },
    {
        "vector_id": 4669,
        "text": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011"
    },
    {
        "vector_id": 4670,
        "text": "[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document\nRanking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,"
    },
    {
        "vector_id": 4671,
        "text": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013"
    },
    {
        "vector_id": 4672,
        "text": "[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560"
    },
    {
        "vector_id": 4673,
        "text": "[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree"
    },
    {
        "vector_id": 4674,
        "text": "Ensembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888\n[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html"
    },
    {
        "vector_id": 4675,
        "text": "8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance"
    },
    {
        "vector_id": 4676,
        "text": "Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211"
    },
    {
        "vector_id": 4677,
        "text": "[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005"
    },
    {
        "vector_id": 4678,
        "text": "[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during\npolitical elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,"
    },
    {
        "vector_id": 4679,
        "text": "Qu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive"
    },
    {
        "vector_id": 4680,
        "text": "Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation"
    },
    {
        "vector_id": 4681,
        "text": "of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine"
    },
    {
        "vector_id": 4682,
        "text": "Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200"
    },
    {
        "vector_id": 4683,
        "text": "[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:"
    },
    {
        "vector_id": 4684,
        "text": "Evidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the\n21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281."
    },
    {
        "vector_id": 4685,
        "text": "[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349."
    },
    {
        "vector_id": 4686,
        "text": "[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search\nQueries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314"
    },
    {
        "vector_id": 4687,
        "text": "[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint"
    },
    {
        "vector_id": 4688,
        "text": "abs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 4689,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data"
    },
    {
        "vector_id": 4690,
        "text": "Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23"
    },
    {
        "vector_id": 4691,
        "text": "July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984."
    },
    {
        "vector_id": 4692,
        "text": "https://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable"
    },
    {
        "vector_id": 4693,
        "text": "models instead. Nature Machine Intelligence 1, 5 (2019), 206.\n[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence."
    },
    {
        "vector_id": 4694,
        "text": "AI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286"
    },
    {
        "vector_id": 4695,
        "text": "[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html"
    },
    {
        "vector_id": 4696,
        "text": "[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,"
    },
    {
        "vector_id": 4697,
        "text": "Australia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234"
    },
    {
        "vector_id": 4698,
        "text": "[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of"
    },
    {
        "vector_id": 4699,
        "text": "Information Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465"
    },
    {
        "vector_id": 4700,
        "text": "[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014)."
    },
    {
        "vector_id": 4701,
        "text": "[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill\n(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017"
    },
    {
        "vector_id": 4702,
        "text": "(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452"
    },
    {
        "vector_id": 4703,
        "text": "[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van\nDurme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX"
    },
    {
        "vector_id": 4704,
        "text": "[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A"
    },
    {
        "vector_id": 4705,
        "text": "Layer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:\n//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,"
    },
    {
        "vector_id": 4706,
        "text": "Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational"
    },
    {
        "vector_id": 4707,
        "text": "Linguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14"
    },
    {
        "vector_id": 4708,
        "text": "[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256"
    },
    {
        "vector_id": 4709,
        "text": "[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for"
    },
    {
        "vector_id": 4710,
        "text": "Computational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17"
    },
    {
        "vector_id": 4711,
        "text": "1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,"
    },
    {
        "vector_id": 4712,
        "text": "J. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf"
    },
    {
        "vector_id": 4713,
        "text": "[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 4714,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation"
    },
    {
        "vector_id": 4715,
        "text": "Generator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 4716,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and"
    },
    {
        "vector_id": 4717,
        "text": "Knowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,"
    },
    {
        "vector_id": 4718,
        "text": "SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325\n[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448"
    },
    {
        "vector_id": 4719,
        "text": "359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999"
    },
    {
        "vector_id": 4720,
        "text": "[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066"
    },
    {
        "vector_id": 4721,
        "text": "[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable"
    },
    {
        "vector_id": 4722,
        "text": "Models with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985\n[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan"
    },
    {
        "vector_id": 4723,
        "text": "Sterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 4724,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with"
    },
    {
        "vector_id": 4725,
        "text": "retrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION"
    },
    {
        "vector_id": 4726,
        "text": "I. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable\ngrowth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from"
    },
    {
        "vector_id": 4727,
        "text": "training millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-"
    },
    {
        "vector_id": 4728,
        "text": "ability concerns. Of particular relevance isretrieval-augmented\ngeneration (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-"
    },
    {
        "vector_id": 4729,
        "text": "cates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability"
    },
    {
        "vector_id": 4730,
        "text": "to override trained knowledge [1]. Our explainability focus\u2014\nwhich aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as"
    },
    {
        "vector_id": 4731,
        "text": "CoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external"
    },
    {
        "vector_id": 4732,
        "text": "knowledge sources used during RAG, exposing the in-context\nlearning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with"
    },
    {
        "vector_id": 4733,
        "text": "respect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-"
    },
    {
        "vector_id": 4734,
        "text": "ios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological\nsequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common"
    },
    {
        "vector_id": 4735,
        "text": "1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In"
    },
    {
        "vector_id": 4736,
        "text": "RAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources.\nA user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context."
    },
    {
        "vector_id": 4737,
        "text": "forms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative"
    },
    {
        "vector_id": 4738,
        "text": "relevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or\npermutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the"
    },
    {
        "vector_id": 4739,
        "text": "Hugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU."
    },
    {
        "vector_id": 4740,
        "text": "In RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of\nsources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with"
    },
    {
        "vector_id": 4741,
        "text": "the perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a"
    },
    {
        "vector_id": 4742,
        "text": "bottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined\nas the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal"
    },
    {
        "vector_id": 4743,
        "text": "size, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens"
    },
    {
        "vector_id": 4744,
        "text": "corresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores\nfor combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with"
    },
    {
        "vector_id": 4745,
        "text": "a different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of"
    },
    {
        "vector_id": 4746,
        "text": "similarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested.\nBefore comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a"
    },
    {
        "vector_id": 4747,
        "text": "set of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined"
    },
    {
        "vector_id": 4748,
        "text": "for each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table\nand pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for"
    },
    {
        "vector_id": 4749,
        "text": "which all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-"
    },
    {
        "vector_id": 4750,
        "text": "tation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum\npermutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining"
    },
    {
        "vector_id": 4751,
        "text": "a given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to"
    },
    {
        "vector_id": 4752,
        "text": "use either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-\ntions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting"
    },
    {
        "vector_id": 4753,
        "text": "the s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the"
    },
    {
        "vector_id": 4754,
        "text": "user to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the"
    },
    {
        "vector_id": 4755,
        "text": "importance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to"
    },
    {
        "vector_id": 4756,
        "text": "support various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats.\nRAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by"
    },
    {
        "vector_id": 4757,
        "text": "exploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources."
    },
    {
        "vector_id": 4758,
        "text": "B. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of\nThe Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user"
    },
    {
        "vector_id": 4759,
        "text": "expects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations"
    },
    {
        "vector_id": 4760,
        "text": "of the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer\nbut remains curious about the document\u2019s relative signifi-"
    },
    {
        "vector_id": 4761,
        "text": "cance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and"
    },
    {
        "vector_id": 4762,
        "text": "to understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 4763,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user"
    },
    {
        "vector_id": 4764,
        "text": "asks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM."
    },
    {
        "vector_id": 4765,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that"
    },
    {
        "vector_id": 4766,
        "text": "the LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 4767,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes"
    },
    {
        "vector_id": 4768,
        "text": "that the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 4769,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open"
    },
    {
        "vector_id": 4770,
        "text": "foundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 4771,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 4772,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu"
    },
    {
        "vector_id": 4773,
        "text": "Ninghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension"
    },
    {
        "vector_id": 4774,
        "text": "calls for a significant transformation in XAI methodologies because of two reasons. First,\nmany existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike"
    },
    {
        "vector_id": 4775,
        "text": "traditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also"
    },
    {
        "vector_id": 4776,
        "text": "provide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    },
    {
        "vector_id": 4777,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"
    },
    {
        "vector_id": 4778,
        "text": "3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    },
    {
        "vector_id": 4779,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
    },
    {
        "vector_id": 4780,
        "text": "5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    },
    {
        "vector_id": 4781,
        "text": "6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    },
    {
        "vector_id": 4782,
        "text": "7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29\n2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 4783,
        "text": "8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    },
    {
        "vector_id": 4784,
        "text": "9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34\n10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
    },
    {
        "vector_id": 4785,
        "text": "11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction"
    },
    {
        "vector_id": 4786,
        "text": "12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 4787,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of"
    },
    {
        "vector_id": 4788,
        "text": "a model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 4789,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on"
    },
    {
        "vector_id": 4790,
        "text": "evaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s"
    },
    {
        "vector_id": 4791,
        "text": "imperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete\nsteps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On"
    },
    {
        "vector_id": 4792,
        "text": "the other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and"
    },
    {
        "vector_id": 4793,
        "text": "mathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the\nexpectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-"
    },
    {
        "vector_id": 4794,
        "text": "ceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining"
    },
    {
        "vector_id": 4795,
        "text": "their inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,\nreasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring"
    },
    {
        "vector_id": 4796,
        "text": "the explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI"
    },
    {
        "vector_id": 4797,
        "text": "which includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)"
    },
    {
        "vector_id": 4798,
        "text": "via Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation"
    },
    {
        "vector_id": 4799,
        "text": "for XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with\nseven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations"
    },
    {
        "vector_id": 4800,
        "text": "can pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored"
    },
    {
        "vector_id": 4801,
        "text": "in LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in\nthe context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the"
    },
    {
        "vector_id": 4802,
        "text": "discussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-"
    },
    {
        "vector_id": 4803,
        "text": "ing LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to\nachieving human alignment. Third, we discuss how explainability could guide the augmentation of data,"
    },
    {
        "vector_id": 4804,
        "text": "including both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities"
    },
    {
        "vector_id": 4805,
        "text": "of LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 4806,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods"
    },
    {
        "vector_id": 4807,
        "text": "for large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight"
    },
    {
        "vector_id": 4808,
        "text": "investigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable\nworkflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods"
    },
    {
        "vector_id": 4809,
        "text": "2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore"
    },
    {
        "vector_id": 4810,
        "text": "case studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs.\n2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt"
    },
    {
        "vector_id": 4811,
        "text": "x, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly"
    },
    {
        "vector_id": 4812,
        "text": "applied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in\nsentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model."
    },
    {
        "vector_id": 4813,
        "text": "However, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification"
    },
    {
        "vector_id": 4814,
        "text": "2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis\nof their suitability for explaining large language models."
    },
    {
        "vector_id": 4815,
        "text": "Perturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle"
    },
    {
        "vector_id": 4816,
        "text": "is that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 4817,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding"
    },
    {
        "vector_id": 4818,
        "text": "\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 4819,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds"
    },
    {
        "vector_id": 4820,
        "text": "of inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model"
    },
    {
        "vector_id": 4821,
        "text": "g, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-\ncision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language"
    },
    {
        "vector_id": 4822,
        "text": "models. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for"
    },
    {
        "vector_id": 4823,
        "text": "computing these relevance scores. These methods have been adapted for Transformer-based language models\nin various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there"
    },
    {
        "vector_id": 4824,
        "text": "are key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the"
    },
    {
        "vector_id": 4825,
        "text": "perturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-\ncantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited"
    },
    {
        "vector_id": 4826,
        "text": "explainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in"
    },
    {
        "vector_id": 4827,
        "text": "Table 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens."
    },
    {
        "vector_id": 4828,
        "text": "and output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of"
    },
    {
        "vector_id": 4829,
        "text": "each input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-\ningtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-"
    },
    {
        "vector_id": 4830,
        "text": "signed to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-"
    },
    {
        "vector_id": 4831,
        "text": "essary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized\nattribution score is brighter. In this example, the user attempts to direct the model to output information"
    },
    {
        "vector_id": 4832,
        "text": "that does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second"
    },
    {
        "vector_id": 4833,
        "text": "span \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed.\n2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt"
    },
    {
        "vector_id": 4834,
        "text": "Language Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze"
    },
    {
        "vector_id": 4835,
        "text": "LLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector.\nIn the following, we provide case studies to illustrate how at-"
    },
    {
        "vector_id": 4836,
        "text": "tribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves"
    },
    {
        "vector_id": 4837,
        "text": "comparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 4838,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We"
    },
    {
        "vector_id": 4839,
        "text": "consider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 4840,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,"
    },
    {
        "vector_id": 4841,
        "text": "and consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-"
    },
    {
        "vector_id": 4842,
        "text": "base(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare\nwith this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses."
    },
    {
        "vector_id": 4843,
        "text": "Table 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the"
    },
    {
        "vector_id": 4844,
        "text": "could best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness"
    },
    {
        "vector_id": 4845,
        "text": "of attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with"
    },
    {
        "vector_id": 4846,
        "text": "or unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming\n9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45"
    },
    {
        "vector_id": 4847,
        "text": "Vectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-"
    },
    {
        "vector_id": 4848,
        "text": "tion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each\ninstance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,"
    },
    {
        "vector_id": 4849,
        "text": "2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set."
    },
    {
        "vector_id": 4850,
        "text": "a training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 4851,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well"
    },
    {
        "vector_id": 4852,
        "text": "as accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 4853,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges"
    },
    {
        "vector_id": 4854,
        "text": "2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 4855,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->"
    },
    {
        "vector_id": 4856,
        "text": "Text: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 4857,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the"
    },
    {
        "vector_id": 4858,
        "text": "semantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution"
    },
    {
        "vector_id": 4859,
        "text": "The versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-\nbution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions"
    },
    {
        "vector_id": 4860,
        "text": "are highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the"
    },
    {
        "vector_id": 4861,
        "text": "first aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-\nplexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could"
    },
    {
        "vector_id": 4862,
        "text": "provide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models."
    },
    {
        "vector_id": 4863,
        "text": "Additionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically\n11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module"
    },
    {
        "vector_id": 4864,
        "text": "3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input"
    },
    {
        "vector_id": 4865,
        "text": "sequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words\nfrom the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate"
    },
    {
        "vector_id": 4866,
        "text": "information from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the"
    },
    {
        "vector_id": 4867,
        "text": "static word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 4868,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module"
    },
    {
        "vector_id": 4869,
        "text": "3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward"
    },
    {
        "vector_id": 4870,
        "text": "network obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of\nmemory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,"
    },
    {
        "vector_id": 4871,
        "text": "2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these"
    },
    {
        "vector_id": 4872,
        "text": "key or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes\nof predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By"
    },
    {
        "vector_id": 4873,
        "text": "leveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining"
    },
    {
        "vector_id": 4874,
        "text": "redundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov\net al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific"
    },
    {
        "vector_id": 4875,
        "text": "concept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das"
    },
    {
        "vector_id": 4876,
        "text": "et al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 4877,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they"
    },
    {
        "vector_id": 4878,
        "text": "develop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that"
    },
    {
        "vector_id": 4879,
        "text": "sparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers\nas output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation."
    },
    {
        "vector_id": 4880,
        "text": "3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from"
    },
    {
        "vector_id": 4881,
        "text": "preceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl\ncould be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation"
    },
    {
        "vector_id": 4882,
        "text": "with the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d"
    },
    {
        "vector_id": 4883,
        "text": "and \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and\n\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded"
    },
    {
        "vector_id": 4884,
        "text": "on specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to"
    },
    {
        "vector_id": 4885,
        "text": "poor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-\nposition of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features."
    },
    {
        "vector_id": 4886,
        "text": "However, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers"
    },
    {
        "vector_id": 4887,
        "text": "generated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in\ncases of errors and increases the trustworthiness of the model from users when the outcomes are accurate."
    },
    {
        "vector_id": 4888,
        "text": "In addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts."
    },
    {
        "vector_id": 4889,
        "text": "responses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 4890,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation"
    },
    {
        "vector_id": 4891,
        "text": "tasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation"
    },
    {
        "vector_id": 4892,
        "text": "is to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the\ngeneration of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction"
    },
    {
        "vector_id": 4893,
        "text": "loss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)"
    },
    {
        "vector_id": 4894,
        "text": "I(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote\nthe number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large"
    },
    {
        "vector_id": 4895,
        "text": "models. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,"
    },
    {
        "vector_id": 4896,
        "text": "TracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in\nxi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions."
    },
    {
        "vector_id": 4897,
        "text": "14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only"
    },
    {
        "vector_id": 4898,
        "text": "leverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 4899,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the"
    },
    {
        "vector_id": 4900,
        "text": "diagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a"
    },
    {
        "vector_id": 4901,
        "text": "small subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language\nmodels, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation"
    },
    {
        "vector_id": 4902,
        "text": "to scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the"
    },
    {
        "vector_id": 4903,
        "text": "EK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)\nwhere \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very"
    },
    {
        "vector_id": 4904,
        "text": "large models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods"
    },
    {
        "vector_id": 4905,
        "text": "4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate\nthe semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training"
    },
    {
        "vector_id": 4906,
        "text": "sample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)"
    },
    {
        "vector_id": 4907,
        "text": "i )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the\ndataset Dtrain as the explanation confidence of the samplezi."
    },
    {
        "vector_id": 4908,
        "text": "Compared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical"
    },
    {
        "vector_id": 4909,
        "text": "foundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 4910,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and"
    },
    {
        "vector_id": 4911,
        "text": "verify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the"
    },
    {
        "vector_id": 4912,
        "text": "pretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,\nand the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense"
    },
    {
        "vector_id": 4913,
        "text": "layer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input"
    },
    {
        "vector_id": 4914,
        "text": "in language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation\nof \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the"
    },
    {
        "vector_id": 4915,
        "text": "generation of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)"
    },
    {
        "vector_id": 4916,
        "text": "A ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence\nIEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis"
    },
    {
        "vector_id": 4917,
        "text": "4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in"
    },
    {
        "vector_id": 4918,
        "text": "https://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,\nMistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs."
    },
    {
        "vector_id": 4919,
        "text": "Strategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when"
    },
    {
        "vector_id": 4920,
        "text": "S and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation.\n4.3 Challenges"
    },
    {
        "vector_id": 4921,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based"
    },
    {
        "vector_id": 4922,
        "text": "explanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian"
    },
    {
        "vector_id": 4923,
        "text": "to be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are\nindependent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold"
    },
    {
        "vector_id": 4924,
        "text": "in practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific"
    },
    {
        "vector_id": 4925,
        "text": "training sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not\nbe understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on"
    },
    {
        "vector_id": 4926,
        "text": "the generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For"
    },
    {
        "vector_id": 4927,
        "text": "the embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the"
    },
    {
        "vector_id": 4928,
        "text": "LLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge"
    },
    {
        "vector_id": 4929,
        "text": "neuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to\ndesign LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work."
    },
    {
        "vector_id": 4930,
        "text": "5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human"
    },
    {
        "vector_id": 4931,
        "text": "ethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in\nassessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It"
    },
    {
        "vector_id": 4932,
        "text": "is worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed"
    },
    {
        "vector_id": 4933,
        "text": "to reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 4934,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack."
    },
    {
        "vector_id": 4935,
        "text": "to perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 4936,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to"
    },
    {
        "vector_id": 4937,
        "text": "novel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights"
    },
    {
        "vector_id": 4938,
        "text": "that identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the\ncapabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data"
    },
    {
        "vector_id": 4939,
        "text": "through a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs"
    },
    {
        "vector_id": 4940,
        "text": "into operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as\ndefenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content"
    },
    {
        "vector_id": 4941,
        "text": "generation. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA"
    },
    {
        "vector_id": 4942,
        "text": "prompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the\nrelation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In"
    },
    {
        "vector_id": 4943,
        "text": "addition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-"
    },
    {
        "vector_id": 4944,
        "text": "pus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a\nrich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023)."
    },
    {
        "vector_id": 4945,
        "text": "Interpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg"
    },
    {
        "vector_id": 4946,
        "text": "& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 4947,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias."
    },
    {
        "vector_id": 4948,
        "text": "To achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate"
    },
    {
        "vector_id": 4949,
        "text": "on removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads\nthat significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific"
    },
    {
        "vector_id": 4950,
        "text": "group of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity"
    },
    {
        "vector_id": 4951,
        "text": "5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of\ntoxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented"
    },
    {
        "vector_id": 4952,
        "text": "within LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating"
    },
    {
        "vector_id": 4953,
        "text": "their utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method\ncalled direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-"
    },
    {
        "vector_id": 4954,
        "text": "tune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d"
    },
    {
        "vector_id": 4955,
        "text": "and then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 4956,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-"
    },
    {
        "vector_id": 4957,
        "text": "ies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy"
    },
    {
        "vector_id": 4958,
        "text": "range between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their\noutputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-"
    },
    {
        "vector_id": 4959,
        "text": "ploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components."
    },
    {
        "vector_id": 4960,
        "text": "A clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-\nplicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden"
    },
    {
        "vector_id": 4961,
        "text": "20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019"
    },
    {
        "vector_id": 4962,
        "text": "output and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-\nments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another"
    },
    {
        "vector_id": 4963,
        "text": "work investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations."
    },
    {
        "vector_id": 4964,
        "text": "Building on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 4965,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe"
    },
    {
        "vector_id": 4966,
        "text": "classifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods"
    },
    {
        "vector_id": 4967,
        "text": "5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and\nrepresentations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads"
    },
    {
        "vector_id": 4968,
        "text": "might be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,"
    },
    {
        "vector_id": 4969,
        "text": "casual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-\nworthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are"
    },
    {
        "vector_id": 4970,
        "text": "typically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another"
    },
    {
        "vector_id": 4971,
        "text": "popular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or\nactivations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting"
    },
    {
        "vector_id": 4972,
        "text": "21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which"
    },
    {
        "vector_id": 4973,
        "text": "is then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,"
    },
    {
        "vector_id": 4974,
        "text": "2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among"
    },
    {
        "vector_id": 4975,
        "text": "these approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and\nxn is the actual question. In a standard question-answering scenario, we have the model output asyn ="
    },
    {
        "vector_id": 4976,
        "text": "arg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)"
    },
    {
        "vector_id": 4977,
        "text": "Y\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 4978,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from"
    },
    {
        "vector_id": 4979,
        "text": "inception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with"
    },
    {
        "vector_id": 4980,
        "text": "enhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below."
    },
    {
        "vector_id": 4981,
        "text": "introduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in"
    },
    {
        "vector_id": 4982,
        "text": "both forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This\ncapability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable"
    },
    {
        "vector_id": 4983,
        "text": "to human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between"
    },
    {
        "vector_id": 4984,
        "text": "these concepts), GoT makes the logical connections within complex systems more understandable (Yao\net al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche"
    },
    {
        "vector_id": 4985,
        "text": "et al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally"
    },
    {
        "vector_id": 4986,
        "text": "powerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as"
    },
    {
        "vector_id": 4987,
        "text": "the scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,"
    },
    {
        "vector_id": 4988,
        "text": "2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step"
    },
    {
        "vector_id": 4989,
        "text": "problem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n..."
    },
    {
        "vector_id": 4990,
        "text": "Answer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N"
    },
    {
        "vector_id": 4991,
        "text": "\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper"
    },
    {
        "vector_id": 4992,
        "text": "is a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer\nafter the modification, we believe that the CoT information does not faithfully reflect the true process of the"
    },
    {
        "vector_id": 4993,
        "text": "answer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?"
    },
    {
        "vector_id": 4994,
        "text": "Thoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),"
    },
    {
        "vector_id": 4995,
        "text": "which includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-"
    },
    {
        "vector_id": 4996,
        "text": "vron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 4997,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,"
    },
    {
        "vector_id": 4998,
        "text": "indicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF."
    },
    {
        "vector_id": 4999,
        "text": "Datasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191"
    },
    {
        "vector_id": 5000,
        "text": "Falcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning"
    },
    {
        "vector_id": 5001,
        "text": "process described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly\nbased on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research."
    },
    {
        "vector_id": 5002,
        "text": "GPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-"
    },
    {
        "vector_id": 5003,
        "text": "tion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower\nfidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,"
    },
    {
        "vector_id": 5004,
        "text": "they are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated"
    },
    {
        "vector_id": 5005,
        "text": "evaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%"
    },
    {
        "vector_id": 5006,
        "text": "Vicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately"
    },
    {
        "vector_id": 5007,
        "text": "reflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,\nthe challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making"
    },
    {
        "vector_id": 5008,
        "text": "processes, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the"
    },
    {
        "vector_id": 5009,
        "text": "CoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging\nproblem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models."
    },
    {
        "vector_id": 5010,
        "text": "Regarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading."
    },
    {
        "vector_id": 5011,
        "text": "Recently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting\nEnhancing models with external knowledge can significantly improve the control and interpretability of"
    },
    {
        "vector_id": 5012,
        "text": "decision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in"
    },
    {
        "vector_id": 5013,
        "text": "the world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 5014,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K"
    },
    {
        "vector_id": 5015,
        "text": "max\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 5016,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific"
    },
    {
        "vector_id": 5017,
        "text": "26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 5018,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches"
    },
    {
        "vector_id": 5019,
        "text": "include Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of"
    },
    {
        "vector_id": 5020,
        "text": "decision-making in LLMs. This method leverages real-time information from external databases to produce\nresponses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response"
    },
    {
        "vector_id": 5021,
        "text": "7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution"
    },
    {
        "vector_id": 5022,
        "text": "to mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model\nis anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-"
    },
    {
        "vector_id": 5023,
        "text": "tectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating"
    },
    {
        "vector_id": 5024,
        "text": "7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng\net al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements"
    },
    {
        "vector_id": 5025,
        "text": "in output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization"
    },
    {
        "vector_id": 5026,
        "text": "7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 5027,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,"
    },
    {
        "vector_id": 5028,
        "text": "finding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that"
    },
    {
        "vector_id": 5029,
        "text": "retrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of"
    },
    {
        "vector_id": 5030,
        "text": "output y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;"
    },
    {
        "vector_id": 5031,
        "text": "Gao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,\n2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting"
    },
    {
        "vector_id": 5032,
        "text": "the generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a"
    },
    {
        "vector_id": 5033,
        "text": "significant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the"
    },
    {
        "vector_id": 5034,
        "text": "prompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade"
    },
    {
        "vector_id": 5035,
        "text": "the question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,\nsuch approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more"
    },
    {
        "vector_id": 5036,
        "text": "precise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution"
    },
    {
        "vector_id": 5037,
        "text": "is data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai\net al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these"
    },
    {
        "vector_id": 5038,
        "text": "samples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field."
    },
    {
        "vector_id": 5039,
        "text": "Explanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively\nguides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts"
    },
    {
        "vector_id": 5040,
        "text": "Machine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020)."
    },
    {
        "vector_id": 5041,
        "text": "The extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the\nunderlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations"
    },
    {
        "vector_id": 5042,
        "text": "between input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features."
    },
    {
        "vector_id": 5043,
        "text": "Explanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical\nfeatures (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train"
    },
    {
        "vector_id": 5044,
        "text": "downstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen"
    },
    {
        "vector_id": 5045,
        "text": "data (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang\net al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these"
    },
    {
        "vector_id": 5046,
        "text": "words are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,"
    },
    {
        "vector_id": 5047,
        "text": "2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a\ncertain demographic, thereby potentially promoting fairness in society."
    },
    {
        "vector_id": 5048,
        "text": "8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations"
    },
    {
        "vector_id": 5049,
        "text": "from LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 5050,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate"
    },
    {
        "vector_id": 5051,
        "text": "spurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based"
    },
    {
        "vector_id": 5052,
        "text": "sentiment analysis models through two methods: augmenting the training data with the explanations or\ndistilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang"
    },
    {
        "vector_id": 5053,
        "text": "et al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-"
    },
    {
        "vector_id": 5054,
        "text": "corporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning\nprocess of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-"
    },
    {
        "vector_id": 5055,
        "text": "tions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-"
    },
    {
        "vector_id": 5056,
        "text": "mance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al.\n(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales."
    },
    {
        "vector_id": 5057,
        "text": "This approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges"
    },
    {
        "vector_id": 5058,
        "text": "8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to\ndevelop fair and robust models. Consequently, the crafting process can be both time and energy-consuming."
    },
    {
        "vector_id": 5059,
        "text": "Given these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model"
    },
    {
        "vector_id": 5060,
        "text": "development but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible\nbut incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially"
    },
    {
        "vector_id": 5061,
        "text": "introducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data"
    },
    {
        "vector_id": 5062,
        "text": "required is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 5063,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see"
    },
    {
        "vector_id": 5064,
        "text": "Eqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,"
    },
    {
        "vector_id": 5065,
        "text": "e.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain\npredictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee"
    },
    {
        "vector_id": 5066,
        "text": "e to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models"
    },
    {
        "vector_id": 5067,
        "text": "have long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure\ntextual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously"
    },
    {
        "vector_id": 5068,
        "text": "that facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs"
    },
    {
        "vector_id": 5069,
        "text": "9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that\nare informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032"
    },
    {
        "vector_id": 5070,
        "text": "{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,"
    },
    {
        "vector_id": 5071,
        "text": "GPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 5072,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as"
    },
    {
        "vector_id": 5073,
        "text": "ChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs"
    },
    {
        "vector_id": 5074,
        "text": "can well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al.\n(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted"
    },
    {
        "vector_id": 5075,
        "text": "to textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs"
    },
    {
        "vector_id": 5076,
        "text": "themselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,\nthe self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data"
    },
    {
        "vector_id": 5077,
        "text": "(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which"
    },
    {
        "vector_id": 5078,
        "text": "fine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon\nP5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the"
    },
    {
        "vector_id": 5079,
        "text": "auxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are"
    },
    {
        "vector_id": 5080,
        "text": "crafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without\nexplanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as"
    },
    {
        "vector_id": 5081,
        "text": "explanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of"
    },
    {
        "vector_id": 5082,
        "text": "in-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that\n32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT"
    },
    {
        "vector_id": 5083,
        "text": "can generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more"
    },
    {
        "vector_id": 5084,
        "text": "suitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 5085,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal"
    },
    {
        "vector_id": 5086,
        "text": "with data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 5087,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development"
    },
    {
        "vector_id": 5088,
        "text": "of Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models"
    },
    {
        "vector_id": 5089,
        "text": "such as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-\nsical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018)."
    },
    {
        "vector_id": 5090,
        "text": "Nevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-"
    },
    {
        "vector_id": 5091,
        "text": "vide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,\nfoundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up"
    },
    {
        "vector_id": 5092,
        "text": "with model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;"
    },
    {
        "vector_id": 5093,
        "text": "Yuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then\napply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of"
    },
    {
        "vector_id": 5094,
        "text": "experts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy"
    },
    {
        "vector_id": 5095,
        "text": "is to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck\nfor downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs"
    },
    {
        "vector_id": 5096,
        "text": "high-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for"
    },
    {
        "vector_id": 5097,
        "text": "model designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are\nonly involved during the augmented model training instead of the inference process. For GAMs training,"
    },
    {
        "vector_id": 5098,
        "text": "LLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents"
    },
    {
        "vector_id": 5099,
        "text": "Traditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable\ndesign of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from"
    },
    {
        "vector_id": 5100,
        "text": "Shen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting"
    },
    {
        "vector_id": 5101,
        "text": "candidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 5102,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared"
    },
    {
        "vector_id": 5103,
        "text": "subtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 5104,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the"
    },
    {
        "vector_id": 5105,
        "text": "interpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with"
    },
    {
        "vector_id": 5106,
        "text": "a reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting\nfrom the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but"
    },
    {
        "vector_id": 5107,
        "text": "they resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving"
    },
    {
        "vector_id": 5108,
        "text": "character of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs\nfor designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations"
    },
    {
        "vector_id": 5109,
        "text": "and mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with"
    },
    {
        "vector_id": 5110,
        "text": "human-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise\nthe main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-"
    },
    {
        "vector_id": 5111,
        "text": "generated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question"
    },
    {
        "vector_id": 5112,
        "text": "answering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023).\nTraditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang"
    },
    {
        "vector_id": 5113,
        "text": "et al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced"
    },
    {
        "vector_id": 5114,
        "text": "LLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 5115,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-"
    },
    {
        "vector_id": 5116,
        "text": "learning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-"
    },
    {
        "vector_id": 5117,
        "text": "tainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these\nselected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators."
    },
    {
        "vector_id": 5118,
        "text": "LLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive"
    },
    {
        "vector_id": 5119,
        "text": "explanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed\nthrough their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical"
    },
    {
        "vector_id": 5120,
        "text": "to exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some"
    },
    {
        "vector_id": 5121,
        "text": "researchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results"
    },
    {
        "vector_id": 5122,
        "text": "may not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could"
    },
    {
        "vector_id": 5123,
        "text": "focus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-"
    },
    {
        "vector_id": 5124,
        "text": "tection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration."
    },
    {
        "vector_id": 5125,
        "text": "36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability.\nIn this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)"
    },
    {
        "vector_id": 5126,
        "text": "has a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable"
    },
    {
        "vector_id": 5127,
        "text": "model that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper.\n\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over"
    },
    {
        "vector_id": 5128,
        "text": "the clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted"
    },
    {
        "vector_id": 5129,
        "text": "the stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs\nis pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging"
    },
    {
        "vector_id": 5130,
        "text": "LLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete"
    },
    {
        "vector_id": 5131,
        "text": "transparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 5132,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive"
    },
    {
        "vector_id": 5133,
        "text": "and unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 5134,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their"
    },
    {
        "vector_id": 5135,
        "text": "prominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can"
    },
    {
        "vector_id": 5136,
        "text": "lie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html\n37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a"
    },
    {
        "vector_id": 5137,
        "text": "false sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general"
    },
    {
        "vector_id": 5138,
        "text": "AI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of\nXAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement"
    },
    {
        "vector_id": 5139,
        "text": "Acknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023."
    },
    {
        "vector_id": 5140,
        "text": "preprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023.\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023."
    },
    {
        "vector_id": 5141,
        "text": "arXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022."
    },
    {
        "vector_id": 5142,
        "text": "2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon"
    },
    {
        "vector_id": 5143,
        "text": "series of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951."
    },
    {
        "vector_id": 5144,
        "text": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 5145,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-"
    },
    {
        "vector_id": 5146,
        "text": "etry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and"
    },
    {
        "vector_id": 5147,
        "text": "Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In\nInternational Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating"
    },
    {
        "vector_id": 5148,
        "text": "layers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023."
    },
    {
        "vector_id": 5149,
        "text": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05."
    },
    {
        "vector_id": 5150,
        "text": "2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving"
    },
    {
        "vector_id": 5151,
        "text": "language models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in\nNeural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language"
    },
    {
        "vector_id": 5152,
        "text": "models with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley."
    },
    {
        "vector_id": 5153,
        "text": "Rationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,\n2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-"
    },
    {
        "vector_id": 5154,
        "text": "esty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022."
    },
    {
        "vector_id": 5155,
        "text": "academic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu"
    },
    {
        "vector_id": 5156,
        "text": "outcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical\nInformatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning"
    },
    {
        "vector_id": 5157,
        "text": "design spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-"
    },
    {
        "vector_id": 5158,
        "text": "shot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b.\nYufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable"
    },
    {
        "vector_id": 5159,
        "text": "recommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models"
    },
    {
        "vector_id": 5160,
        "text": "in digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language\nmodel with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022."
    },
    {
        "vector_id": 5161,
        "text": "40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained"
    },
    {
        "vector_id": 5162,
        "text": "transformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In"
    },
    {
        "vector_id": 5163,
        "text": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey"
    },
    {
        "vector_id": 5164,
        "text": "of the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023."
    },
    {
        "vector_id": 5165,
        "text": "Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 5166,
        "text": "information processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 5167,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint"
    },
    {
        "vector_id": 5168,
        "text": "arXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 5169,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of"
    },
    {
        "vector_id": 5170,
        "text": "the ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 5171,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text"
    },
    {
        "vector_id": 5172,
        "text": "classification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac"
    },
    {
        "vector_id": 5173,
        "text": "Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv\npreprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-"
    },
    {
        "vector_id": 5174,
        "text": "national Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint"
    },
    {
        "vector_id": 5175,
        "text": "arXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias\nBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the"
    },
    {
        "vector_id": 5176,
        "text": "16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019."
    },
    {
        "vector_id": 5177,
        "text": "Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,"
    },
    {
        "vector_id": 5178,
        "text": "Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021."
    },
    {
        "vector_id": 5179,
        "text": "Qing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023.\nSai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language"
    },
    {
        "vector_id": 5180,
        "text": "model pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting"
    },
    {
        "vector_id": 5181,
        "text": "data evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model"
    },
    {
        "vector_id": 5182,
        "text": "editing with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising"
    },
    {
        "vector_id": 5183,
        "text": "differences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\nRuining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of"
    },
    {
        "vector_id": 5184,
        "text": "the eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998."
    },
    {
        "vector_id": 5185,
        "text": "43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 5186,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-"
    },
    {
        "vector_id": 5187,
        "text": "jay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate"
    },
    {
        "vector_id": 5188,
        "text": "quality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large"
    },
    {
        "vector_id": 5189,
        "text": "language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can"
    },
    {
        "vector_id": 5190,
        "text": "large language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-"
    },
    {
        "vector_id": 5191,
        "text": "durally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019."
    },
    {
        "vector_id": 5192,
        "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023."
    },
    {
        "vector_id": 5193,
        "text": "arXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023."
    },
    {
        "vector_id": 5194,
        "text": "PMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018"
    },
    {
        "vector_id": 5195,
        "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022."
    },
    {
        "vector_id": 5196,
        "text": "Conference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 5197,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280."
    },
    {
        "vector_id": 5198,
        "text": "PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-"
    },
    {
        "vector_id": 5199,
        "text": "tending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on\nNews Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large"
    },
    {
        "vector_id": 5200,
        "text": "language models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024."
    },
    {
        "vector_id": 5201,
        "text": "Artificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea.\nA mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general"
    },
    {
        "vector_id": 5202,
        "text": "intelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023."
    },
    {
        "vector_id": 5203,
        "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-"
    },
    {
        "vector_id": 5204,
        "text": "jiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD"
    },
    {
        "vector_id": 5205,
        "text": "Conference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a.\nJiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b."
    },
    {
        "vector_id": 5206,
        "text": "arXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The"
    },
    {
        "vector_id": 5207,
        "text": "dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 5208,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018."
    },
    {
        "vector_id": 5209,
        "text": "pp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 5210,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e."
    },
    {
        "vector_id": 5211,
        "text": "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g."
    },
    {
        "vector_id": 5212,
        "text": "Tom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does\ncircuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting"
    },
    {
        "vector_id": 5213,
        "text": "of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024."
    },
    {
        "vector_id": 5214,
        "text": "Computational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021."
    },
    {
        "vector_id": 5215,
        "text": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b."
    },
    {
        "vector_id": 5216,
        "text": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,\nYu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d."
    },
    {
        "vector_id": 5217,
        "text": "arXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter"
    },
    {
        "vector_id": 5218,
        "text": "Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024."
    },
    {
        "vector_id": 5219,
        "text": "Yun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,"
    },
    {
        "vector_id": 5220,
        "text": "Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022."
    },
    {
        "vector_id": 5221,
        "text": "Alessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023."
    },
    {
        "vector_id": 5222,
        "text": "Papers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory"
    },
    {
        "vector_id": 5223,
        "text": "in a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023."
    },
    {
        "vector_id": 5224,
        "text": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-\ntations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods"
    },
    {
        "vector_id": 5225,
        "text": "in Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing"
    },
    {
        "vector_id": 5226,
        "text": "deep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering"
    },
    {
        "vector_id": 5227,
        "text": "with human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,"
    },
    {
        "vector_id": 5228,
        "text": "Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."
    },
    {
        "vector_id": 5229,
        "text": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-"
    },
    {
        "vector_id": 5230,
        "text": "bastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 5231,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from"
    },
    {
        "vector_id": 5232,
        "text": "natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 5233,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,"
    },
    {
        "vector_id": 5234,
        "text": "and Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 5235,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019."
    },
    {
        "vector_id": 5236,
        "text": "Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 5237,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and"
    },
    {
        "vector_id": 5238,
        "text": "capacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 5239,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016."
    },
    {
        "vector_id": 5240,
        "text": "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 5241,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021."
    },
    {
        "vector_id": 5242,
        "text": "Sandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 5243,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996."
    },
    {
        "vector_id": 5244,
        "text": "Yan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 5245,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017."
    },
    {
        "vector_id": 5246,
        "text": "Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 5247,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing"
    },
    {
        "vector_id": 5248,
        "text": "for sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE"
    },
    {
        "vector_id": 5249,
        "text": "transactions on neural networks and learning systems, 32(11):4793\u20134813, 2020.\n51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
    },
    {
        "vector_id": 5250,
        "text": "language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what"
    },
    {
        "vector_id": 5251,
        "text": "they think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:"
    },
    {
        "vector_id": 5252,
        "text": "Debugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in"
    },
    {
        "vector_id": 5253,
        "text": "neural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the\nblack box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on"
    },
    {
        "vector_id": 5254,
        "text": "Empirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language"
    },
    {
        "vector_id": 5255,
        "text": "understanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-\nity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label"
    },
    {
        "vector_id": 5256,
        "text": "words are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c."
    },
    {
        "vector_id": 5257,
        "text": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons"
    },
    {
        "vector_id": 5258,
        "text": "in pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically"
    },
    {
        "vector_id": 5259,
        "text": "generated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024.\nZimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial"
    },
    {
        "vector_id": 5260,
        "text": "examples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023."
    },
    {
        "vector_id": 5261,
        "text": "preprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 5262,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing"
    },
    {
        "vector_id": 5263,
        "text": "competing explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021."
    },
    {
        "vector_id": 5264,
        "text": "Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing\nand interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024."
    },
    {
        "vector_id": 5265,
        "text": "53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020."
    },
    {
        "vector_id": 5266,
        "text": "17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the"
    },
    {
        "vector_id": 5267,
        "text": "Association for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in"
    },
    {
        "vector_id": 5268,
        "text": "transformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a.\nYue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and"
    },
    {
        "vector_id": 5269,
        "text": "Xiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 5270,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a."
    },
    {
        "vector_id": 5271,
        "text": "6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 5272,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023."
    },
    {
        "vector_id": 5273,
        "text": "6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 5274,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of"
    },
    {
        "vector_id": 5275,
        "text": "Information Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 5276,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024."
    },
    {
        "vector_id": 5277,
        "text": "Xinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:"
    },
    {
        "vector_id": 5278,
        "text": "Assessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies"
    },
    {
        "vector_id": 5279,
        "text": "for bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b."
    },
    {
        "vector_id": 5280,
        "text": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a."
    },
    {
        "vector_id": 5281,
        "text": "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    },
    {
        "vector_id": 5282,
        "text": "Explainable Information Retrieval: A Survey\nAVISHEK ANAND and LIJUN LYU,Delft University of Technology, The Netherlands\nMAXIMILIAN IDAHL, YUMENG WANG, JONAS WALLAT, and ZIJIAN ZHANG,L3S Research\nCenter, Leibniz University Hannover, Germany\nExplainable information retrieval is an emerging research area aiming to make transparent and trustworthy\ninformation retrieval systems. Given the increasing use of complex machine learning models in search systems,\nexplainability is essential in building and auditing responsible information retrieval models. This survey fills a\nvital gap in the otherwise topically diverse literature of explainable information retrieval. It categorizes and\ndiscusses recent explainability methods developed for different application domains in information retrieval,\nproviding a common framework and unifying perspectives. In addition, it reflects on the common concern of\nevaluating explanations and highlights open challenges and opportunities.\n1 INTRODUCTION"
    },
    {
        "vector_id": 5283,
        "text": "1 INTRODUCTION\nInformation retrieval (IR) systems are one of the most user-centric systems on the Web, in digital\nlibraries, and enterprises. Search engines can be general-purpose (e.g., Web search) to specialized\nexpert systems that are geared towards expert consumption or support, including legal and patent\nretrieval IR [22], historical search [55, 56], and scholarly search [49, 116]. On the one hand, riding\non the recent advances of complex machine learning (ML) models trained on large amounts of\ndata, IR has seen impressive performance gains over classical models [ 73]. On the other hand,\ncomplex models also tend to be opaque and less transparent than their classical and arguably\nsimpler counterparts. Therefore, towards an important goal of ensuring a reliable and trustworthy\nIR systems, recent years have seen increased interest in the area of explainable information retrieval\n(ExIR).\n1.1 Motivation"
    },
    {
        "vector_id": 5284,
        "text": "(ExIR).\n1.1 Motivation\nFirstly, in IR, there has been sufficient evidence of how user interaction data from search engines\ncan be a source of biases, especially associated with gender and ethnicity [ 13, 83, 100]. When\nundetected and unidentified, the users of an IR system too are exposed to stereotypical biases that\nreinforce known yet unfair prejudices. Secondly, model retrieval models based on transformer-style\nover-parameterized models can be brittle and sensitive to small adversarial errors [132]. Recently\ndeveloped inductive biases, pre-training procedures, and transfer learning practices might lead\nthese statistical over-parameterized models to learn shortcuts [44]. Consequently, shortcuts that do\nnot align with human understanding results in learning patterns that areright for the wrong reasons .\nFinally, expert users using specialized search systems \u2013 in legal search, medicine, journalism,"
    },
    {
        "vector_id": 5285,
        "text": "and patent search \u2013 need control, agency, and lineage of the search results. For all the above\nIR-centric reasons, among many other general reasons \u2013 like utility for legal compliance, scientific\ninvestigation, and model debugging \u2013 the field of ExIR provides the tools/primitives to examine\nlearning models and the capability to build transparent IR systems.\n1.2 The Landscape of Explainable Information Retrieval\nAlthough interpretability in IR is a fairly recent phenomenon, there has been a large amount of\ngrowing yet unorganized work that covers many tasks and aspects of data-driven models in IR.\nThis survey aims to collect, organize and synthesize the progress in ExIR in the last few years. ExIR\nAuthors\u2019 addresses: Avishek Anand, avishek.anand@tudelft.nl; Lijun Lyu, L.Lyu@tudelft.nl, Delft University of Technology,\nP.O. Box 1212, Delft, The Netherlands; Maximilian Idahl, idahl@l3s.de; Yumeng Wang, wang@l3s.de; Jonas Wallat, wallat@"
    },
    {
        "vector_id": 5286,
        "text": "l3s.de; Zijian Zhang, zzhang@l3s.de, L3S Research Center, Leibniz University Hannover, Appelstr. 9a, Hannover, Lower\nSaxony, Germany.\narXiv:2211.02405v1  [cs.IR]  4 Nov 2022 2 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nExplainable IR\nPost-hoc\nFeature\nAttribution \u00a73\nFree-text\nExplanations \u00a74\nAdversarial\nExamples \u00a75\nGrounding to IR Properties\nAxiomatic\nAnalysis \u00a76\nProbing \u00a77\nInterpretable by-design\nExplainable\nArchitectures \u00a78\nRationale-based\nMethods \u00a79\nFig. 1. Categorization of explainable IR approaches, where \u00a7 indicates the section the approach is discussed.\nhas quite a diverse landscape owing to the continued and sustained interest in the last few years.\nThe initial approaches in ExIR were adaptations of widely popular feature-attribution approaches\n(e.g., LIME [102] and SHAP\u2019s [76]). However, in the following years, there has been a multitude\nof approaches that tackle specific problems in IR. We cover a wide range of approaches, from"
    },
    {
        "vector_id": 5287,
        "text": "post-hoc approaches (cf. Sections 3, 4 and 5), grounding to axiomatic approaches (cf. Section 6), to\ninterpretable-by-design methods (cf. Section 8 and Section 9).\n1.3 Methodology and Scope\nBefore we started our literature review, we needed to collect a corpus of relevant papers for ExIR\nand delineate the boundaries of the review.\n1.3.1 Corpus Creation. We started with very first works in ExIR (e.g., [29, 112, 113]), to build up an\ninitial pool of papers. We did then forward search from this initial set of papers that mention terms\n\u201c(explain* OR interpretab* OR explanation* OR transparen*)\u201d AND \u201c(retriev* OR rank*\u201d. Secondly, we\nlimited our search to articles published in the past five years (2018 \u2013 2022) to provide a representative\nwindow into current best practices that have emerged since the inception of the earliest works in\nExIR in the following IR venues \u2013 ACM Special Interest Group on Information Retrieval (SIGIR),"
    },
    {
        "vector_id": 5288,
        "text": "International Conference on the Theory of Information Retrieval (ICTIR), International Conference\non Web Search and Data Mining(WSDM), Conference on Information and Knowledge Management\n(CIKM), the ACM Web Conference (TheWebConf). In total, after filtering, we ended up with 68\npapers that we consider in this review that are partially relevant. A subset of 32 papers of those\npartially relevant papers find more detailed treatment in this survey.\n1.3.2 Scope. We note that many of the methods in ExIR have methodological overlap with those\ninvented in ML, natural language processing (NLP), and recommender systems (RS) communities. In\nfact, most of the approaches in ExIR are based on seminal papers in these communities. We only focus\non core-IR issues in this survey and, wherever possible, clearly spell out the distinctions from similar\napproaches in NLP, RS and ML in general. Rationale-based models have been heavily investigated"
    },
    {
        "vector_id": 5289,
        "text": "in NLP. We cover only the methods popularized in IR-centric or venues. Our survey focuses on\nrationale-based models, i.e., document-ranking tasks, in learning-to-rank (LTR), and tasks that rely\non a retrieval component. Also, RS have a lot of work and even surveys in explainability [ 145].\nWe only survey those approaches that are useful for query modeling in query-based systems. The\npapers on the topics of personalization search or explainable RS, although they can be considered as\nuser modeling applications of ExIR, were not selected due to either lack of specific interpretability\nmethods or being more suitable to be classified into a relatively independent field of study. We also\nexclude IR approaches dealing with image or multi-modal data.\nPre-print Explainable Information Retrieval: A Survey 3\n2 NOTIONS AND CATEGORIZATION\nWe start the survey by first introducing the notions and terminologies that are commonly used"
    },
    {
        "vector_id": 5290,
        "text": "in ExIR. Note that most of the terminologies in ExIR are adapted from the general area of inter-\npretable machine learning [82], explainable vision [107], natural language processing [117], and\nrecommendation systems [145]. We harmonize the differences in the categorizations used in these\nareas to distill a specific method-centric classification of all approaches used in ExIR in Figure 1.\nOur classification permeates the binary divides of post-hoc and interpretable-by-design approaches\nby covering IR-specific dimensions of axiomatic characterization and free-text explanations.\n2.1 Notions in Explainable Information Retrieval\nExplanations are the outputs of an interpretable machine learning procedure or an interpretability\nmethod. In general machine learning, explanations vary in scope and type. The scope of an explana-\ntion can be a single instance or the entire dataset. The type of explanation refers to the style or"
    },
    {
        "vector_id": 5291,
        "text": "form of the explanation. Notions in ExIR share commonalities for the most part with general XAI.\nHowever, there are some variations due to different tasks, inputs, and output types in IR. In the\nfollowing, we describe these IR-specific notions pertaining to explainability.\n2.1.1 Local vs global interpretability. Local interpretability refers to per-instance interpretability.\nFor the task of document ranking, an individual query is usually considered as a single instance\neven though multiple decisions might be involved (e.g., multiple query-document pairs and multiple\npreference pairs). Specifically, local interpretability aims to explain the model decisions in the\nlocality of a specific query. On the other hand, global interpretability refers to the case when there\nis no distinction across instances/queries in terms of model parameters, input spaces, etc.\n2.1.2 Pointwise, Pairwise, Listwise. Ranking models output a ranked candidate list for a given"
    },
    {
        "vector_id": 5292,
        "text": "query. Therefore, the explanation of pointwise methods can only explain the models\u2019 decision of a\nsingle element in the list; while pairwise methods intend to explain the model\u2019s preference of a\ncandidate pair. The explanation of listwise methods, however, aims to cover all individual decisions\nin the entire ranking list.\n2.1.3 Type of Explanations. A model decision can be explained differently in terms of input\nfeatures, training data, model parameters, or human-understandable decision structures. When\nan explanation method measures the contribution of each feature in the input instance leading\nto a specific decision, the generated explanation can be a feature attribution . On the one hand,\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\nhand, they can also be presented as boolean or hard masks where a feature is either present or\nabsent in the explanation. An explanation is understandable to humans or users based not only"
    },
    {
        "vector_id": 5293,
        "text": "if the feature space is understandable but also if the explanation is small. An attribution over a\nfeature space of hundreds of dimensions is hard to interpret, even if it is over words and phrases\nthat are themselves understandable. In IR, we typically deal with long text documents, and using\nfeature attributions and sparsity is a key design criterion. Explanation procedures can enforce\nsparsity constraints to have short extractive attributions or generate a small set of words or terms\ncalled free-text explanation. Unlike feature-based explanations, explanations can be in terms of\ninput instances. Contrastive explanations are such types of explanations where the objective is to\ngenerate example instances with minor differences from the input example but with contrasting\npredictions. The value of contrastive examples as explanations is grounded in social sciences [81].\nTherefore, using contrastive explanations to understand model behavior is one crucial aspect of"
    },
    {
        "vector_id": 5294,
        "text": "gaining more transparency into the model\u2019s decision-making process. Finally, rules are also one of\nthe prevalent explanations. We denote the explicit decision-making rules ashard-rule, such as a\nPre-print 4 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\nFig. 2. Example ranking result showing top-5 ranked documents with predicted relevance scores for the\nquery \u201ccan you do yoga from a chair\u201d. Query and Documents are selected from TREC-DL (2021) [28]"
    },
    {
        "vector_id": 5295,
        "text": "and MS MARCO [86], respectively.\ndecision-tree path and the well-established IR principles (axioms). On the other hand, a soft-rule\nrefers to those that partially impact the model decision.\n2.2 Post-hoc Interpretability\nPost-hoc interpretability methods explain the decisions of already trained machine learning models.\nPost-hoc approaches are either model-agnostic (black-box) where the interpretability approach has\nno access to the trained model parameters [77, 102], or model introspective (white-box) which have\nfull access to the parameters of the underlying model [110, 120] or data [64]. In this survey, we will\nreview approaches for both white-box and black-box settings. Moreover, specifically in IR, we make\na distinction between a strongly- and weakly-agnostic setting depending on if we are provided\nonly access to a ranking of documents or also the score of a document given a query. Most of the\nwork in the existing literature only considers our definition of a weakly agnostic model."
    },
    {
        "vector_id": 5296,
        "text": "2.2.1 Methods of post-hoc interpretability. A dominant class of post-hoc explanation approaches\noutput what is known as feature attributions or saliency maps. Most of the white-box approaches\nadapt gradient-based attribution approaches with task-specific calibrations. For black-box ap-\nproaches, explanation methods use words/sentences/passages in text retrieval and ranking, and\nnumeric and categorical features in LTR for modeling the feature space. We discuss methods in\ndetail about feature attribution in Section 3, free-text explanations in Section 4, and adversarial\nexamples in Section 5.\n2.3 Interpretability by Design\nA common problem with post-hoc approaches is that it is often unclear how much the model\nbehavior is indeed understood. In fact, Rudin [106] advocates using Interpretable-by-design (IBD)\nmodels as much as possible specifically for high-stakes decision-making. However, building an"
    },
    {
        "vector_id": 5297,
        "text": "IBD model that is indeed fully transparent and meanwhile maintaining competitive performance is\nchallenging, especially for complex non-linear and over-parameterized neural models. We note\nthat most proposals in literature are partially interpretable, instead of exhibiting full transparency.\n2.3.1 Explainable by Architecture vs Rationales. Many approaches brand themselves as IBD methods,\nwhen in fact they are partially interpretable. On one hand, some methods have only interpretable\nfeature interactions and score compositions [42, 63]. On the other hand, methods choose extractive\ninput sequences as explanations while the models themselves are non-interpretable [70, 146]. In\nthis survey, we firstly subdivide the family of IBD approaches by explainable by architecture (cf.\nSection 8) where components of the model architecture are partially or fully interpretable. Secondly,\nIBD methods that enforce input feature sparsity are detailed in Section 9 asrationale-based methods."
    },
    {
        "vector_id": 5298,
        "text": "Pre-print Explainable Information Retrieval: A Survey 5\n2.4 Grounding to Information Retrieval Principles\nThere is a long-standing history of building text ranking models in IR. Most of the well-known and\nrobust approaches for understanding relevance are based on establishing closed-formed relevance\nequations based on probabilistic [92] or axiomatic foundations [16]. A possible improve way to\nimprove the transparency of data-driven complex ML models is to determine if the learned models\nadhere to well-understood IR principles. Towards this, there are two streams of research efforts\nthat attempt to ground the predictions of learned ranking models into axioms or probing models\nfor known relevance factors of matching, term proximity, and semantic similarity. We review these\napproaches in Sections 6 and 7. Note that the methods utilizing IR principles can be implemented\nin both post-hoc and IBD manner.\n2.5 Evaluation of Explanations"
    },
    {
        "vector_id": 5299,
        "text": "2.5 Evaluation of Explanations\nEvaluation of interpretability or explainability approaches has long been an arduous and challenging\ntask. There is no agreed-upon set of experimental protocols leading to various design decisions due\nto a lack of ground truths and differences in the perceived utility, stakeholders, and forms. Doshi-\nVelez and Kim [32] classify evaluation approaches as application-grounded, human-grounded, and\nfunctionality-grounded. The difference between application- and human-grounded evaluations is\nusing experts and non-experts as evaluation subjects. Functionality-grounded evaluation does not\ninvolve humans and relies on a closed-form definition of interpretability that serves as a proxy to\nevaluate the explanation quality. We introduce the following three classes of evaluation strategies\nemployed in ExIR.\n2.5.1 Human evaluation. Most current papers in ExIR involve human evaluation, but primarily do"
    },
    {
        "vector_id": 5300,
        "text": "not differentiate between expert- and non-expert users. Evaluations can be simply anecdotal. In\nthis case, example explanations are shown to users, and typically binary judgments regarding the\ngoodness of the explanations are elicited. A surprising number of ExIR papers claim interpretability\nof their approaches but conduct simple anecdotal experiments. A more fine-grained human evalua-\ntion is to ask users to solve specific tasks with the assistance of explanations. Such an approach\nevaluates the utility of the explanations or answers the question \u2013 how helpful are the explanations\nin the context of a given application?\n2.5.2 Fidelity-based Evaluation. Fidelity measures to which degree the explanations can replicate\nthe underlying model decisions. Fidelity is measured by generating a second prediction and com-\nputing the agreement between the actual and the generated prediction. The second prediction could"
    },
    {
        "vector_id": 5301,
        "text": "be derived from either 1) using a part of the input, 2) using a surrogate model, or 3) generating\na counterfactual or adversarial example. A more fine-grained category of fidelity can include\nevaluating the comprehensiveness, sufficiency, etc. We will further discuss the detailed metrics when\nwe come to specific methods.\n2.5.3 Reference-based Evaluation. The lack of ground truths for explanations is a central problem\nin explainable AI. Whenever the ground-truth explanations are available, we can use them as the\nreference to compare with the generated explanations. In case of a lack of ground truth explanations,\nsome methods choose a well-understood and fully explainable/transparent model as a reference\nmodel. In such cases, we can evaluate the truthfulness of the explanation methods by comparing\nthe explanations generated by the reference model and the explanation method.\n3 FEATURE ATTRIBUTION\nFeature attribution methods, also known as feature importance or saliency methods, typically"
    },
    {
        "vector_id": 5302,
        "text": "generate explanations for individual predictions by attributing the model output to the input\nPre-print 6 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nfeatures. A scalar representing the importance is assigned to each input feature or groups of input\nfeatures. These scores are then commonly visualized using a heatmap or a bar chart, informing\nthe user about which features the model\u2019s prediction is most sensitive to. Figure 3 demonstrates\nexample feature attributions for the top-2 ranked documents, following the example from Figure 2.\nFeature attribution methods have been found to be the most popular explanation technique and\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 5303,
        "text": "Fig. 3. A fictive example using a heatmap to visualize feature attributions for the top-2 ranked documents for\nthe query \u201ccan you do yoga from a chair\u201d. Feature importance is highlighted in orange.\nare used in many domains [11]. However, as is common for interpretation techniques, most feature\nattribution methods were originally designed to explain the predictions of classification models.\nRecent work explores how such methods can be applied or adapted to explain the output of\nranking models, where feature importance scores can be computed for the query or document\ninput individually, or for both, as shown in Figure 3. Following our categorization (Section 2), we\ndifferentiate between model-agnostic and model-introspective feature attribution methods.\n3.1 Model-agnostic Feature Attribution\nA variety of feature attribution methods generate explanations in a model-agnostic way by perturb-\ning input features and observing the change in the model output. The underlying model is treated"
    },
    {
        "vector_id": 5304,
        "text": "as a black box.\n3.1.1 Feature Ablation. Feature ablation is a simple perturbation-based approach to computing\nimportance scores. Individual (or groups of) input features are removed one at a time, and an\nimportance score is assigned based on the observed difference between the model predictions.\nTo interpret a BERT-based ranking model, Qiao et al. [95] compute the importance of tokens\nthrough feature ablation. To produce feature importance scores, they compare the ranking score of\nan unmodified document with the ranking score for the same document when removing a randomly\nchosen input token. Specifically, they only remove input tokens corresponding to regular words\nand keep all tokens that are special tokens or correspond to stopwords. They find that the ranking\nscore produced by a BERT model depends on only a few tokens in each document. The ranking\nscore often decreases significantly when these tokens are removed. When manually examining the"
    },
    {
        "vector_id": 5305,
        "text": "important tokens, the authors find that they often correspond to exact match terms, i.e., terms that\nalso appear in the input query, and terms in close semantic context. In contrast, when examining\ntoken importance scores for a neural ranker based on convolutions and interactions [ 30] that\nsoft-matches n-grams for ad-hoc search, the most important terms appear to be rather loosely\nrelated to the input query.\n3.1.2 Surrogate Models. Local Interpretable Model-agnostic Explanations (LIME) [102] is an in-\nterpretability method that generates explanations by training a surrogate model on a dataset of\nperturbed samples to locally approximate the behavior of the underlying black-box model. Typically,\na linear model, preferably sparse, is chosen as the interpretable surrogate model since the weights\ndirectly specify the importance of each feature. Using LIME to generate feature attributions, Singh\nPre-print Explainable Information Retrieval: A Survey 7"
    },
    {
        "vector_id": 5306,
        "text": "and Anand [112] propose EXS, an explainable search system that provides explanations to users\nthrough feature attribution. Specifically, EXS aims to provide information on three questions: 1)\nWhy is a document relevant to the query, 2) Why is a document ranked higher than another\ndocument, and 3) What the intent of the query is according to the ranker? LIME is designed to\nexplain the output of a classifier, and EXS casts the output of a pointwise ranker into a classification\nproblem by transforming query-document scores into class probabilities. A binary classification\nproblem is created by considering the top-\ud835\udc58documents in an input ranking as relevant and the rest\nas irrelevant, essentially considering document ranking as a classification problem where the black\nbox ranker is considered as a classifier. Polley et al. [91] compare EXS with their evidence-based\nexplainable document search system, ExDocS, which performs reranking using interpretable fea-"
    },
    {
        "vector_id": 5307,
        "text": "tures. In a user study, they found that EXS is on par with the ExDocS system in completeness and\ntransparency metrics, although users rated ExDocS as more interpretable compared to EXS. At the\nsame time, the use of ExDocS resulted in a drop in ranking performance, whereas the use of EXS\ndoes not affect performance at all.\nSimilarly, Verma and Ganguly[125] adapt LIME to create locally interpretable ranking model\nexplanations (LIRME). In contrast to EXS, LIRME trains the local surrogate model directly on\nthe query-document scores and does not transform them into class probabilities. Instead, they\nexperiment with different strategies to sample documents in the neighborhood of the document\nto be explained. In their experiments, they create explanations for the output of a Jelinek-Mercer\nsmoothed language model on the TREC-8 dataset and find that uniform or TF-IDF-biased term\nreplacement strategies produce better explanations than replacement strategies that use term\nposition information."
    },
    {
        "vector_id": 5308,
        "text": "position information.\nInstead of training a local surrogate model to generate explanations for individual examples,\nSingh and Anand [111] distill an already trained black-box LTR model into an interpretable global\nsurrogate model that is used to generate explanations. This global surrogate model only operates on\nthe interpretable subset of features and is trained to mimic the predictions of the black-box ranker.\nFor training, they create numerous artificial training examples In their experiments, they validate\nwhether it is possible to train an interpretable model that approximates a complex model. On the\nLTR datasets [96] they find that a faithful interpretable ranker can only be learned for certain query\nlocalities. This showcases the limitation that simple models, even when trained with a much larger\nquantity of training data, are not able to faithfully explain all localities of the decision boundary of\na complex model and that using local surrogate models can be advantageous."
    },
    {
        "vector_id": 5309,
        "text": "3.1.3 Searching for Explanations. An alternative to the above approaches is to search the space of all\npossible explanations, optimizing for a metric of choice. For LTR models, Singh et al. [115] propose\na simple, yet effective greedy search-based approach to find explanations. Their approach aims\nto find a subset of explanatory features that maximizes two measures, validity and completeness.\nThe validity of an explanation is defined as the amount of predictive capacity contained in a subset\nof explanatory features. The idea is that the explanatory features should be sufficient to produce\nthe original output ranking. In fact, this measure aligns with the sufficiency metric introduced\nby DeYoung et al. [31]. The completeness metric measures whether removing explanatory features\nfrom the input significantly changes the output. When all explanatory features are removed, it\nshould not be possible to produce the original output ranking. Kendall\u2019s tau rank correlation"
    },
    {
        "vector_id": 5310,
        "text": "measures differences in output rankings; the underlying model is treated as a black-box.\nPre-print 8 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n3.2 Model-introspective Feature Attribution\nIn contrast to model-agnostic methods, model-introspective feature attribution methods require\nwhite-box access to the model being explained. Model-introspective methods typically rely on\ngradients or other properties of the model to compute feature importance scores.\n3.2.1 Gradient-based Methods. Many feature attribution methods generate an explanation by\ncomputing the gradient with respect to the input features. This gradient reflects how a small\nchange in the input features affects the prediction. The vanilla gradient method can produce\nnoisy explanations and suffers from a saturation problem. A variety of methods aim to remedy\nthese issues. For example, Integrated Gradients [120] accumulates gradients on a path between"
    },
    {
        "vector_id": 5311,
        "text": "a baseline input and the actual input. While this resolves the saturation problem, the baseline\ninput is a hyperparameter to be chosen carefully. It is unclear what baseline is best, and each\nbaseline makes assumptions about the distribution of the data and the concept of missingness\nin the feature space [119]. Other gradient-based feature attribution methods, such as Layer-wise\nRelevance Propagation [4], Guided Backpropagation [ 118], or DeepLIFT [ 110] back-propagate\ncustom relevance scores using modified, sometimes layer-specific, rules.\nFernando et al. [39] apply DeepSHAP [77], a combination of SHAP [77] and DeepLIFT [110], to\nneural retrieval models. Specifically, they investigate the sensitivity of the explanations to different\nchoices for constructing a baseline input document. Generating explanations for a subset of queries\nfrom the TREC Robust04 test collection and the corresponding top-3 ranked documents, they find"
    },
    {
        "vector_id": 5312,
        "text": "that the explanations are indeed sensitive to the baseline input. The DeepSHAP explanations are\nalso compared to explanations produced by LIME, and while for some baseline inputs there is high\noverlap in the most important features, there is a lack of overlap for others.\nPurpura et al. [94] use simple gradient-based feature attribution to find the most important\nfeatures used by LTR models. They generate a saliency map for each instance in a training dataset\nand select feature groups by thresholding the normalized importance values. Feature selection\nis then performed by counting how often each feature group is considered important across all\nextracted saliency maps.\nZhan et al. [141] use Integrated Gradients [120] to obtain feature attributions for a BERT-based\nranking model. As a baseline input, they create an empty query and an empty document input by\nreplacing the corresponding tokens with the special padding token \u201c[PAD]\u201d. An example of feature"
    },
    {
        "vector_id": 5313,
        "text": "attributions for BERT-style input is visualized in Figure 4.\n[CLS] can you do yoga from a chair [SEP] 10 Yoga Poses You Can Do in a Chair | Chair pose ... [SEP]\nFig. 4. Example visualization of feature attributions for a single query-document pair using the BERT-style\ninput format, which is \u201c[CLS] query [SEP] document [SEP]\u201d. Important tokens are highlighted in orange.\n3.2.2 Attention-based Methods. Instead of using gradients, attention-based feature attribution\nmethods use the attention weights contained in attention layers, which are a core building block of\ntransformer models. The attention weights can be used to explain what part of the input a model\nattends to when making a prediction, for example, by visualizing the attention weights at certain\nlayers [126]. However, whether attention weights actually provide explanations is subject to an\nongoing debate [8, 12].\nQiao et al. [95] analyze the learned attentions of BERT-based ranking models, using attention"
    },
    {
        "vector_id": 5314,
        "text": "weights to measure the importance of features. They group input tokens into three categories, as\nvisualized in Figure 5: Regular Words, Stopwords, and Markers, which are the special tokens \u201c[CLS]\u201d\nand \u201c[SEP]\u201d. In their experiments on the MS MARCO passage reranking dataset [86], they find that\nPre-print Explainable Information Retrieval: A Survey 9\nmarker tokens receive the highest attention. The importance of the marker tokens is confirmed\nby observing a strong decrease in model performance when they are removed from the inputs.\nStopwords appear to be as important as regular words; however, removing them does not appear\nto affect the ranking performance. Additionally, they observe that the attention scores spread more\nuniformly across the input sequence in deeper layers of BERT, as the embeddings become more\ncontextualized.\nRegular Words-\nStopwords-\nSpecial Tokens- \nAvg. Importance\nFig. 5. Example bar chart visualization of feature attributions for different groups of tokens."
    },
    {
        "vector_id": 5315,
        "text": "In addition to Integrated Gradients, Zhan et al. [141] also use attention weights to obtain feature\nattributions for a BERT-based ranking model. With an experimental setup similar to Qiao et al .\n[95], they compute attribution scores for different groups of input tokens: The special \u201c[CLS]\u201d and\n\u201c[SEP]\u201d tokens, the query tokens, the document tokens, and the period token. While confirming\nthat a significant amount of attention weight is distributed to the special tokens and the period\ntoken, the authors also find that the attributions produced using attention weights are negatively\ncorrelated with the attributions produced by Integrated Gradients. Based on their results, the\nauthors speculate that these tokens receive high attention weights due to their high document\nfrequency. They argue that the model dumps redundant attention on these tokens, while these\nactually carry little relevance information.\n3.3 Evaluating Feature Attributions"
    },
    {
        "vector_id": 5316,
        "text": "3.3 Evaluating Feature Attributions\nInput feature attributions can be evaluated in many ways. However, there is little agreement on\nwhich evaluation strategy is best. Sanity-checks [1, 123] test functionally grounded assumptions\nbehind feature attributions. Whether feature attributions are faithful to the model that is explained\ncan also be evaluated by removing important features and re-evaluating model performance, either\nwith or without retraining [ 58, 79, 103]. However, if the model is not retrained, removing or\nreplacing features can result in out-of-distribution inputs. Other works propose shortcut, artifact,\nor spurious correlation detection tasks to evaluate feature attributions [2, 7, 59, 137], where bugs\nare added to a model on purpose and then used as ground-truth for explanation evaluation. Feature\nattribution methods that rely on surrogate models need to evaluate their fidelity, that is, how well the"
    },
    {
        "vector_id": 5317,
        "text": "surrogate model approximates the black box model being explained. Unfortunately, the evaluation\nof feature attributions in IR is often limited to anecdotal examples. Singh and Anand [112] neither\nevaluate the explanation quality of EXS nor the fidelity of the local surrogate models used to\ngenerate explanations. Verma and Ganguly[125] evaluate LIRME by comparing the explanations to\na reference of important terms obtained from relevance judgments but also do not explicitly evaluate\nfaithfulness. Fernando et al. [39] include an analysis of the faithfulness of LIME explanations for\nneural ranking models by measuring accuracy and mean-squared error of the local surrogate model.\nTo evaluate explanations produced by DeepSHAP, they use LIME explanations as a reference.\nDirectly optimizing explanations based on evaluation metrics, as done by Singh et al. [115], seems\nadvantageous, but does not provide any guarantees of finding a good explanation. Based on the"
    },
    {
        "vector_id": 5318,
        "text": "limited work on evaluating feature attributions in IR, we argue that claims and hypotheses based on\ninsights from feature attribution explanations should be handled with caution unless the explanation\nmethodology has been evaluated rigorously.\nPre-print 10 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nApproach Task Explanation Evaluation\nEXS [112] Text Ranking Feature Attribution Anecdotal\nLIRME [125] Text Ranking Feature Attribution Anecdotal/Reference\nDeepSHAP [39] Text Ranking Feature Attribution Reference\nAttention [95, 141] Text Ranking Feature Attribution Visualization\nGlobal Surrogate Model [111] LTR Global Feature Attribution Faithfulness\nGreedy Search [115] LTR Feature Attribution Sufficiency/Completeness\nGradient Saliency [94] LTR Feature Attribution Faithfulness\nIntent Modeling [113] Text Ranking Terms/Words Faithfulness/Reference\nCtrsGen [143] Text Ranking Free-text Reference\nGenEx [97] Text Ranking Free-text Reference/Human"
    },
    {
        "vector_id": 5319,
        "text": "GenEx [97] Text Ranking Free-text Reference/Human\nLiEGe [138] Text Ranking Topic Words Reference\nUniversal Adv. Triggers [132] Text Ranking Trigger Anecdotal/Visualization\nTable 1. Overview of post-hoc explanation methods. The evaluation of post-hoc methods can be anecdotal,\nvisualized, or can be intrinsically measured by a corresponding faithfulness measure. \u201cReference\u201d refers to\ncomparison with ground-truth explanations, an interpretable model, or another attribution method.\n4 FREE-TEXT EXPLANATIONS\nFree-text explanations methods aim to generate explanations using natural language and are thus\nalso called natural language explanations. Compared to feature attributions, the explanations can\nbe more expressive, as they are not limited to words that already contain the input. Typical free-text\nexplanations are not more than a few sentences long, and sometimes even limited to a few words."
    },
    {
        "vector_id": 5320,
        "text": "This form of explanation is popular for both textual and visual-textual tasks, for which a variety of\ndatasets have been collected or expanded to include explanations [133]. However, apart from a few\nquestion-answering datasets, none of them are closely related to IR. Instead, this explanation style\nis commonly used for tasks that involve reasoning. Since for such tasks, the information contained\nin the inputs is often insufficient to achieve good task performance, the explanations must also\ncontain external information apart from what is contained in the inputs. In fact, many datasets that\ninclude free-text explanations are used to improve the task performance of the model. The idea is\nthat a model will generalize better if it can also explain its predictions [20, 65, 74, 98].\nApproaches to generating free-text explanations for text ranking models focus either on inter-\npreting the query intent as understood by a ranking model or on producing a short text summary"
    },
    {
        "vector_id": 5321,
        "text": "to explain why an individual document or a list of documents is relevant.\n4.1 Explaining Query Intent\nSatisfying the information need of a user that issues a search query is a key concept in IR. Explaining\nthe intent as understood by black box ranking models can be useful to examine whether complex\nranking models perform in accordance with a user\u2019s intent.\n4.1.1 Query Expansion. Singh and Anand [113] propose a model-agnostic approach to interpret\na query intent as understood by a black-box ranker. Given a single query and a set of expansion\nterms as input, they fit an interpretable term-based ranking model to mimic the complex model to\nbe interpreted. The goal is to identify a set of query expansion terms such that most of the pairwise\npreferences in the output ranking are preserved. Query expansion terms are selected by optimizing\nthe preference pair coverage using greedy search. The expanded query terms act as an explanation"
    },
    {
        "vector_id": 5322,
        "text": "for the intent perceived by the black-box ranking model, as Figure 6 demonstrates. In experiments\nPre-print Explainable Information Retrieval: A Survey 11\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n3\n4\n5\n9.37\n9.34\n9.31\n9.31\n9.29\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ...\nChair Seated Twists Yoga | Yoga Sequences, Benefits, Variations, and Sanskrit ... \n5 Best Yoga Ball Chair for Homes and Offices (2021 Buying Guide) - Learn ... \nChair Yoga for Seniors: 8 Chair Yoga Poses Seniors Can Do Easily At Home ...\n+ {poses, guide, home, how, hip, sequence, learn}\nFig. 6. Example of query expansion terms (green) as explanations. The expansion terms are chosen such that\nan interpretable term-based ranker best approximates the ranking of documents produced by a black-box\nranking model."
    },
    {
        "vector_id": 5323,
        "text": "ranking model.\nwith a variety of ranking models, including RM3 [ 67], DESM [85], DRMM [46], P-DRMM [80],\ntrained on the Robust04 collection [129], they show that this approach can produce explanations\nwith high fidelity.\n4.1.2 Generating Query Descriptions. Zhang et al. [143] introduce a Query-to-Intent-Description\ntask for query understanding. Given a query and a set of both relevant and irrelevant documents,\nthe goal is to generate a natural language intent description. To solve this task, they propose\nCtrsGen, a contrastive generation model that generates a query intent description by contrasting\nthe relevant and irrelevant documents. The training data for CtrsGen consists of multiple TREC and\nSemEval [23] collections that already include query descriptions. Although not explicitly discussed\nby Zhang et al. [143], CtrsGen can be used to explain query intents as understood by a black-box\nranker by selecting relevant and irrelevant documents based on the output of the ranking model."
    },
    {
        "vector_id": 5324,
        "text": "However, it has not yet been examined whether the generations of CtrsGen explain the underlying\nranking model faithfully.\n4.2 Explaining Document Relevance\nA recent line of work in explainable document retrieval aims to explain why a document or a set of\ndocuments is considered relevant to a query by generating free-text explanations. Compared to\nother model-agnostic explanation methods, free-text explanations are not limited to explaining\ndocument relevance using features that are already contained in the input. A user study by [97]\nsuggests that adding free-text document relevance explanations to search engine result pages can\nhelp users identify relevant documents faster and more accurately.\n4.2.1 Pointwise Explanations. Rahimi et al. [97] generate document relevance explanations for\nindividual query-document pairs. They propose GenEx, a transformer-based model that outputs\nfree-text document relevance explanations. Given a query-document pair, GenEx learns to generate"
    },
    {
        "vector_id": 5325,
        "text": "a text sequence that explains why the document is relevant to the query. The explanations consist\nof only a few words instead of whole snippets, and explicitly avoid reusing the terms already\ncontained in the query. The model uses an encoder-decoder architecture, with the decoder being\nextended by a query-masking mechanism to decrease the probability of generating tokens that are\nalready contained in the query. The training data consists of query-document-explanation triplets\nand is automatically constructed from Wikipedia articles and the ClueWeb09 dataset [18].\n4.2.2 Listwise Explanations. Yu et al. [138] argue that explaining documents independently is\ninherently limited. Per-document explanations do not explain differences between documents, and\na single document can potentially cover multiple query aspects at the same time. As a solution,\nthey propose a listwise explanation generator (LiEGe) that for a given query jointly explains all the"
    },
    {
        "vector_id": 5326,
        "text": "documents contained in a ranked result list. LiEGe is based on an encoder-decoder transformer\nPre-print 12 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\narchitecture and uses pre-trained weights from BART [71]. The authors introduce two settings\nfor search result explanations: 1) comprehensive explanation generation, where the explanation\ncontains all query aspects covered by each document, and 2) novelty explanation generation,\nwhere the explanation contains a description of the relevant information of a document that is\nnovel, considering all the preceding documents in the ranked list. Two weakly labeled datasets\nare constructed from Wikipedia to train LiEGe for these two settings, the evaluation dataset is\nconstructed using query logs from the MIMICS dataset [140].\n4.3 Evaluation of Free-text Explanations\nThe evaluation of free-text explanations is generally based on the availability of ground-truth"
    },
    {
        "vector_id": 5327,
        "text": "explanations. Although explanations are not included in most IR datasets, proxy explanations can\nbe created from query descriptions, query aspect annotations, topic annotations, or click logs [97,\n138, 143]. BLEU [87] and ROUGE [72], two metrics commonly used to evaluate text summarization\nand machine translation tasks, can be used to compare generated free-text explanations with\nreference explanations. Furthermore, Rahimi et al. [97] and Yu et al. [138] use BERTScore [144] to\nmeasure semantic coherence. However, human-annotated but model-independent ground-truth\nexplanations can only be used to evaluate the plausibility of generated explanations. Whether the\ngenerated explanations are faithful to the ranking model being explained remains an open question.\nOnly Singh and Anand [113] evaluate the faithfulness of their query intent explanations since they\nhave to ensure that the interpretable ranker used during optimization closely mimics the black-box"
    },
    {
        "vector_id": 5328,
        "text": "ranking model being explained. To examine whether GenEx explanations actually help users,\nRahimi et al. [97] conduct a user study. Specifically, they collect explanation preferences, linguistic\nquality ratings, and relevance judgments from crowd-workers, comparing GenEx explanations\nwith different baseline explanations.\n5 ADVERSARIAL EXAMPLES\nAdversarial examples are commonly used to demonstrate the fragility or robustness of machine\nlearning models. However, they can also serve as explanations and provide valuable insight. In\nfact, adversarial examples are closely related to counterfactual examples, but instead of providing\nactionable recourse, the goal is to fool machine learning models. Given an individual input to a\nmodel, a corresponding adversarial example is crafted by applying small deliberate perturbations to\ndeceive a model into making a wrong prediction. The resulting adversarial examples inform about"
    },
    {
        "vector_id": 5329,
        "text": "the minimal input changes required to change a prediction and thus provide insight into the decision\nbehavior of the model. Specifically, the adversarial perturbations indicate which input features have\nto change by how much to alter a predicted outcome. Compared to feature attributions (Section 3),\nadversarial explanations are contrastive explanations, since the adversarial example is always\ncompared to the unmodified input example. From the perspective of social science, Miller [81]\nargues that such contrastive explanations can be considered more human-grounded.\n5.1 Adversarial Examples in Ranking\nMost of the work on adversarial examples is concerned with classification tasks, where a wrong\nprediction is defined by comparing the predicted label with a target label. For ranking tasks, the\nmain objective of an adversarial perturbation is to cause a relatively large rank promotion or rank\ndemotion of a document. For example, a company aiming to optimize search engines could leverage"
    },
    {
        "vector_id": 5330,
        "text": "adversarial attacks to promote a specific web page to the top of a search result page with minor\nchanges in the page content itself.\nPre-print Explainable Information Retrieval: A Survey 13\nRaval and Verma [99] generate adversarial examples for black-box retrieval models that lower\nthe position of a top-ranked document with minimal changes to the document text. Given the non-\ndifferentiability of replacing discrete tokens, they optimize adversarial examples using a stochastic\nevolutionary algorithm with a one-token-at-a-time replacement strategy. Wu et al. [135] take a\ndifferent approach by training a surrogate model based on pseudo-relevance feedback, which is\nused to approximate the gradient of the underlying black box ranking model. This approximated\ngradient is then used to find adversarial perturbations that promote a target document. Additionally,\nthe adversarial perturbations are restricted by semantic similarity to the original document. The"
    },
    {
        "vector_id": 5331,
        "text": "authors argue that the perturbations are imperceptible and evade spam detection when constraining\nthe perturbations to semantic synonyms. Goren et al . [45] craft adversarial examples for the\nLambdaMART LTR model. For a given query, they use past rankings to create perturbations by\nreplacing passages in the target document with passages from other high-ranked documents.\nWang et al. [132] use gradient-based optimization to generate adversarial examples for BERT-\nbased ranking models. They add or replace a few tokens in documents that cause significant rank\npromotions and demotions.\n5.2 Universal Adversarial Triggers\nWhile adversarial examples focus on input perturbations that change the prediction of individual\ninputs, universal adversarial triggers [130] are input-agnostic perturbations that lead to a model\nmaking a specific prediction whenever the trigger is concatenated to any input. Starting from"
    },
    {
        "vector_id": 5332,
        "text": "an initial sequence of tokens, a trigger is optimized via a gradient-based search algorithm that\niteratively replaces tokens. The effect of replacing a discrete token is usually approximated using\nHotFlip [33]. Since the resulting triggers transfer across input examples, they can be used to explain\nthe global behavior of a model and can reveal global patterns.\n5.2.1 Universal Triggers for Text Ranking. Wang et al. [132] adapt universal adversarial triggers for\ntext-based ranking models. They propose a global ranking attack to find trigger tokens that are\nadversarial to all queries contained in a dataset. Specifically, they optimize a fixed-length trigger\nso that any document to which it is concatenated will be demoted (or promoted) as much as\npossible for any given query. In their experiments with BERT-based ranking models fine-tuned on\nClueWeb09 [18] and MS MARCO [86], they discover topical patterns within and between datasets"
    },
    {
        "vector_id": 5333,
        "text": "and expose potential dataset and model biases. For example, the trigger\nhinduism earthquakes childbirth tornadoes Wikipedia\npromotes a document by 63 ranks on average, and the trigger\nacceptable competition rayon favour ##kei\ndemotes a document by 84 ranks on average across all queries. In general, finding triggers for which\nhighly relevant documents get demoted appears easier than finding triggers for which low-ranked\ndocuments are promoted.\n6 AXIOMATIC ANALYSIS OF TEXT RANKING MODELS\nUnlike current data-driven, parameterized models for relevance estimation, traditional IR ap-\nproaches to ranking involve probabilistic models of relevance such as BM25 [ 3] and axiomatic\napproaches. Both approaches have a top-down defined notion of relevance, allowing for some sort of\ninterpretability. Yet, the probabilistic models are currently dominant and axiomatic approaches less\npopular. In contrast to the recent development of neural, and therefore less interpretable, rankers,"
    },
    {
        "vector_id": 5334,
        "text": "Axiomatic IR postulates and formalizes the properties of principled rankers. The term axiom in IR\nPre-print 14 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nProperty Axiom Details\nTerm Frequency TFC1 [36] Prefer documents with more query term occurrences\nGiven \ud835\udc44 = \ud835\udc5e,|\ud835\udc371|= |\ud835\udc372|, \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc5e,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc372\nDocument Length LNC1 [36]\nPenalize long documents for non-relevant terms\nGiven \ud835\udc61 \u2209 \ud835\udc44, arbitrary term \ud835\udc64, \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc371)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc372)\u2227\n\u2200\ud835\udc64\u2260\ud835\udc61\ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc371)= \ud835\udc61\ud835\udc53 (\ud835\udc64,\ud835\udc372)\u21d2 \ud835\udc371 >\ud835\udc3f\ud835\udc41\ud835\udc361 \ud835\udc372\nSemantic Similarity STMC1 [38]\nPrefer terms more similar to query terms\nGiven \ud835\udc44 = \ud835\udc5e,\ud835\udc371 = \ud835\udc611,\ud835\udc372 = \ud835\udc612,\ud835\udc5e \u2260 \ud835\udc611,\ud835\udc5e \u2260 \ud835\udc612, \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc611)> \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5e,\ud835\udc612)\n\u21d2\ud835\udc371 >\ud835\udc46\ud835\udc47\ud835\udc40\ud835\udc361 \ud835\udc372\nQuery Aspect AND [148]\nPrefer documents containing all query terms\nGiven \ud835\udc44 = \ud835\udc5e1,\ud835\udc5e2,\ud835\udc61\ud835\udc51(\ud835\udc5e1)\u2265 \ud835\udc61\ud835\udc51(\ud835\udc5e_2), \ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc371)= 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc371)= 1\n\u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e1,\ud835\udc372)> 1 \u2227\ud835\udc61\ud835\udc53 (\ud835\udc5e2,\ud835\udc372)= 0 \u21d2\ud835\udc371 >\ud835\udc34\ud835\udc41\ud835\udc37 \ud835\udc372\nTable 2. Selection of proposed retrieval axioms. Adapted from a more complete list of axioms available in [14]."
    },
    {
        "vector_id": 5335,
        "text": "was first coined by Bruza and Huibers [16], who proposed to describe retrieval mechanisms using\naxioms expressed through concepts in the field of IR.\nAxiom TFC1: Prefer documents with more query term occurences.\n\ue048ery: are dogs great ?\ndi: We have all kinds of dogs. Because dogs are superior pets.\ndj: Cats are way better than dogs. We love our cute cats.\ndi >TFC1 dj\nFig. 7. Example of applying the TFC1 [ 36] axiom to rank two documents. Query terms are highlighted. \ud835\udc51\ud835\udc56 is\nranked higher than \ud835\udc51\ud835\udc57 because it contains more query terms.\nAn example axiom is TFC1 [36] which proposes to prefer documents having more query terms\noccurrences (Figure 7). Formally, given a query \ud835\udc5e = \ud835\udc61 and two documents \ud835\udc511,\ud835\udc512 with |\ud835\udc511|= |\ud835\udc512|,\nTFC1 is defined as\n\ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc511)> \ud835\udc61\ud835\udc53 (\ud835\udc61,\ud835\udc512)\u21d2 \ud835\udc511 >\ud835\udc47\ud835\udc39\ud835\udc361 \ud835\udc512. (1)\nSimilarly, a large set of axioms has been proposed in recent decades, ranging over different\naspects of relevance such as term frequency [36, 37], document length [36], semantic similarity"
    },
    {
        "vector_id": 5336,
        "text": "[38], or term proximity [47] among others (see Table 2). For a more detailed description of the\nvarious axioms, we refer to an overview by Bondarenko et al. [14].\nAxioms are human-understandable concepts. This is in stark contrast to neural networks, which\nhave been shown time and time again to learn spurious correlations [44] and to be susceptible to\nadversarial attacks [132]. Although not yet achieved, a long-term goal of axiomatic IR could be a\nconcept of relevance built on axioms. This conceptualization of relevance would then be robust to\nattacks, generalize to novel distributions, and be interpretable for humans.\nAlthough there is no general model of relevance yet, previous work aggregated axioms to build\naxiomatic rankers (Section 6.1), analyze and explain existing neural ranking approaches by aligning\nthem to known axioms (Section 6.2), and use axioms to regularize the training of neural rankers"
    },
    {
        "vector_id": 5337,
        "text": "(Section 6.3). An overview of this classification and papers in this section can be found in Table 3.\nPre-print Explainable Information Retrieval: A Survey 15\nPaper Task Approach Dataset Evaluation\nHagen et al. [47] LTR IBD TREC Web tracks 2009-2014 -\nRennings et al. [101] Text Ranking Post-hoc WikiPassageQA -\nC\u00e2mara and Hauff [19] Text Ranking Post-hoc TREC 2019 DL -\nV\u00f6lske et al. [128] Text Ranking Post-hoc Robust04, MS MARCO Fidelity\nRosset et al. [104] Text Ranking Regularization MS MARCO -\nCheng and Fang [26] Text Ranking Regularization WikiQA, MS MARCO -\nChen et al. [24] Text Ranking Regularization MS MARCO, TREC 2019 DL Anecdotal\nTable 3. Classification of axiomatic methods. The evaluation w.r.t. interpretability can be anecdotal or intrin-\nsically measured by a corresponding faithfulness measure.\n6.1 Interpretable Axiomatic Rankers\nHagen et al. [47]is one of the first to operationalize retrieval axioms to perform axiomatic re-ranking."
    },
    {
        "vector_id": 5338,
        "text": "By learning the importance of individual axioms, they aggregate the axioms\u2019 partial orderings.\nDespite being inherently more interpretable, they evaluate their axiomatic re-ranking step with a\nselection of retrieval models, showing that for most of them the performance significantly increases.\nGiven that the axioms and the aggregation method are fully interpretable, the resulting re-ranking\nis also fully interpretable. Bondarenko et al. [14] proposed a utility library called ir_axioms that\nallows experimenting with a collection of 25 different axioms and allows one to add new axioms.\nThis library can be used for axiomatic result re-ranking and diagnostic experiments to explain\nneural ranking models.\n6.2 Axioms for Model Diagnostics\nMore directly related to the classical post-hoc interpretability work is a line of recent works\ndiagnosing and explaining ranking models using axioms. Rennings et al. [101]constructed diagnostic"
    },
    {
        "vector_id": 5339,
        "text": "datasets based on existing axioms and checked whether classical neural ranking models are in\nagreement with the axiomatic rules. They find that out-of-the-box neural rankers conform with the\naxiomatic rankings to only a limited extent. However, they hypothesize that including diagnostic\ndatasets in the training process could boost this conformity. C\u00e2mara and Hauff[19] extend this work\nand apply diagnostic datasets similarly to ad-hoc retrieval with BERT. They find thatBERT does\nnot align with most of the ranking axioms but significantly outperforms other neural and classical\napproaches. The authors conclude that the current set of axioms is insufficient to understand BERT\u2019s\nnotion of relevance. Last in this line of work is an approach to produce axiomatic explanations for\nneural ranking models by V\u00f6lske et al. [128]. Similar to existing work on axiomatic re-ranking [47]\nand diagnosing neural rankers [ 19, 101], this study investigates whether neural rankings can"
    },
    {
        "vector_id": 5340,
        "text": "be explained by the combination of existing axioms. To do so, they train a small random forest\nexplanation model on the axioms\u2019 partial orderings to reconstruct the ranking list produced by the\nneural ranking model. They find that axiomatic explanations work well in cases where the ranking\nmodels are confident in their relevance estimation. However, these explanations fail for pairs with\nsimilar retrieval scores and conclude that more axioms are needed to close this gap.\n6.3 Axioms for Regularizing Neural Rankers\nRecently, a variety of approaches foraxiomatic regularization of neural ranking models has been pro-\nposed [24, 26, 104]. These approaches aim to regularize opaque neural rankers to incentivize learning\nPre-print 16 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nof the principled, axiomatic notions of relevance. This has the benefits of faster convergence [104],"
    },
    {
        "vector_id": 5341,
        "text": "improved performance [26] or generalization ability [24, 104], and improved interpretability [24].\nThe method by which the ranking models are regularized varies from adding a regularization term\nto the loss function [24, 104] to axiomatically perturbing the training data to amplify desirable\nproperties [26]. An example of such a regularization term is applied by Chen et al. [24] who add a\nrelevance loss to their final loss function that checks how well the model\u2019s relevance judgments\ncoincides with the axioms\u2019. Cheng and Fang [26] extend the training dataset by randomly sam-\npling instances and perturbing them according to three document length normalization axioms,\nsuch as by adding noise terms. Then, these more noisy documents are assigned a lower relevancy\nvalue. From such perturbed data examples, the model is expected to understand the corresponding\nnormalization axiom based on document length. While current regularization methods offer only"
    },
    {
        "vector_id": 5342,
        "text": "limited (perceived) interpretability, the approach similar to the neuro-symbolic approaches [108]\nmarry the benefits of both axioms and data-driven models.\n6.4 Evaluation\nIR axioms have been applied in various works over the past decades, and many revolve around\ninterpretability. However, little formal evaluation of the insights gained through the axioms has\nbeen done from an interpretability perspective. One exception is Chen et al. [24], who give anecdotal\nexamples of their axiomatically regularized model\u2019s input attribution being more sparse and focused\non relevant tokens. In addition, only V\u00f6lske et al. [128] use established interpretability evaluation\nmetrics and measure the fidelity of their generated (post-hoc) explanations. From the interpretability\nperspective, two steps are needed for upcoming work: 1), proposing new axioms or methods to\nbetter explain neural ranking models and 2), rigorously evaluating the produced explanations with"
    },
    {
        "vector_id": 5343,
        "text": "established metrics and eventually human acceptance studies.\n7 PROBING AND PARAMETRIC ANALYSIS OF TEXT RANKING MODELS\nProbing is a method to analyze the content of latent embeddings. It allows us to understand the\ninformation encoded in the model\u2019s representations. Usually, probing includes training a small\nclassifier to predict the property of interest (e.g., part-of-speech tags or question types) directly\nfrom the embeddings [9, 121, 122, 127].\n7.1 The Probing Methodology\nFigure 8 shows an example in which we test whether a ranking model encodes information on\ndifferent question types.\nWhat attracts tourists to Rome? Location\nEmbeddings\nFrozen\nRanker\nProbe\nModel\nFig. 8. Example of the probing paradigm. A small classifier (the probe model) is used to predict properties (in\nthis case the question type) from a ranker\u2019s frozen representations.\nTo do so, we need a small, labeled dataset of questions and their respective question types."
    },
    {
        "vector_id": 5344,
        "text": "We then train the probing classifier to recover the question type information from the ranker\u2019s\nfrozen embeddings. Originally, the model would be considered to encode the property of interest\nif the classifier can better predict it than a majority classifier. However, depending on the task\u2019s\ndifficulty, dataset size, and classifier complexity, large portions of the resulting performance must\nbe attributed to the classifier. Therefore, a large set of improvements to the probing paradigm have\nPre-print Explainable Information Retrieval: A Survey 17\nPaper Task Concept under Investigation Architectural component\nChoi et al. [27] Text Ranking IDF Attention\nZhan et al. [141] Text Ranking Attention, <Q, D> Interactions Attention, Embeddings\nFormal et al. [43] Text Ranking Lexical Matching Behavioral\nFormal et al. [42] Text Ranking Matching, Term Importance Behavioral\nSen et al. [109] Text Ranking TF, IDF, Document Length Behavioral"
    },
    {
        "vector_id": 5345,
        "text": "MacAvaney et al. [78] Text Ranking Matching, Manipulation, Style Embeddings\nFan et al. [35] Various IR Tasks Relevance Modeling Embeddings\nvan Aken et al. [124] QA QA Subtasks Embeddings\nCai et al. [17] RC MRC Subtasks \u2013\nWallat et al. [131] Various NLP Tasks Factual Knowledge Embeddings\nPetroni et al. [89] <Benchmark> Factual Knowledge <Benchmark>\nTable 4. Classification of the probing literature (Section 7). These papers usually investigate whether models\ntrained on a downstream (IR) task encode a concept (such as lexical matching) in different architectural\ncomponents (e.g., the attention maps). Behavioral studies do not probe a specific model component but\ninvestigate the model\u2019s general behavior.\nbeen proposed \u2013 from introducing suitable baselines [ 142] and control tasks [52], over varying\nthe classifier complexity [ 90], to measuring the target property\u2019s ease of extraction from the\nembeddings [127]. For a more comprehensive overview of the initial probing paradigm and the"
    },
    {
        "vector_id": 5346,
        "text": "proposed improvements, we refer to the paper by Belinkov [9].\n7.2 Probing Ranking Models\nSeveral variations of the probing paradigm have also been applied to various IR tasks and models.\nAn overview of the papers, together with a classification, can be found in Table 4. As atext-ranking\nmodel, the approach of Zhan et al. [141] investigates the attention patterns of BERT after fine-tuning\non the document ranking task. Their experiments show that large parts of the attention are off-\nloaded to low information tokens such as punctuation, which might lead to increased susceptibility\nto adversarial attacks. Similarly, a recent study by Choi et al. [27] probes the attention maps of\na BERT ranker, finding that inverse document frequency is captured. As discussed in Section 6,\nthe existing ranking axioms are insufficient to explain rankings produced by BERT-based models.\nTherefore, Formal et al. [42] investigate the ColBERT regarding its term-matching mechanism. By"
    },
    {
        "vector_id": 5347,
        "text": "stratifying on IDF bins, they show that ColBERT indeed captures a notion of term importance,\nwhich is enhanced by fine-tuning. However, the results suggest that estimating term importance is\nlimited when no exact matches are available. Given the limited ability of current neural retrieval\nmodels to generalize to new datasets, Formal et al. [43] question whether this is caused by their\ninability to perform lexical matching in the out-of-domain scenario. While general lexical matching\nability is present in neural retrievers (such as TAS-B or ColBERT), the understanding of which\nterms are important to match seems to be missing in the out-of-domain setting. Sen et al. [109] aim\nto attribute relevance prediction performance to term frequency, document frequency, or document\nlength. To do so, they train a linear model using these aspects to approximate the ranking model.\nThe resulting coefficients are then used to understand the importance of the corresponding aspects."
    },
    {
        "vector_id": 5348,
        "text": "The resulting explanations confirm that the model behavior follows certain constraints used in\naxiomatic IR (Section 6). MacAvaney et al. [78] also further investigate the hidden abilities of neural\nPre-print 18 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nrankers that lead to their good ranking performance. They attribute the model\u2019s matching ability\nto three properties (concepts), relevance, document length, and term frequency. They devise a\nbehavioral-probing setup that verifies to what extent the model could capture these concepts. For\nmanipulation-sensitivity analysis , they test the effect of shuffled words, sentences, or typos on the\nmodel performance. Lastly, MacAvaney et al. [78] create probing sets for writing style concepts\nsuch as fluency, formality, or factuality. Their results suggest that neural rankers are biased toward\nfactually correct articles and that appending irrelevant text can improve the relevance scores."
    },
    {
        "vector_id": 5349,
        "text": "Similarly, the work by Fan et al. [35] strives to understand the relevance-modeling of IR models.\nThey also propose to probe for a large set of lexical, syntactic, and semantic concepts such as\nnamed entities or coreference resolution ability. By comparing the performance of their fine-tuned\nmodels to a pre-trained BERT, they find that these IR models generally seem to sacrifice small\nparts of their ability to perform lexical and syntactic tasks and improve especially in semantic\nmatching (e.g., identifying synonyms). Furthermore, causal intervention analysis is applied to the\nmodel parameters, input features, and training objectives, resulting in suggesting that a careful\nintervention on linguistic properties can improve the performance of downstream IR models.\n7.3 Probing other Information Retrieval Models\nIn addition to the core ranking objective, models for other IR-related taskshave been probed."
    },
    {
        "vector_id": 5350,
        "text": "van Aken et al. [124] investigate BERT embeddings of a QA model and how do they interact over\nthe layers when answering questions. Specifically, they probed a pre-trained BERT and a QA\nmodel, finding that training the model for QA improves the performance on related tasks such\nas question type classification or identification of supporting facts. The question of how BERT\nreacts to fine-tuning has also been investigated in several studies [35, 124, 131]. Cai et al. [17] probe\nMRC (machine reading comprehension) models for relevant subtasks (synonyms, abbreviations,\ncoreference, as well as question type classification). They find that only for core MRC subtasks, the\ntoken representation varies in the later layers of the MRC model. The core MRC subtasks include\ntasks such as coreference, question type classification, and answer boundary detection. However,\nfor tasks like synonym and abbreviation detection, the representations are only moderately different"
    },
    {
        "vector_id": 5351,
        "text": "from the pre-trained BERT representations. Wallat et al. [131] probe models fine-tuned for various\ntasks to assess the effect of fine-tuning on (factual) knowledge retention. In their layer-wise\nexperiments, they find the ranking model to be specifically knowledgeable, dropping the least\namount of knowledge compared to the question-answering and named entity recognition models.\nAdditionally, large parts, though not all, of the factual knowledge seem to be captured in the latter\nlayers. Petroni et al. [89] identify the requirement of world knowledge for many IR tasks such\nas open-domain question-answering, slot filling, entity linking, or fact-checking. To understand\nto what extent do current models capture real-world knowledge, Petroni et al . [89] propose a\nbenchmark containing knowledge-intensive tasks (QA, slot filling, entity linking, fact-checking,\namong others) all derived from a single Wikipedia corpus.\n7.4 Evaluation"
    },
    {
        "vector_id": 5352,
        "text": "7.4 Evaluation\nIn the past, probing results have been evaluated differently by the interpretability community\nthan other post hoc methods. Whereas other methods such as feature attributions have been\nrigorously evaluated concerning metrics such as fidelity or faithfulness, this has not been the case\nin the probing literature. As suggested by Belinkov [9], a standard probing setting can answer the\nquestion: What information can be decoded from the model\u2019s embeddings? It does not offer a human-\ncentered explanation for a specific data instance, but rather provides general information about the\nmodel. Thereafter, it does not offer interpretability for users but for model developers, although the\nprobing methodology has been scrutinized and extended in various works [52, 119, 127]. Given the\nPre-print Explainable Information Retrieval: A Survey 19\nX\nExplainable Decision Structure\nFeature-interaction-based\nPrediction\nExplainable Text RankingExplainable Learning-to-rank\n\ue048ery Document"
    },
    {
        "vector_id": 5353,
        "text": "\ue048ery Document\nRationale-based\nFeature Aggregation\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Interaction\nFeat. Extraction Feature Extraction\nbike theft report stolen bicycle online\nbike theft report stolen bicycle online\nPrediction\n\ue048ery Document\nFeature Aggregation\nSparse Input Representation\nFeat. Extraction Feature Extraction\nbike theft stolen\nX\nbicycle\nbike theft report stolen bicycle online\nPrediction\n\ue048ery-document Vector\nFeature Aggregation\nFeature Extraction\nExplicit Feature Contribution\nFig. 9. Types of IBD models. Green and gray color refers tomostly interpretable/non-interpretable components,\nrespectively.\ncorrect baselines and a tightly controlled setup, it might be able to shed light on the question of\nWhat information is learned by training on a specific task? or How easily extractable is information\nabout a concept from the model? [127]. However, it is unclear whether this information is actually"
    },
    {
        "vector_id": 5354,
        "text": "being used by the model at inference time [9]. To resolve this, recent studies borrow ideas from\ncausality research to understand whether a specific concept is utilized during the inference using\ncounterfactual representations, where the concept is voided [ 34, 66]. The model is proven to\nhave used the concept if the counterfactual representations result in worse task performance. In\nconclusion, while there has been an in-depth evaluation of the probing paradigm by the NLP and\ninterpretability community and many improvements have been proposed, little of that found its\nway into IR-related probing studies. Future probing studies in IR will need to include learnings and\nbest practices from established research and use them to evaluate and validate the findings for IR\nmodels.\n8 EXPLAINABLE-BY-ARCHITECTURE MODELS\nWe refer to the first family of IBD models as explainable-by-architecture models. Those models can"
    },
    {
        "vector_id": 5355,
        "text": "be viewed as a modular framework of multiple components (see Figure 9). The general architecture of\nthese models involves intermediate feature extraction (that might involve feature attributions), and\na task-specific decision structure (that might involve feature interactions). Pragmatically speaking,\nnot all components are fully interpretable to ensure competitive task performance. Therefore, most\nof the IBD resort to making only specific components interpretable or transparent. In the following,\nwe look at two major use cases of such models in text ranking and LTR tasks.\nPre-print 20 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n8.1 Explainable Text Rankers\nIn text ranking, the need for interpretability is based on large input sizes and complex feature\ninteractions. Since documents can be long, it is hard to ascertain what sections of text the query\nterms interact with within a complex model. This problem is particularly acute in the case of"
    },
    {
        "vector_id": 5356,
        "text": "contextual models with transformers, where the self-attention mechanism essentially considers all\npairs of interactions between the query and the document terms. Therefore, one strategy of the\nIBD models in the text ranking family focuses on building interpretable query-document interaction\nfunctions and, in turn, leading to a more transparent decision-making path. In this setup, the query\nand the document are encoded separately by two individual models and each token (or word) is\nrepresented by a fixed-size embedding vector. Note that this encoding process remains opaque\nfor both context-free and contextualized embeddings. A (partially) explainable model employs\nhuman-understandable functions to measure the degree of query-document interactions, which\nessentially indicates the similarity of the query and the document. The final relevance judgment can\nthen be made based on the interactions. Another line of IBD text rankers is focusing on reducing"
    },
    {
        "vector_id": 5357,
        "text": "the large input space, which we refer to as rationale-based methods. The idea is to use a small set\nof explicit words or sentences as input leading to the final prediction, whereas how the input is\nselected, and how the prediction is made, remains agnostic. There are extensive works in building\nsuch sorts of models, to highlight the popularity, we will further discuss this method family in\nSection 9.\n8.1.1 Feature Interaction. We summarizethree ranking models, which utilize two BERT/Transformer-\nstyle encoders to generate the vectorized representations for query and document individually.\nIn the following paragraphs, we emphasize on their interaction and decision-making processes,\nshowing how the relevance decision can be explained.\nColbert [63] follows the conventional term-matching strategy. For each query token, it com-\nputes the cosine similarity scores with each token from the document and keeps the maximum"
    },
    {
        "vector_id": 5358,
        "text": "similarity score. The final document relevance is computed by simply summing up the maximum\nscores of all query tokens. Essentially, Colbert measures the semantic similarity between the query\nand the document, and a document is deemed more relevant if it contains more terms that are\nsemantically closer to the query. Boytsov and Kolter [15] propose NeuralModel1, which adds an ex-\nplainable layer, namely Model1 [10] on top of the input embedding. Specifically, the non-parametric\nModel1 layer maintains pairwise similarity statistics between query-document tokens, which are\nlearned/computed from parallel datasets beforehand. The final document relevance is combined\nfrom all query-document similarity scores by the product-of-sum formula. This approach is very\nsimilar to Colbert, where the cosine similarity computation can also be viewed as an explainable\nlayer. NeuralModel1 experimented with slightly more comprehensive similarity learning, resulting"
    },
    {
        "vector_id": 5359,
        "text": "in lower interpretability. Nevertheless, with a more complex interaction mechanism, NeuralModel1\nachieves better balance in terms of ranking performance and efficiency.\nTransformer-Kernel [54] maintains a matching matrix, where each row represents the cosine\nsimilarity scores between a particular query token and all document tokens. In contrast to Colbert,\nwhich simply takes the maximum similarity from each row and sums them up to obtain the query-\ndocument relevance, Transformer-Kernel transforms the matching matrix to a set of isomorphic\nmatrices with RBF-kernels [ 136] and each kernel focuses on a specific similarity range. This\ninteraction shares a similar idea as the similarity histogram in DRMM [46] model but employs the\nkernel-pooling technique to solve the non-differentiation of the hard histogram. The final relevance\nscore is learned by a feed-forward layer, given the semantic matrices as the input. Therefore,"
    },
    {
        "vector_id": 5360,
        "text": "the interaction in Transformer-Kernel can be viewed as smoothed semantic similarity, and the\nPre-print Explainable Information Retrieval: A Survey 21\nrelevance decision is learned via a neural layer, making the Transformer-Kernel less interpretable\nin comparison to Colbert and NeuralModel1.\n8.2 Explainable Learning-to-Rank\nFor LTR task dealing with smaller numerical input features, there are works relying on explicitly\naggregating feature contribution for relevance measurement, or a fully transparent tree model . The\ngoal of LTR is to predict a relevance score for a fixed-size input vector. Because of the smaller and\nstructured input features, it is more practical to build a transparent model in input space or the\nentire decision path. In the following paragraphs, we introduce one LTR model with explicit feature\ncontribution and one transparent decision-tree model incorporated with reduced input space."
    },
    {
        "vector_id": 5361,
        "text": "8.2.1 Explicit Feature Contribution. Different from the previously discussed feature-attribution\nmethods, explicit feature contribution indicates a simple and transparent correlation between each\ninput feature and the relevance prediction, in addition to showing importance heatmaps.\nNeuralGAM [149] is based on Generalized Additive Models (GAMs). For each individual feature,\nNeuralGAM employs an isolated black-box (e.g., neural networks) model to generate a score\nindicating the contribution (or importance) of the feature. The relevance of the input is aggregated\nby simply summing up all contribution scores. NeuralGAM is explainable in terms of feature\ncontribution, as the relevance is aggregated from the feature importance score directly by a simple\nsum operation. Nevertheless, it remains opaque how each feature importance score is generated by\nthe black-box model.\n8.2.2 Explainable Tree Models. The main challenge of interpreting tree models is the over-complex"
    },
    {
        "vector_id": 5362,
        "text": "decision path caused by the massive number of features and their interactions. Thus, an explainable\ntree model should have a limited number of features and interactions and, in turn, be able to provide\na simple and understandable decision-making path.\nILMART [75] shares a similar structure as GAM, while using LambdaMART as the sub-model.\nILMART starts from learning a set of trees, with each dealing with one single distinct feature only.\nThis step enables ILMART to identify a small yet crucial set of features and exclude the rest. Then,\nILMART enforces a new ensemble of trees to explore the interactions between every two remaining\nfeatures only. This design can effectively reduce the model\u2019s complexity. Finally, ILMART combines\ntrees from the previous two steps and learns a much smaller and simpler ensemble-tree model with\nthe input space hugely reduced.\n8.3 Evaluation\nA key attribute of interpretable models is, it does not just highlight the importance of input"
    },
    {
        "vector_id": 5363,
        "text": "snippets/dimensions (e.g., tokens in a query or document), but also suggest why those snippets\nlead to the decision. Namely, a set of rules can be implicitly inferred from the explanations, even\nwhen only the input features are presented. This is the usual case when the audience group of\nexplanation is system developers or domain experts. One explanation example for Colbert can be\na small set of tokens in the query and document, together with their cosine similarity degree. We\ndenote this type of explanation as soft-rule, to distinguish from the hard-rule of an explicit path in\na tree model. NeuralGAM presents feature attribution scores (similar to Section 3) as explanations\nand moreover, the relevance decision can be explicitly induced from the scores.\nExcept for Colbert, all methods evaluate the goodness of explanations by showing anecdotal\nexamples. Additionally, NeuralGAM compares the features to a referenced tree-model, and justifies"
    },
    {
        "vector_id": 5364,
        "text": "the faithfulness of explanations by a similar trend. A summary of methods can be found in Table 5.\nPre-print 22 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\nMethod Task Components Explanation Dataset Evaluation\nColbert [63] Text Ranking Interaction Soft-rule MS MARCO -\nTransformer-Kernel [54] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralModel1 [15] Text Ranking Interaction Soft-rule MS MARCO Anecdotal\nNeuralGAM [149] LTR Input Feature Attr. Yahoo Reference\nILMART [75] LTR Fully Hard-rule Yahoo Anecdotal\nTable 5. Explainable-by-architecture Methods. Components indicate which component of the model archi-\ntecture is explainable. Note that Colbert did not discuss or evaluate explainability. More similar datasets are\nused in each paper, and we choose one as representative.\n9 RATIONALE-BASED METHODS\nThe second class of IBD methods deals to enhance the interpretability of IR models by generating"
    },
    {
        "vector_id": 5365,
        "text": "rationales as an intermediate sparse input representation (see Figure 9). A rationale is defined as an\nextractive piece of the input text that is responsible for the decision of the model. A rationale-based\nmethod performs the task prediction two-stage. In the first feature-extraction phase, a model learns\nto extract the rationale from the input text. In the subsequent prediction phase, another independent\ntask model predicts the task output solely based on the extractive explanation. Note that in such\na setup, each prediction can be unambiguously attributed to the generated rationale that is both\nhuman-understandable and acts as an explanation. Examples of rationales are provided below in\nFigure 10.\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n\ue048ery: can you do yoga from a chair\nRank Score Document Text\n1\n2\n9.37\n9.34\n10 Yoga Poses You Can Do in a Chair | Chair pose yoga, Chair yoga, Yoga poses ...\nChair Hip Opening And Strength Flow Yoga | Yoga Sequences, Benefits ..."
    },
    {
        "vector_id": 5366,
        "text": "Fig. 10. Example of a binary rationale selection. First, a subset of tokens is selected. Then, a prediction is\nmade based on the selected rationale tokens. Selected rationales are highlighted in orange.\nWe summarize the approaches in this section in Table 6. The feature extraction stage in rationale-\nbased models is sometimes called the selection or rationale-generation stage [70, 146]. The major\nchallenge in rationale-based methods is training the rationale-extraction module due to the discrete\nrationale output of this stage. There are essentially two types of rationale-based methods based on\nthe optimization styles \u2013 pipeline or end-to-end.\n9.1 Pipeline Approaches\nA rationale-based model is a pipeline model if the rationale-extraction module is trained separately\nfrom the task prediction module. Most pipeline methods require the presence of human-annotated,\nextractive rationale data to train the rationale-extraction network. The first pipeline model was"
    },
    {
        "vector_id": 5367,
        "text": "proposed by Lehman et al . [68]. Their approach was proposed for the analysis of clinical trial\nreports, where the aim is to predict whether the clinical trial causes a significant effect compared\nwith the control group. The reports are themselves annotated by human experts, where experts do\nannotate not only the significance of the trial but also the snippet of the reports as the rationale\nsupporting such prediction. This constitutes the training data for the rationale-extraction module.\nPre-print Explainable Information Retrieval: A Survey 23\nMethod Task Training Dataset Evaluation\nExpred [146] Knowledge Intensive Pipeline ERASER Benchmark[31] PRF Score, C/S\nWojtas and Chen [134] Medical Search Pipeline GM12878 Feature Ranking\nJiang\u2019s IB [61] Text Ranking End-to-End MS MARCO \u2013\nSelect and Rank[70] Text Ranking End-to-End TrecDL, Core17, CW09 Anecdotal\nIDCM[53] Text Ranking Pipeline MS MARCO \u2013\nIOT-Match[139] Legal Case Matching Pipeline ELAM, eCAIL PRF Scores"
    },
    {
        "vector_id": 5368,
        "text": "Table 6. Rationale-based approaches. The C/S score in the evaluation refers to comprehensive and sufficiency.\nDuring the inference, the prediction model takes the output of the rationale-extraction module as\nits input.\nLater in the same year, DeYoung et al. [31] released a benchmark called ERASER to evaluate\nrationale-based interpretability. The ERASER benchmark consists of a large variety of knowledge-\nintensive tasks that presupposes an IR system, like question answering (QA) and fact-checking.\nDespite the reasonable performance benefits of such select-and-predict approaches, they suffer\nfrom a crucial deficiency. That is, the rationale-extraction module could \u201ccheat\u201d to overfit the\npattern of the rationale sub-sequences instead of selecting the rationales based on their semantic\nmeaning [7, 60]. To this end, another pipeline approach ExPred [146] was proposed. The main\nidea of ExPred was to make the rationale-extraction phase task-aware by training it in a multitask"
    },
    {
        "vector_id": 5369,
        "text": "fashion with the downstream task. By doing so, they use an auxiliary output to force the rationale\nselector to learn the semantics of the inputs with respect to the classification task.\nThe pipeline models introduced above contain only one extractor-predictor training cycle. Wojtas\nand Chen [134], however, propose to train the rationale extractor and the task predictor alternatively\nfor multiple rounds and select the masks using a genetic algorithm. The alternative training cycle is\ninitialized by training the classifier on multiple randomly sampled rationales and keeping the best\nrationale mask, resulting in the best classification performance. Then they optimize the rationale-\nextractor and the task-predictor alternatively.\nFor document ranking tasks, Hofst\u00e4tter et al. [53] propose the IDCM (Intra-Document Cascading\nModel) approach to overcome the input length limitations of modern transformer-based rankers."
    },
    {
        "vector_id": 5370,
        "text": "IDCM is a pipeline approach whose rationale extractor is an effectively simple model (student\nmodel) trained to mimic the passage-selection behavior of a more complex model (teacher model).\nThe student extractor model selects important passages as rationales from the huge amount of\ndocuments before calculating the ranking score of selected passages with respect to the current\nquery using another complex model. Evaluated on the MS MARCO dataset [86], IDCM turns to\noutperform traditional ranking models.\nFinally, another pipeline model called IOT-match [139] focuses on the case-matching problem.\nThe case-matching problem is defined as: given two different legal cases, the model should predict\nwhether the two cases are related. They try to solve the problem using optimal transport theory.\nThe intuition behind their algorithm is that the predicted sentence matching matrix is also an\noptimal transport matrix that minimizes the transport distance given the sentence discrepancy"
    },
    {
        "vector_id": 5371,
        "text": "matrix. The sentence matching matrix is a selection matrix that selects sentence pairs from both\ncases, where the sum of their discrepancies is minimized (similarity maximized). After selecting the\nmost similar sentences from both cases, they utilize the task prediction model to predict whether\nthe two cases are related based on matched sentence pairs as the rationales.\nPre-print 24 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n9.2 End-to-End Approaches\nAs its name suggests, we can train both the task and the rationale-generation module jointly using\nthe gradients from the task supervision signal. The major technical challenge in this setting is\nthat of gradient estimation for the rationale generation parameters. Lei et al. [69] is the first work\nthat proposes an end-to-end approach for the problem of rationale-based models in the context of\nvanilla text classification. The rationale extraction module parameterizes a per-token distribution"
    },
    {
        "vector_id": 5372,
        "text": "of relevance. The output of this layer is a hard binary mask as rationales. The parameters of this\nmodule are optimized by estimated gradients, using a REINFORCE-style algorithm. Additionally,\nthey also applied constraints like the continuity of the selected rationales and the sparsity of the\nrationales to further enhance the sparsity. Extensions of this approach include [68] and [6] that\nalso focus on text classification, albeit using reparameterization trick for better numerical stability\nand convergence rate when training the rationale-extractor.\nThe first work to propose end-to-end methods for ranking tasks is [70]. Their approach is called\nselect and rank and is based on the observation that only a few sentences in a related document are\nrelevant given a query. In the rationale-selection phase, they first select relevant sentences from a\ndocument with respect to the query input. The selected rationales act as an extractive summary of"
    },
    {
        "vector_id": 5373,
        "text": "the document. After that, only these rationales are used in the re-ranking phase with the query in a\ncross-encoder ranker. Both the selector and the rankers are trained end-to-end using a combination\nof the Gumbel-Softmax and reservoir sampling to ensure a user-specified \ud835\udc58-sentences to be sampled.\nUnlike previous rationale-based models, Chen et al. [25] use a mutual-information-based for-\nmulation. Their theory is to select the rationales containing the most mutual information with\nthe final prediction. Jiang et al . [61] and Bang et al. [5] further extend this information-theory-\nbased approach by considering the information bottleneck (IB) as the rationale . Specifically, the\ninformation bottleneck T is an intermediate (usually latent) representation that maximizes the\nmutual information between T and the prediction Y, while its mutual information with the input\nX is minimized, i.e., I(Y; T)\u2212 \ud835\udefd\ud835\udc3c(X; T), where \ud835\udefd is a hyper-parameter that balances both terms."
    },
    {
        "vector_id": 5374,
        "text": "Specifically, the information bottleneck can be seen as a rationale mask applied to the input, i.e.\n\ud835\udc47 = m. The approach from Bang et al. [5] leverages the Gumbel-Softmax trick to sub-sample \ud835\udc58\ntokens as the rationale, while Jiang et al . [61]\u2019s approach predicts the probability of being the\nrationale for each feature individually and obtain the rationale mask by rounding the probability.\n9.3 Evaluation\nEvaluation regimes to evaluate rationale-based models typically trade off task performance and\nthe interpretability achieved. The desirable objective for these approaches is: a good IBD approach\nshould provide a task-prediction model that performs at least no worse, if not better, than its\nnon-explainable comparators, and delivers valuable rationales.\nThe quality of the rationales can be measured by the degree of their agreement with the ground-\ntruth rationales. Essentially, they try to answer the interpretability question: To what degree do the"
    },
    {
        "vector_id": 5375,
        "text": "rationales agree to what humans consider as true reasons? Benchmarks like [31] collect multiple\nhuman-annotated datasets in IR ranging from sentiment analysis, and fact-checking to entailment\nprediction. Therefore, given the human-annotated rationale data, one can also evaluate the rationales\noutput by the rationale-extractor by calculating their similarity to the human annotations. The\nsimilarity metrics include but are not restricted to the accuracy, precision, recall, and F1 score of the\nrationale selection.\nApart from correspondence with human reasoning, DeYoung et al. [31] also introduces C/S scores,\ntwo evaluation metrics that evaluate semantic attribution of selected rationales: \u201ccomprehensiveness\u201d\nand \u201csufficiency\u201d. For an arbitrary input x with its corresponding label \ud835\udc59 on a fine-tuned model \ud835\udc40,\nPre-print Explainable Information Retrieval: A Survey 25\nthe comprehensiveness of a rationale-selection mask m is defined as the difference between the"
    },
    {
        "vector_id": 5376,
        "text": "model prediction made based on the whole input and on all-but-rationale tokens, i.e.:\ncomprehensiveness(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc66 = \ud835\udc59|x \u2299\u00afm; \ud835\udc40), (2)\nwhere \u00afm is the inverse mask and \ud835\udc43(\u00b7)indicates the predicted probability. The sufficiency is defined\nas the difference between the full-input prediction and the prediction based on the rationale-only\ntokens, i.e.:\nsufficiency(\ud835\udc40,m,(x,\ud835\udc59)):= \ud835\udc43(\ud835\udc66 = \ud835\udc59|x; \ud835\udc40)\u2212\ud835\udc43(\ud835\udc40(x \u2299m)= \ud835\udc59). (3)\nFurthermore, Bang et al. [5] evaluate their information-bottleneck model with the fidelity of the\nrationales. They define the fidelity similar to the sufficiency score introduced above, i.e., to answer\nhow well does the rationale-based prediction match the prediction on the full input?\n10 LIMITATIONS AND OPEN QUESTIONS\nIn this section, we will discuss the limitations, challenges, and open questions in the area of\nexplainable information retrieval. We have reviewed many interpretability methods and approaches"
    },
    {
        "vector_id": 5377,
        "text": "that cover various aspects and tasks in IR. However, there are many unanswered questions, use\ncases, and scenarios that need further research. We feel that most interpretability approaches have\nfocussed on the functional aspect of the central IR tasks of ranking items. There are, however, many\nmore IR tasks that employ learning systems. Similarly, an IR system has different stakeholders \u2013\nmost prominently, the benefactor of the IR system is the user, but much of the work has focused on\nthe system developer as the most likely stakeholder. Finally, most of the explanation methods have\nrelied on feature attributions as the dominant type of explanations. However, explanations can be\nin terms of training instances, adversarial examples, rules, etc.\n10.1 Limitations\nThere are multiple limitations and challenges in facilitating and developing interpretable approaches\nfor information retrieval tasks. For the common task of document retrieval, we discussed early"
    },
    {
        "vector_id": 5378,
        "text": "heard that we require listwise or pairwise explanations instead of pointwise explanations.\n10.1.1 Limiting Assumptions. The underlying assumption for surrogate models is that a simple\nmodel can locally approximate the behavior of a complex black-box ranker. However, the ranked\noutput from a complex retrieval model can involve multiple relevance factors. While one document\nin the ranking might rely on term matching with the query, another document in the same ranking\nmight be deemed relevant by the same ranking model due to the proximity of query terms in the\ndocument. Therefore, rankings with multiple and sometimes conflicting relevance factors for a\nsingle simple surrogate model might not be able to provide high fidelity.\n10.1.2 Disentangling Explanations. Many of the feature attribution methods provide one explana-\ntion, but complex machine learning models learn multiple features for the same behavior, which"
    },
    {
        "vector_id": 5379,
        "text": "are also difficult to disentangle. This problem is exaggerated when it is coupled with the problem\nof correlated reasons. Specifically, many relevance factors are known to be correlated. A document\nthat exhibits high semantic similarity with the query might also have a high term-level matching\nscore. In these cases, it is likely that the methods covered in this survey (for example, probing\napproaches) will not be able to disentangle the effects of the underlying relevant factors from each\nother.\nPre-print 26 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n10.2 Open Questions\nNow we turn to some of the open questions in the area of explainable information retrieval. We\ndivide the questions into three main categories - types of explanations, explaining user models,\nevaluation of explanations, causal explanations and the utility of explanations.\n10.2.1 Explanation by Example. As discussed before, most of the explanation methods have been"
    },
    {
        "vector_id": 5380,
        "text": "limited to explaining the feature space \u2013 words, sentences, or numerical features in learning to rank\ntasks. Prominent among these are attribution methods and hard masking techniques. However,\ndata attribution or instance attributes and methods have not been explored in the context of\ninformation retrieval tasks. Current papers that deal with explanation by examples are limited to\nfinding adversarial examples of generated text documents that result in errors of contextual rankers.\nInstance attribution methods attribute the rationale or reason behind the prediction back to the\ninput instances in the training data. Examples of instance attribution methods include influence\nfunctions and data poisoning methods. The interpretability question that instance attribution\nmethod methods answer is which of the input instances in the training data is responsible for\ntraining the model in a certain way to cause the following decision."
    },
    {
        "vector_id": 5381,
        "text": "For a document retrieval task, the interpretability question could be \u201cwhich of the queries in\nthe training set affect a certain test query ?\u201d. The output of instance attribution tasks can result in\nisolating mislabelled training instances, identifying dataset biases, and providing insights into query\nrepresentation of the complex encoders. Other types of explanations can be generated explanations\nfor the stakeholders who are end users. These generative explanations can take the form of fully-\nfledged natural language that is composed of extractive explanations, feature attributions, or even\ninstance attribution methods.\n10.2.2 Explaining User Models. Personalized ranking models tailor the search result list according\nto a user\u2019s profile as inferred by the search engine. While it is useful, modern personalization\ntechniques cause growing anxiety in their users \u2013 \u201cWhy am I seeing these search results? What\ndoes the search engine believe my interests are?\u201d"
    },
    {
        "vector_id": 5382,
        "text": "does the search engine believe my interests are?\u201d\nSearch engines have recently come under increased scrutiny due to their influence on people\u2019s\ndecision-making in critical scenarios such as health and elections. Search personalization typically\ninvolves tailoring the ranking of results for individual users based on models of their past preferences\nand interests. Consequently, there is a growing concern in users due to the possible negative effects\nof personalization that include bias [ 50, 84], filter bubbles [41, 48, 88] and increased opacity of\nthe ranking mechanism. Modern personalization techniques are based on learning an effective\nrepresentation of a user by mining sensitive behavioral data like click-throughs [62], query logs [21]\nand topical interests [51] from social media. Given today\u2019s landscape of partisan news coupled with\nthe fact that commercial search engines do not highlight personalized results, there is a real need"
    },
    {
        "vector_id": 5383,
        "text": "to help us better understand what a search engine infers about its users. Specifically, an interesting\ninterpretability question to ask is what does the search engine perceive the user as when they issue a\ncertain query? This manner of post-hoc auditing of retrieval models can be useful in a variety of\ndownstream bias detection and validation applications.\n10.2.3 Evaluation of Explanations. Evaluation of explanations is a general problem in the area\nof interpretable machine learning. There is a large spectrum of evaluation methods, starting\nfrom functionally grounded evaluations to Human-centered evaluation in the wider domain of\nmachine learning and natural language processing. However, in information retrieval, most of the\nexplanation evaluation techniques have focused on functionally grounded evaluation. Approaches\nthat we reviewed in this paper propose and evaluate explanation methods by their fidelity, validity,"
    },
    {
        "vector_id": 5384,
        "text": "completeness, and human congruence. We refer to these methods as intrinsic methods.\nPre-print Explainable Information Retrieval: A Survey 27\nA deeper problem lies in the absence of ground truth for evaluating or validating the accuracy of\noutput explanations of post-hoc methods. Unfortunately, this leads to a chicken-and-egg problem\nthat is hard to fix \u2013 to evaluate an explanation, one needs a mechanism to generate or collect ground\ntruth, which in the first place is the objective of the interpretability task . If we indeed have a procedure\nto create ground-truth explanations from a black-box model, that is, to determine what exactly the\nmodel pays attention to, then we would have solved the problem. Note that this is in stark contrast\nto standard ML tasks, where the ground-truth are indeed the observed variables that are explicitly\nspecified in the data. While intrinsic methods in the absence of ground-truth explanations are"
    },
    {
        "vector_id": 5385,
        "text": "reasonable proxies, they still do not answer the utility question of explanations \u2013 that is, to what\nextent do the explanations assist the end-user in performing a given task. Examples of tasks depend\nupon the stakeholder. For a machine learning expert, the task can be explanation-based model\ndebugging, while for an end-user the question would be why the machine learning model ranks an\nobviously relevant document lower than an irrelevant document. Apart from these open questions,\nwe believe that there is ample opportunity for explainable IR methods to many vertical search\napplications like medical search [49], high-recall search [22], scholarly and historical search [55\u201357].\nApart from specialized search application, explainable IR has direct applications in knowledge\nintensive tasks that use an information retrieval component like fact checking [40, 93, 147], question\nanswering [105], entity addition [114].\n11 CONCLUSION"
    },
    {
        "vector_id": 5386,
        "text": "11 CONCLUSION\nWe provided an extensive investigation into the state of ExIR research. We fill a distinct gap in the\nIR literature to curate, organize, and synthesize works relating to explainability of learning systems.\nOur analysis reveals that while post-hoc interpretability was heavily researched in the initial years,\ncurrent efforts are trying to propose approaches that are interpretable by design (IBD). Due to a\nvariety of design choices in IBD models, we find that authors are often vague about the extent\nand style of interpretability in their IBD approaches. We explored the feature-attribution, free-\ntext generation, and adversarial examples for post-hoc interpretability. Moreover, we summarize\nmethods that make use of well-established IR principles to explain and probe ranking models.\nFinally, we explored the two major subtypes of IBD methods for IR tasks. Based on our findings,\nwe reflect on the design trade-offs and experimental protocols that are used in evaluating ExIR"
    },
    {
        "vector_id": 5387,
        "text": "approaches. In the end, we present some limitations and open questions that we foresee as the next\nsteps toward building transparent, trustworthy search systems.\nACKNOWLEDGMENTS\nWe acknowledge all the helpful comments from the anonymous reviewers, and funding from DFG\nAN 996/1-1.\nREFERENCES\n[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks\nfor Saliency Maps. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , Samy Bengio, Hanna M. Wallach, Hugo\nLarochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett (Eds.). 9525\u20139536. https://proceedings.\nneurips.cc/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html\n[2] Julius Adebayo, Michael Muelly, Harold Abelson, and Been Kim. 2022. Post hoc Explanations may be Ineffective for"
    },
    {
        "vector_id": 5388,
        "text": "Detecting Unknown Spurious Correlation. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=xNOVfCCvDpM\n[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on\nMeasuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357\u2013389. https://doi.org/10.1145/\n582415.582416\nPre-print 28 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[4] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech\nSamek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS\none 10, 7 (2015), e0130140.\n[5] Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining A Black-box By Using A Deep"
    },
    {
        "vector_id": 5389,
        "text": "Variational Information Bottleneck Approach. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13\n(2021), 11396\u201311404. https://doi.org/10.1609/aaai.v35i13.17358\n[6] Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary\nVariables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for\nComputational Linguistics, Florence, Italy, 2963\u20132977. https://doi.org/10.18653/v1/P19-1284\n[7] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2021. \"Will You Find\nThese Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification. ArXiv\npreprint abs/2111.07367 (2021). https://arxiv.org/abs/2111.07367\n[8] Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation"
    },
    {
        "vector_id": 5390,
        "text": "when we have saliency methods?. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online, 149\u2013155. https://doi.org/10.18653/v1/\n2020.blackboxnlp-1.14\n[9] Yonatan Belinkov. 2022. Probing Classifiers: Promises, Shortcomings, and Advances. Comput. Linguistics 48, 1 (2022),\n207\u2013219. https://doi.org/10.1162/coli_a_00422\n[10] Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval . 222\u2013229.\n[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri,\nJos\u00e9 M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920). Association for Computing"
    },
    {
        "vector_id": 5391,
        "text": "Machinery, New York, NY, USA, 648\u2013657. https://doi.org/10.1145/3351095.3375624\n[12] Adrien Bibal, R\u00e9mi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas Fran\u00e7ois, and Patrick Watrin. 2022.\nIs Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Dublin, Ireland,\n3889\u20133900. https://doi.org/10.18653/v1/2022.acl-long.269\n[13] Amin Bigdeli, Negar Arabzadeh, Shirin Seyedsalehi, Morteza Zihayat, and Ebrahim Bagheri. 2022. Gender Fairness in\nInformation Retrieval Systems. InSIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3436\u20133439. https://doi.org/10.1145/3477495.3532680\n[14] Alexander Bondarenko, Maik Fr\u00f6be, Jan Heinrich Reimer, Benno Stein, Michael V\u00f6lske, and Matthias Hagen. 2022."
    },
    {
        "vector_id": 5392,
        "text": "Axiomatic Retrieval Experimentation with ir_axioms. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 3131\u20133140. https:\n//doi.org/10.1145/3477495.3531743\n[15] Leonid Boytsov and Zico Kolter. 2021. Exploring classic and neural lexical translation models for information retrieval:\nInterpretability, effectiveness, and efficiency benefits. In European Conference on Information Retrieval . Springer,\n63\u201378.\n[16] Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In Proceedings\nof SIGIR Forum 1994 . 112\u2013121.\n[17] Jie Cai, Zhengzhou Zhu, Ping Nie, and Qian Liu. 2020. A Pairwise Probe for Understanding BERT Fine-Tuning on\nMachine Reading Comprehension. In Proceedings of the 43rd International ACM SIGIR conference on research and"
    },
    {
        "vector_id": 5393,
        "text": "development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi\nCheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1665\u20131668. https://doi.org/10.1145/\n3397271.3401195\n[18] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://lemurproject.org/clueweb09/\n[19] Arthur C\u00e2mara and Claudia Hauff. 2020. Diagnosing BERT with Retrieval Heuristics. In Proceedings of ECIR 2020 ,\nVol. 12035. Springer, 605\u2013618.\n[20] Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language\nInference with Natural Language Explanations. In Advances in Neural Information Processing Systems 31: Annual\nConference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada .\n9560\u20139572. https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html"
    },
    {
        "vector_id": 5394,
        "text": "[21] Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization\nusing topic models. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM\n2010, Toronto, Ontario, Canada, October 26-30, 2010 . ACM, 1849\u20131852. https://doi.org/10.1145/1871437.1871745\nPre-print Explainable Information Retrieval: A Survey 29\n[22] Manajit Chakraborty, David Zimmermann, and Fabio Crestani. 2021. PatentQuest: A User-Oriented Tool for Integrated\nPatent Search. In Proceedings of the 11th International Workshop on Bibliometric-enhanced Information Retrieval co-\nlocated with 43rd European Conference on Information Retrieval (ECIR 2021), Lucca, Italy (online only), April 1st, 2021\n(CEUR Workshop Proceedings, Vol. 2847) . CEUR-WS.org, 89\u2013101. http://ceur-ws.org/Vol-2847/paper-09.pdf\n[23] Ankush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. Semeval-2019 task 3: Emocon-"
    },
    {
        "vector_id": 5395,
        "text": "text contextual emotion detection in text. In Proceedings of the 13th international workshop on semantic evaluation .\n39\u201348.\n[24] Jia Chen, Yiqun Liu, Yan Fang, Jiaxin Mao, Hui Fang, Shenghao Yang, Xiaohui Xie, Min Zhang, and Shaoping Ma.\n2022. Axiomatically Regularized Pre-training for Ad hoc Search. In SIGIR \u201922: The 45th International ACM SIGIR\nConference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 1524\u20131534.\nhttps://doi.org/10.1145/3477495.3531943\n[25] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to Explain: An Information-\nTheoretic Perspective on Model Interpretation. InProceedings of the 35th International Conference on Machine Learning,\nICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018 , Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.\nPMLR. http://proceedings.mlr.press/v80/chen18j.html"
    },
    {
        "vector_id": 5396,
        "text": "[26] Zitong Cheng and Hui Fang. 2020. Utilizing Axiomatic Perturbations to Guide Neural Ranking Models. In ICTIR \u201920:\nThe 2020 ACM SIGIR International Conference on the Theory of Information Retrieval, Virtual Event, Norway, September\n14-17, 2020 . ACM, 153\u2013156. https://doi.org/10.1145/3409256.3409828\n[27] Jaekeol Choi, Euna Jung, Sungjun Lim, and Wonjong Rhee. 2022. Finding Inverse Document Frequency Information\nin BERT. ArXiv preprint abs/2202.12191 (2022). https://arxiv.org/abs/2202.12191\n[28] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning\ntrack. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662\n[29] Ronan Cummins and Colm O\u2019Riordan. 2007. An Axiomatic Comparison of Learned Term-Weighting Schemes in\nInformation Retrieval: Clarifications and Extensions. Artif. Intell. Rev. 28, 1 (2007), 51\u201368."
    },
    {
        "vector_id": 5397,
        "text": "[30] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching\nN-Grams in Ad-hoc Search. InProceedings of the Eleventh ACM International Conference on Web Search and Data Mining,\nWSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 . ACM, 126\u2013134. https://doi.org/10.1145/3159652.3159659\n[31] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C.\nWallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4443\u20134458.\nhttps://doi.org/10.18653/v1/2020.acl-main.408\n[32] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. https:\n//doi.org/10.48550/ARXIV.1702.08608\n[33] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text"
    },
    {
        "vector_id": 5398,
        "text": "Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). Association for Computational Linguistics, Melbourne, Australia, 31\u201336. https://doi.org/10.18653/v1/P18-2006\n[34] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation\nwith Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics 9 (2021), 160\u2013175.\nhttps://doi.org/10.1162/tacl_a_00359\n[35] Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on\nRelevance Modeling in Information Retrieval. In WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana,\nSlovenia, April 19-23, 2021 . ACM / IW3C2, 1053\u20131064. https://doi.org/10.1145/3442381.3450009\n[36] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. InProceedings of\nSIGIR 2004 . 49\u201356."
    },
    {
        "vector_id": 5399,
        "text": "SIGIR 2004 . 49\u201356.\n[37] Hui Fang, Tao Tao, and ChengXiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans.\nInf. Syst. 29, 2 (2011), 7:1\u20137:42.\n[38] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval.\nIn Proceedings of SIGIR 2006 . 115\u2013122.\n[39] Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval\nModels using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development\nin Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 1005\u20131008. https://doi.org/10.1145/3331184.\n3331312\n[40] Besnik Fetahu, Katja Markert, Wolfgang Nejdl, and Avishek Anand. 2016. Finding news citations for wikipedia.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 337\u2013346.\nhttps://doi.org/10.1145/2983323.2983808"
    },
    {
        "vector_id": 5400,
        "text": "https://doi.org/10.1145/2983323.2983808\n[41] Seth Flaxman, Sharad Goel, and Justin M Rao. 2016. Filter bubbles, echo chambers, and online news consumption.\nPublic opinion quarterly 80, S1 (2016), 298\u2013320.\nPre-print 30 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[42] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021. A White Box Analysis of ColBERT. InAdvances\nin Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021,\nProceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) . Springer, 257\u2013263. https://doi.org/10.1007/978-3-\n030-72240-1_23\n[43] Thibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2022. Match Your Words! A Study of Lexical\nMatching in Neural Information Retrieval. In Advances in Information Retrieval - 44th European Conference on IR"
    },
    {
        "vector_id": 5401,
        "text": "Research, ECIR 2022, Stavanger, Norway, April 10-14, 2022, Proceedings, Part II (Lecture Notes in Computer Science,\nVol. 13186). Springer, 120\u2013127. https://doi.org/10.1007/978-3-030-99739-7_14\n[44] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge,\nand Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 11 (2020), 665\u2013673.\nhttps://doi.org/10.1038/s42256-020-00257-z\n[45] Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2020. Ranking-Incentivized Quality Preserving\nContent Modification. In Proceedings of the 43rd International ACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 259\u2013268. https://doi.org/10.1145/\n3397271.3401058\n[46] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc"
    },
    {
        "vector_id": 5402,
        "text": "Retrieval. In Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM\n2016, Indianapolis, IN, USA, October 24-28, 2016 . ACM, 55\u201364. https://doi.org/10.1145/2983323.2983769\n[47] Matthias Hagen, Michael V\u00f6lske, Steve G\u00f6ring, and Benno Stein. 2016. Axiomatic Result Re-Ranking. In Proceedings\nof the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN,\nUSA, October 24-28, 2016 . ACM, 721\u2013730. https://doi.org/10.1145/2983323.2983704\n[48] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the filter bubble? Effects of personalization on\nthe diversity of Google News. Digital Journalism 6, 3 (2018), 330\u2013343.\n[49] David A Hanauer. 2006. EMERSE: the electronic medical record search engine. InAMIA annual symposium proceedings ,\nVol. 2006. American Medical Informatics Association, 941."
    },
    {
        "vector_id": 5403,
        "text": "[50] Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, Alan Mislove, and\nChristo Wilson. 2013. Measuring personalization of web search. In 22nd International World Wide Web Conference,\nWWW \u201913, Rio de Janeiro, Brazil, May 13-17, 2013 . International World Wide Web Conferences Steering Committee /\nACM, 527\u2013538. https://doi.org/10.1145/2488388.2488435\n[51] Morgan Harvey, Fabio Crestani, and Mark James Carman. 2013. Building user profiles from topic models for\npersonalised search. In 22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San\nFrancisco, CA, USA, October 27 - November 1, 2013 . ACM, 2309\u20132314. https://doi.org/10.1145/2505515.2505642\n[52] John Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural"
    },
    {
        "vector_id": 5404,
        "text": "Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 2733\u20132743.\nhttps://doi.org/10.18653/v1/D19-1275\n[53] Sebastian Hofst\u00e4tter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-Document\nCascading: Learning to Select Passages for Neural Document Ranking. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021 . ACM,\n1349\u20131358. https://doi.org/10.1145/3404835.3462889\n[54] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2020. Interpretable & Time-Budget-Constrained Contex-\ntualization for Re-Ranking. In ECAI 2020 . IOS Press, 513\u2013520.\n[55] Helge Holzmann and Avishek Anand. 2016. Tempas: Temporal archive search based on tags. InProceedings of the\n25th International Conference Companion on World Wide Web . 207\u2013210. https://doi.org/10.1145/2872518.2890555"
    },
    {
        "vector_id": 5405,
        "text": "[56] Helge Holzmann, Wolfgang Nejdl, and Avishek Anand. 2017. Exploring web archives through temporal anchor texts.\nIn Proceedings of the 2017 ACM on Web Science Conference . 289\u2013298.\n[57] H. Holzmann, W. Nejdl, and A. Anand. 2017. Exploring web archives through temporal anchor texts. In Proceedings\nof the 2017 ACM on Web Science Conference . 289\u2013298. https://doi.org/10.1145/3091478.3091500\n[58] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods\nin Deep Neural Networks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada . 9734\u20139745. https:\n//proceedings.neurips.cc/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html\n[59] Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, and Avishek Anand. 2021. Towards Benchmarking the Utility of"
    },
    {
        "vector_id": 5406,
        "text": "Explanations for Model Debugging. In Proceedings of the First Workshop on Trustworthy Natural Language Processing .\nAssociation for Computational Linguistics, Online, 68\u201373. https://doi.org/10.18653/v1/2021.trustnlp-1.8\n[60] Alon Jacovi and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of\nthe Association for Computational Linguistics 9 (2021), 294\u2013310. https://doi.org/10.1162/tacl_a_00367\nPre-print Explainable Information Retrieval: A Survey 31\n[61] Zhiying Jiang, Raphael Tang, Ji Xin, and Jimmy Lin. 2021. How Does BERT Rerank Passages? An Attribution Analysis\nwith Information Bottlenecks. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Punta Cana, Dominican Republic, 496\u2013509.\nhttps://doi.org/10.18653/v1/2021.blackboxnlp-1.39"
    },
    {
        "vector_id": 5407,
        "text": "https://doi.org/10.18653/v1/2021.blackboxnlp-1.39\n[62] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM\nSIGKDD international conference on Knowledge discovery and data mining . ACM, 133\u2013142.\n[63] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late\nInteraction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in\nInformation Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 39\u201348. https://doi.org/10.1145/3397271.\n3401075\n[64] Pang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) . PMLR, 1885\u20131894. http://proceedings.mlr.press/v70/koh17a.html"
    },
    {
        "vector_id": 5408,
        "text": "[65] Sawan Kumar and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language\nExplanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 8730\u20138742. https://doi.org/10.18653/v1/2020.acl-main.771\n[66] Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the Usage\nof Grammatical Number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 . Association for Computational Linguistics,\n8818\u20138831. https://doi.org/10.18653/v1/2022.acl-long.603\n[67] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval (New Orleans, Louisiana,"
    },
    {
        "vector_id": 5409,
        "text": "USA) (SIGIR \u201901) . Association for Computing Machinery, New York, NY, USA, 120\u2013127. https://doi.org/10.1145/\n383952.383972\n[68] Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring Which Medical Treatments Work\nfrom Reports of Clinical Trials. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, Minneapolis, Minnesota, 3705\u20133717. https://doi.org/10.18653/v1/N19-1371\n[69] Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Austin,\nTexas, 107\u2013117. https://doi.org/10.18653/v1/D16-1011\n[70] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2021. Learnt Sparsity for Effective and Interpretable Document"
    },
    {
        "vector_id": 5410,
        "text": "Ranking. ArXiv preprint abs/2106.12460 (2021). https://arxiv.org/abs/2106.12460\n[71] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 7871\u20137880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[72] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches\nOut. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013\n[73] Jimmy Lin. 2019. The neural hype, justified!: a recantation. SIGIR Forum 53, 2 (2019), 88\u201393. https://doi.org/10.1145/\n3458553.3458563"
    },
    {
        "vector_id": 5411,
        "text": "3458553.3458563\n[74] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explanation Framework\nfor Text Classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 5570\u20135581. https://doi.org/10.18653/v1/P19-1560\n[75] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Alberto Veneri. 2022. ILMART:\nInterpretable Ranking with Constrained LambdaMART. In SIGIR \u201922: The 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 2255\u20132259. https:\n//doi.org/10.1145/3477495.3531840\n[76] Scott M. Lundberg, Gabriel G. Erion, and Su-In Lee. 2018. Consistent Individualized Feature Attribution for Tree\nEnsembles. ArXiv preprint abs/1802.03888 (2018). https://arxiv.org/abs/1802.03888"
    },
    {
        "vector_id": 5412,
        "text": "[77] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA . 4765\u20134774. https://proceedings.neurips.cc/paper/2017/hash/\n8a20a8621978632d76c43dfd28b67767-Abstract.html\n[78] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing\nthe Behavior of Neural IR Models. ArXiv preprint abs/2011.00696 (2020). https://arxiv.org/abs/2011.00696\nPre-print 32 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[79] Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, and Siva Reddy. 2021. Evaluating the Faithfulness of Importance\nMeasures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. ArXiv preprint abs/2110.08412\n(2021). https://arxiv.org/abs/2110.08412"
    },
    {
        "vector_id": 5413,
        "text": "(2021). https://arxiv.org/abs/2110.08412\n[80] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-\nQuery Interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Brussels, Belgium, 1849\u20131860. https://doi.org/10.18653/v1/D18-1211\n[81] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artif. Intell. 267 (2019), 1\u201338.\nhttps://doi.org/10.1016/j.artint.2018.07.007\n[82] Christoph Molnar. 2022. Interpretable Machine Learning (2 ed.). https://christophm.github.io/interpretable-ml-book\n[83] Abbe Mowshowitz and Akira Kawaguchi. 2005. Measuring search engine bias. Inf. Process. Manag. 41, 5 (2005),\n1193\u20131205. https://doi.org/10.1016/j.ipm.2004.05.005\n[84] Eni Mustafaraj, Emma Lurie, and Claire Devine. 2020. The case for voter-centered audits of search engines during"
    },
    {
        "vector_id": 5414,
        "text": "political elections. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 559\u2013569.\n[85] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving Document Ranking with Dual\nWord Embeddings. In Proceedings of the 25th International Conference Companion on World Wide Web (Montr\u00e9al,\nQu\u00e9bec, Canada) (WWW \u201916 Companion) . International World Wide Web Conferences Steering Committee, Republic\nand Canton of Geneva, CHE, 83\u201384. https://doi.org/10.1145/2872518.2889361\n[86] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A Human Generated MAchine Reading COmprehension Dataset. InProceedings of the Workshop on Cognitive\nComputation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) ,"
    },
    {
        "vector_id": 5415,
        "text": "Tarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-\nws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf\n[87] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation\nof Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311\u2013318. https://doi.org/10.3115/\n1073083.1073135\n[88] Eli Pariser. 2011. The filter bubble: How the new personalized web is changing what we read and how we think . Penguin.\n[89] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\nJernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a\nBenchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American"
    },
    {
        "vector_id": 5416,
        "text": "Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational\nLinguistics, Online, 2523\u20132544. https://doi.org/10.18653/v1/2021.naacl-main.200\n[90] Tiago Pimentel, Naomi Saphra, Adina Williams, and Ryan Cotterell. 2020. Pareto Probing: Trading Off Accuracy\nfor Complexity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 3138\u20133153. https://doi.org/10.18653/v1/2020.emnlp-main.254\n[91] Sayantan Polley, Atin Janki, Juliane Thiel, Marcusand Hoebel-Mueller, and Andreas Nuernberger. 2021. ExDocS:\nEvidence based Explainable Document Search. In ACM SIGIR Workshop on Causality in Search and Recommendation .\nACM. https://csr21.github.io/polley-csr2021.pdf\n[92] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the"
    },
    {
        "vector_id": 5417,
        "text": "21st annual international ACM SIGIR conference on Research and development in information retrieval . ACM, 275\u2013281.\n[93] Adithya Pratapa, Sai Muralidhar Jayanthi, and Kavya Nerella. 2020. Constrained fact verification for FEVER. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7826\u20137832.\n[94] Alberto Purpura, Karolina Buchner, Gianmaria Silvello, and Gian Antonio Susto. 2021. Neural feature selection for\nlearning to rank. In European Conference on Information Retrieval . Springer, 342\u2013349.\n[95] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.\nArXiv preprint abs/1904.07531 (2019). https://arxiv.org/abs/1904.07531\n[96] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets. CoRR abs/1306.2597 (2013).\n[97] Razieh Rahimi, Youngwoo Kim, Hamed Zamani, and James Allan. 2021. Explaining Documents\u2019 Relevance to Search"
    },
    {
        "vector_id": 5418,
        "text": "Queries. ArXiv preprint abs/2111.01314 (2021). https://arxiv.org/abs/2111.01314\n[98] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging\nLanguage Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . Association for Computational Linguistics, Florence, Italy, 4932\u20134942. https://doi.org/10.\n18653/v1/P19-1487\n[99] Nisarg Raval and Manisha Verma. 2020. One word at a time: adversarial attacks on retrieval models. ArXiv preprint\nabs/2008.02197 (2020). https://arxiv.org/abs/2008.02197\nPre-print Explainable Information Retrieval: A Survey 33\n[100] Navid Rekabsaz and Markus Schedl. 2020. Do Neural Ranking Models Intensify Gender Bias?. In Proceedings of the\n43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual\nEvent, China, July 25-30, 2020 . ACM, 2065\u20132068. https://doi.org/10.1145/3397271.3401280"
    },
    {
        "vector_id": 5419,
        "text": "[101] Dani\u00ebl Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models.\nIn Proceedings of ECIR 2019 . 489\u2013503.\n[102] Marco T\u00falio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\": Explaining the Predictions\nof Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C.\nAggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135\u20131144. https://doi.org/10.1145/2939672.2939778\n[103] Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. 2022. A Consistent and Efficient\nEvaluation Strategy for Attribution Methods. In International Conference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research, Vol. 162) . PMLR, 18770\u201318795.\nhttps://proceedings.mlr.press/v162/rong22a.html"
    },
    {
        "vector_id": 5420,
        "text": "https://proceedings.mlr.press/v162/rong22a.html\n[104] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An Axiomatic\nApproach to Regularizing Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR Conference\non Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 . ACM, 981\u2013984.\nhttps://doi.org/10.1145/3331184.3331296\n[105] Rishiraj Saha Roy and Avishek Anand. 2021. Question Answering for the Curated Web: Tasks and Methods in QA\nover Knowledge Bases and Text Collections. Synthesis Lectures onSynthesis Lectures on Information Concepts, Retrieval,\nand Services 13, 4 (2021), 1\u2013194. https://doi.org/10.1007/978-3-031-79512-1\n[106] C. Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable\nmodels instead. Nature Machine Intelligence 1, 5 (2019), 206."
    },
    {
        "vector_id": 5421,
        "text": "[107] Wojciech Samek, Gr\u00e9goire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M\u00fcller (Eds.). 2019.\nExplainable AI: Interpreting, Explaining and Visualizing Deep Learning . Lecture Notes in Computer Science, Vol. 11700.\nSpringer. https://doi.org/10.1007/978-3-030-28954-6\n[108] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-Symbolic Artificial Intelligence.\nAI Commun. 34, 3 (2021), 197\u2013209. https://doi.org/10.3233/AIC-210084\n[109] Procheta Sen, Debasis Ganguly, Manisha Verma, and Gareth J. F. Jones. 2020. The Curious Case of IR Explainability:\nExplaining Document Scores within and across Ranking Models. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 .\nACM, 2069\u20132072. https://doi.org/10.1145/3397271.3401286\n[110] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating"
    },
    {
        "vector_id": 5422,
        "text": "Activation Differences. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye\nTeh (Eds.). PMLR, 3145\u20133153. http://proceedings.mlr.press/v70/shrikumar17a.html\n[111] Jaspreet Singh and Avishek Anand. 2018. Posthoc Interpretability of Learning to Rank Models using Secondary\nTraining Data. In Workshop on ExplainAble Recommendation and Search (EARS 2018) at SIGIR 2018 . https://ears2018.\ngithub.io/ears18-singh.pdf\n[112] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In\nProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC,\nAustralia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM,\n770\u2013773. https://doi.org/10.1145/3289600.3290620"
    },
    {
        "vector_id": 5423,
        "text": "770\u2013773. https://doi.org/10.1145/3289600.3290620\n[113] Jaspreet Singh and Avishek Anand. 2020. Model agnostic interpretability of rankers via intent modelling. InProceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency . 618\u2013628. https://doi.org/10.1145/3351095.3375234\n[114] Jaspreet Singh, Johannes Hoffart, and Avishek Anand. 2016. Discovering entities with just a little help from you.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 1331\u20131340.\nhttps://doi.org/10.1145/2983323.2983798\n[115] Jaspreet Singh, Megha Khosla, Wang Zhenye, and Avishek Anand. 2021. Extracting per Query Valid Explanations\nfor Blackbox Learning-to-Rank Models. In Proceedings of the 2021 ACM SIGIR International Conference on Theory of\nInformation Retrieval (Virtual Event, Canada) (ICTIR \u201921) . Association for Computing Machinery, New York, NY, USA,\n203\u2013210. https://doi.org/10.1145/3471158.3472241"
    },
    {
        "vector_id": 5424,
        "text": "203\u2013210. https://doi.org/10.1145/3471158.3472241\n[116] Jaspreet Singh, Wolfgang Nejdl, and Avishek Anand. 2016. Expedition: a time-aware exploratory search system\ndesigned for scholars. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in\nInformation Retrieval. 1105\u20131108. https://doi.org/10.1145/2911451.2911465\n[117] Anders S\u00f8gaard. 2021. Explainable Natural Language Processing . Morgan & Claypool Publishers. https://doi.org/10.\n2200/S01118ED1V01Y202107HLT051\nPre-print 34 Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang\n[118] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806 (2014).\n[119] Pascal Sturmfels, Scott Lundberg, and Su-In Lee. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill"
    },
    {
        "vector_id": 5425,
        "text": "(2020). https://doi.org/10.23915/distill.00022 https://distill.pub/2020/attribution-baselines.\n[120] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In Proceedings\nof the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017\n(Proceedings of Machine Learning Research, Vol. 70) , Doina Precup and Yee Whye Teh (Eds.). PMLR, 3319\u20133328.\nhttp://proceedings.mlr.press/v70/sundararajan17a.html\n[121] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics,\nFlorence, Italy, 4593\u20134601. https://doi.org/10.18653/v1/P19-1452\n[122] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van"
    },
    {
        "vector_id": 5426,
        "text": "Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for\nsentence structure in contextualized word representations. In7th International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?id=SJzSgnRcKX\n[123] Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun D. Preece. 2020. Sanity Checks\nfor Saliency Metrics. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, February 7-12, 2020 .\nAAAI Press, 6021\u20136029. https://aaai.org/ojs/index.php/AAAI/article/view/6064\n[124] Betty van Aken, Benjamin Winter, Alexander L\u00f6ser, and Felix A. Gers. 2019. How Does BERT Answer Questions?: A\nLayer-Wise Analysis of Transformer Representations. In Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM 2019, Beijing, China, November 3-7, 2019 . ACM, 1823\u20131832. https:"
    },
    {
        "vector_id": 5427,
        "text": "//doi.org/10.1145/3357384.3358028\n[125] Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019,\nParis, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, \u00c9ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and\nFalk Scholer (Eds.). ACM, 1281\u20131284. https://doi.org/10.1145/3331184.3331377\n[126] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations . Association for Computational\nLinguistics, Florence, Italy, 37\u201342. https://doi.org/10.18653/v1/P19-3007\n[127] Elena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. In Proceedings"
    },
    {
        "vector_id": 5428,
        "text": "of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 183\u2013196. https://doi.org/10.18653/v1/2020.emnlp-main.14\n[128] Michael V\u00f6lske, Alexander Bondarenko, Maik Fr\u00f6be, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek Anand.\n2021. Towards Axiomatic Explanations for Neural Ranking Models. In ICTIR \u201921: The 2021 ACM SIGIR International\nConference on the Theory of Information Retrieval, Virtual Event, Canada, July 11, 2021 , Faegheh Hasibi, Yi Fang, and\nAkiko Aizawa (Eds.). ACM, 13\u201322. https://doi.org/10.1145/3471158.3472256\n[129] Ellen M Voorhees. 2006. The TREC 2005 robust track. In ACM SIGIR Forum , Vol. 40. ACM New York, NY, USA, 41\u201348.\n[130] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers\nfor Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language"
    },
    {
        "vector_id": 5429,
        "text": "Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for\nComputational Linguistics, Hong Kong, China, 2153\u20132162. https://doi.org/10.18653/v1/D19-1221\n[131] Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020. BERTnesia: Investigating the capture and forgetting of\nknowledge in BERT. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks\nfor NLP . Association for Computational Linguistics, Online, 174\u2013183. https://doi.org/10.18653/v1/2020.blackboxnlp-\n1.17\n[132] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT Rankers are Brittle: A Study using Adversarial Document\nPerturbations. In Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval .\n115\u2013120.\n[133] Sarah Wiegreffe and Ana Marasovic. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural"
    },
    {
        "vector_id": 5430,
        "text": "Language Processing. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks ,\nJ. Vanschoren and S. Yeung (Eds.), Vol. 1. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/\n698d51a19d8a121ce581499d7b701668-Paper-round1.pdf\n[134] Maksymilian Wojtas and Ke Chen. 2020. Feature Importance Ranking for Deep Learning. In Advances in Neural\nInformation Processing Systems , Vol. 33. Curran Associates, Inc., 5105\u20135114. https://proceedings.neurips.cc/paper/\n2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf\n[135] Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2022. PRADA: Practical\nBlack-Box Adversarial Attacks against Neural Ranking Models. ArXiv preprint abs/2204.01321 (2022). https:\nPre-print Explainable Information Retrieval: A Survey 35\n//arxiv.org/abs/2204.01321\n[136] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking"
    },
    {
        "vector_id": 5431,
        "text": "with Kernel Pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 . ACM, 55\u201364. https://doi.org/10.1145/3077136.3080809\n[137] Mengjiao Yang and Been Kim. 2019. Benchmarking Attribution Methods with Relative Feature Importance. CoRR\nabs/1907.09701 (2019).\n[138] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation\nGenerator. In SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, Madrid, Spain, July 11 - 15, 2022 . ACM, 669\u2013680. https://doi.org/10.1145/3477495.3532067\n[139] Weijie Yu, Zhongxiang Sun, Jun Xu, Zhenhua Dong, Xu Chen, Hongteng Xu, and Ji-Rong Wen. 2022. Explainable\nLegal Case Matching via Inverse Optimal Transport-based Rationale Extraction. In SIGIR \u201922: The 45th International"
    },
    {
        "vector_id": 5432,
        "text": "ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 . ACM,\n657\u2013668. https://doi.org/10.1145/3477495.3531974\n[140] Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu, and Nick Craswell. 2020. MIMICS: A Large-\nScale Data Collection for Search Clarification. In CIKM \u201920: The 29th ACM International Conference on Information and\nKnowledge Management, Virtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff,\nEdward Curry, and Philippe Cudr\u00e9-Mauroux (Eds.). ACM, 3189\u20133196. https://doi.org/10.1145/3340531.3412772\n[141] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An Analysis of BERT in Document Ranking.\nIn Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,\nSIGIR 2020, Virtual Event, China, July 25-30, 2020 . ACM, 1941\u20131944. https://doi.org/10.1145/3397271.3401325"
    },
    {
        "vector_id": 5433,
        "text": "[142] Kelly Zhang and Samuel Bowman. 2018. Language Modeling Teaches You More than Translation Does: Lessons\nLearned Through Auxiliary Syntactic Task Analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP . Association for Computational Linguistics, Brussels, Belgium,\n359\u2013361. https://doi.org/10.18653/v1/W18-5448\n[143] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent De-\nscription Generation. In CIKM \u201920: The 29th ACM International Conference on Information and Knowledge Management,\nVirtual Event, Ireland, October 19-23, 2020 , Mathieu d\u2019Aquin, Stefan Dietze, Claudia Hauff, Edward Curry, and Philippe\nCudr\u00e9-Mauroux (Eds.). ACM, 1823\u20131832. https://doi.org/10.1145/3340531.3411999\n[144] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text"
    },
    {
        "vector_id": 5434,
        "text": "Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020 . OpenReview.net. https://openreview.net/forum?id=SkeHuCVFDr\n[145] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey and New Perspectives. Found. Trends\nInf. Retr. 14, 1 (2020), 1\u2013101. https://doi.org/10.1561/1500000066\n[146] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. Explain and Predict, and then Predict Again. In WSDM \u201921,\nThe Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 .\nACM, 418\u2013426. https://doi.org/10.1145/3437963.3441758\n[147] Zijian Zhang, Koustav Rudra, and Avishek Anand. 2021. FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable\nModels with HumAn Correction in the Loop. In Proceedings of the 30th ACM International Conference on Information\n& Knowledge Management . 4823\u20134827. https://doi.org/10.1145/3459637.3481985"
    },
    {
        "vector_id": 5435,
        "text": "[148] Wei Zheng and Hui Fang. 2010. Query Aspect Based Term Weighting Regularization in Information Retrieval. In\nProceedings of ECIR 2010 . 344\u2013356.\n[149] Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev, Ethan\nSterling, Nathan Bell, Walker Ravina, and Hai Qian. 2021. Interpretable Ranking with Generalized Additive Models.\nIn WSDM \u201921, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 . ACM, 499\u2013507. https://doi.org/10.1145/3437963.3441796\nPre-print"
    },
    {
        "vector_id": 5436,
        "text": "RAGE Against the Machine: Retrieval-Augmented\nLLM Explanations\nJoel Rorseth\nUniversity of Waterloo\njerorset@uwaterloo.ca\nParke Godfrey\nYork University\ngodfrey@yorku.ca\nLukasz Golab\nUniversity of Waterloo\nlgolab@uwaterloo.ca\nDivesh Srivastava\nAT&T Chief Data Office\ndivesh@research.att.com\nJaroslaw Szlichta\nYork University\nszlichta@yorku.ca\nAbstract\u2014This paper demonstrates RAGE, an interactive tool\nfor explaining Large Language Models (LLMs) augmented with\nretrieval capabilities; i.e., able to query external sources and pull\nrelevant information into their input context. Our explanations\nare counterfactual in the sense that they identify parts of the\ninput context that, when removed, change the answer to the\nquestion posed to the LLM. RAGE includes pruning methods to\nnavigate the vast space of possible explanations, allowing users\nto view the provenance of the produced answers.\nI. I NTRODUCTION\nMotivation. Artificial Intelligence (AI) has seen remarkable"
    },
    {
        "vector_id": 5437,
        "text": "growth in terms of both capability and popularity, exemplified\nby recent large language models (LLMs) such as OpenAI\u2019s\nChatGPT, Microsoft\u2019s Copilot, and Google\u2019s Gemini. The\nrapid progress in LLM capability is driven by scale, as AI re-\nsearchers train increasingly complex models with increasingly\nlarge datasets using enormous computational resources. Within\na short span, state-of-the-art models have progressed from\ntraining millions, to billions, and now to trillions of internal pa-\nrameters. However, this increased complexity further obscures\nthe underlying decision-making process of LLMs, making it\nchallenging to rationalize or troubleshoot their outputs. As\nLLMs are adopted in critical sectors, it is imperative that\nverifiable explanations accompany their outputs, to build trust.\nThe unique enhancements in capability that distinguish\nLLMs from previous language models amplify their explain-\nability concerns. Of particular relevance isretrieval-augmented"
    },
    {
        "vector_id": 5438,
        "text": "generation (RAG), a popular prompt engineering strategy that\nleverages a powerful new LLM capability known as in-context\nlearning. With RAG, an LLM augments its trained knowledge\nby learning from external knowledge sources, supplied directly\nvia the LLM\u2019s input context (prompt). RAG has been pivotal\nfor LLMs in reducing their tendency to hallucinate plausible\nyet incorrect outputs. This complex process, however, obfus-\ncates the provenance of the produced answers.\nBackground. Due to the recency of LLMs and their emer-\ngent capabilities, few efforts have been made to explain their\nphenomena. Under the umbrella of mechanistic interpretabil-\nity, low-level analyses have sought to understand the mecha-\nnisms behind transformer-based language models, and capabil-\nities like in-context learning, by analyzing circuits that form\namongst attention heads, or by assessing an LLM\u2019s ability\nto override trained knowledge [1]. Our explainability focus\u2014"
    },
    {
        "vector_id": 5439,
        "text": "which aims to trace the provenance of LLM answers during\nRAG\u2014instead demands high-level explanations of RAG in\nsimple terms. While specific prompting strategies such as\nchain-of-thought (CoT) prompting could serve as interpretable\nexplanations, RAG has yet to receive dedicated focus in the\nexplainability literature. RAG is a leading prompting strategy\nfor the use of modern LLMs in question answering (QA), as\nCoT and other prompting techniques are less applicable and\nrequire specialized examples. Specific concerns about RAG,\nsuch as the lack of provenance in LLM answers, or the \u201clost\nin the middle\u201d context position bias observed in recent LLMs\n[2], warrant dedicated study under an explainability lens.\nContributions. To fill this gap, we demonstrate RAGE,1\nan interactive tool designed to enable RAG Explainability for\nLLMs.2 Our tool deduces provenance and salience for external\nknowledge sources used during RAG, exposing the in-context"
    },
    {
        "vector_id": 5440,
        "text": "learning behaviors of the LLM. Motivated by our prior work\nusing counterfactual explanations for information retrieval [3],\nwe derive provenance counterfactually by identifying minimal\ncontext perturbations that change an LLM\u2019s output. Our con-\ntributions are summarized as follows.\n1) Answer Origin Explainability. We introduce a novel\nframework to assess the origin of LLM answers, with\nrespect to context knowledge sources, by evaluating coun-\nterfactual source combinations and permutations.\n2) Pruning Strategies. We present inference pruning strate-\ngies to reduce the space of possible counterfactual expla-\nnations, by prioritizing the evaluation of important context\nperturbations.\n3) Interactive Demo. Participants will pose questions to\nan LLM augmented with knowledge sources from real\ndatasets. RAGE will display explanations for RAG scenar-\nios where answers are ambiguous, sourced from inconsis-\ntent external knowledge, or traced through a chronological"
    },
    {
        "vector_id": 5441,
        "text": "sequence. Participants will see how subjective questions,\nsuch as determining the greatest professional tennis player,\ncan be answered differently by an LLM, depending on the\ncombination and order of context sources.\nII. S YSTEM DESCRIPTION\nA. Problem Description\nOpen-book question answering is a task where a system\ndetermines an answer to a given question using common\n1A video is available at https://vimeo.com/877281038.\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\nSearch\nCounterfactual\nSearch\nRetrieval Model\n(Pyserini BM25)\nLlama 2 Chat 7B LLMCounterfactual\nExplanations\nAnswers\nAnalysis\nLucene\nIndex\nUsers RAGE \nWeb App\n(Plotly Dash)\nKnowledge \nLLM \nFig. 1. The architecture of RAGE.\nknowledge about the topic and a provided set of sources. In\nRAGE, we explain how an LLM performs this task, using its\nown pre-trained knowledge and retrieved knowledge sources."
    },
    {
        "vector_id": 5442,
        "text": "A user initiates the process by posing a search query q to\na retrieval model M. Given an index of knowledge sources\nand a relevance threshold k, the retrieval model M scores and\nranks the k most relevant sources from the index with respect\nto query q. The resulting ordering of sources, denoted as Dq,\nforms a sequence we refer to as the context.\nWe combine Dq and q to form a natural language prompt\np for the LLM L. This prompt instructs L to answer question\nq using the information contained within the set of delimited\nsources from Dq. Although p serves as the final and sole input\nto the LLM, we denote the answer a produced by the LLM for\na given query q and the sequence of knowledge sources Dq\nas a = L(q, Dq). We also define S(q, d, Dq) as the relative\nrelevance score of a source d \u2208 Dq with respect to the query\nq and other sources within Dq. To derive explanations, we\nassess the answers generated across various combinations or"
    },
    {
        "vector_id": 5443,
        "text": "permutations of the sources in Dq. We refer in general to these\ntwo methods as context perturbations.\nB. Architecture\nRAGE is an interactive Python web application developed us-\ning the Plotly Dash web framework. We installed the 7B Llama\n2 Chat LLM [4] (meta-llama/Llama-2-7b-chat-hf ) through the\nHugging Face Transformers library. Our software is, however,\nfully compatible with any similar transformer-based LLM. All\nknowledge sources (documents) are retrieved from our locally-\nconfigured document indexes, using a BM25 retrieval model\nfrom the Pyserini retrieval toolkit [5].\nWe run our application on an Ubuntu 22.04 server, with\nan Intel Core i9-7920x CPU, 128GB of DDR4 RAM, and\nan NVIDIA RTX 4090 GPU with 24GB of memory. We use\nPyTorch\u2019s CUDA library to run LLM operations on the GPU.\nIn RAGE, users can generate explanations in terms of two\ncomplementary perturbations: source combinations or source\npermutations. Combinations elucidate how the presence of"
    },
    {
        "vector_id": 5444,
        "text": "sources affects the LLM\u2019s predicted answer, while permuta-\ntions elucidate the effect of their order. Alongside counterfac-\ntual explanations for each answer, RAGE presents a pie chart\nto visualize the distribution of answers, a list of perturbation-\nanswer rules, and a table associating different answers with\nthe perturbations that produced them.\nC. RAG Explanations\nIn generating counterfactuals, RAGE aims to identify minimal\nperturbations to the context that lead to a change in the LLM\u2019s\npredicted answer. Combination-based counterfactual explana-\ntions, which can serve as citations, may be generated using\na top-down or bottom-up search. A top-down counterfactual\nmust remove a combination of sources (subset of Dq) to flip\nthe full-context answer to a target answer. On the other hand, a\nbottom-up counterfactual must retain sources to flip the empty-\ncontext answer to the target answer.\nIn either case, the candidate solution search space is defined"
    },
    {
        "vector_id": 5445,
        "text": "as the set of all combinations of the given sources. We propose\nan iterative algorithm that tests combinations in increasing\norder of subset size. Specifically, we evaluate all combinations\ncontaining k sources before moving on to those with k + 1\nsources. Since there may be multiple combinations of equal\nsize, we iterate through these equal-size combinations in order\nof their estimated relevance. This is calculated as the sum\nof the relative relevance scores of all sources within the\ncombination, which can be expressed as P\nd\u2208Dq S(q, d, Dq).\nTo estimate the relative relevance of a source d \u2208 Dq,\nthe user can select from two scoring methods S. In the first\nmethod, we aggregate the LLM\u2019s attention values, summing\nthem over all internal layers, attention heads, and tokens\ncorresponding to a combination\u2019s constituent sources. In the\nsecond method, we sum the relevance scores produced by the\nretrieval model for each source. Since we only compare scores"
    },
    {
        "vector_id": 5446,
        "text": "for combinations of equal size, there is no need to normalize\ncombination scores by the number of sources.\nTo generate permutation-based counterfactual explanations,\nRAGE searches for the most similar source permutation (with\nrespect to their given order) such that the LLM responds with\na different answer. These explanations quantify the stability\nof the LLM\u2019s answer with respect to the order of the con-\ntext sources, thus revealing any unexpected context position\nbias. Our algorithm generates all length- k permutations for\nthe k sources, then computes Kendall\u2019s Tau rank correlation\ncoefficient for each permutation (with respect to their given\norder in Dq). Once generated and measured, the permutations\nare subsequently sorted and evaluated in decreasing order of\nsimilarity, based on decreasing Kendall\u2019s Tau.\nFor both combinations and permutations, our algorithm\ncontinues until it finds a perturbation that changes the answer,\nor until a maximum number of perturbations have been tested."
    },
    {
        "vector_id": 5447,
        "text": "Before comparing against the original answer, we convert an-\nswers to lowercase, remove punctuation, and trim whitespace.\nTo supplement this counterfactual analysis, we analyze the\nanswers over a selected set of perturbed sources. To obtain a\nset of combinations, RAGE considers all combinations of the\nretrieved sources Dq, or draws a fixed-size random sample of s\ncombinations. Based on the user\u2019s original question, a prompt\nis created for each selected combination, which is then used to\nretrieve corresponding answers from the LLM. After analyzing\nthe answers, RAGE renders a table that groups combinations\nby answer, along with a pie chart illustrating the proportion\nof each answer across all combinations. A rule is determined\nfor each answer, when applicable, identifying sources that\nappeared in all combinations leading to this answer.\nIn a similar manner, the user can instruct RAGE to analyze answers from a selected set of source permutations. The table"
    },
    {
        "vector_id": 5448,
        "text": "and pie chart illustrating associations between answers and\npermutations resemble those of the combination case, with the\nrule calculation adopting a unique definition. For each answer,\nwe determine a rule that identifies any context positions for\nwhich all permutations leading to this answer shared the same\nsource. Users may again choose to analyze all permutations,\nor a fixed-size random sample of s permutations.\nFor the latter, a naive solution might generate all k! permu-\ntations of the k sources, then uniformly sample s permutations,\nresulting in O(k!) time complexity. To improve the efficiency,\nwe propose an implementation using the Fisher\u2013Yates shuffle\nalgorithm [6], which produces an unbiased random permu-\ntation of any finite sequence in O(k). In our approach, we\ninvoke the Fisher-Yates algorithms times to generate s random\npermutations, resulting in an efficient O(ks) solution.\nRAGE also allows the user to analyze the most optimum"
    },
    {
        "vector_id": 5449,
        "text": "permutations. As observed in recent works [2], LLMs often ex-\nhibit a context position bias, paying more attention to sources\nappearing at the beginning and end of the context than those in\nthe middle. As a result, sources that are important for obtaining\na given answer may not receive sufficient consideration by\nthe LLM. Given a distribution of the expected attention paid\nto each position, this \u201clost in the middle\u201d bias can be coun-\nteracted by positioning important sources in high-attention\npositions. By requesting \u201coptimal permutations\u201d from RAGE,\nthe user can analyze a set of permutations with optimum\nplacement of relevant sources in high-attention positions.\nTo estimate the relevance of a source, the user can choose to\nuse either the LLM\u2019s attention scores or the retrieval model\u2019s\nassessed relevance score. If desired, the user can calibrate the\nexpected distribution of LLM context position attention by\nselecting a predefined V-shaped distribution. Optimal permuta-"
    },
    {
        "vector_id": 5450,
        "text": "tions aim to maximize both the relevance and attention of their\nconstituent sources. A naive O(k!) solution might generate all\nk! permutations, scoring each by summing the product of each\nsource\u2019s relevance and attention, then sorting and selecting\nthe s highest-scoring permutations. Recognizing that optimal\npermutations must maximize both the relevance and attention\nof their constituent sources, we propose an efficient solution\nby formulating this problem as an instance of the assignment\nproblem in combinatorics.\nNumerous algorithms have been proposed to solve this\nproblem, which aim to find the most optimal assignment of all\nk sources to all k context positions. Since RAGE allows the\nuser to request the top-s optimal permutations, our formulation\nadopts a variant of the assignment problem that seeks the s\nassignments with minimal cost. We use the algorithm proposed\nby Chegireddy and Hamacher [7], which allows us to calculate\nthe s optimal permutations in O(sk3)."
    },
    {
        "vector_id": 5451,
        "text": "the s optimal permutations in O(sk3).\nIII. D EMONSTRATION PLAN\nConference participants will explore the provenance of\ninformation included in retrieval-augmented LLM responses.\nThey will then reinforce these findings by evaluating the\nimportance of relative position among sources.\nFig. 2. Combination insights for the query about The Big Three.\nA. Categorization of Use Cases\nThe explanations generated by RAGE are applicable across\ncountless domains. Use cases can be categorized based on\nvarious factors, such as whether knowledge sources form a\ntimeline, or when questions are subjective, leading to am-\nbiguous answers. In the former case, RAGE identifies salient\nperiods in time. In the latter case, it procures evidence to\nsupport various answers. Knowledge sources may differ in\nterms of their consistency. Our tool can identify consistent and\ninconsistent sources. Sources may or may not share semantic\ndependencies, and may or may not share syntactic formats."
    },
    {
        "vector_id": 5452,
        "text": "RAGE will highlight source agreement and disagreement.\nIn the following subsections, we introduce several use cases\nthat highlight the axes of this categorization. We begin by\nexploring the possibility of an ambiguous answer, which requires efficient evaluation over a large sample of knowledge\nsource combinations. Next, we present a scenario in which\nsources are slightly inconsistent, testing RAGE\u2019s ability to\nidentify minor differences that can change the LLM\u2019s answer.\nLast, we provide an example in which the sources form a\ntimeline, requiring RAGE to strategically navigate alternate\ntimelines by minimally combining and permuting the sources.\nB. Use Case #1: Ambiguous Answers\nThe user asks an LLM to determine the best tennis player\namong \u201cThe Big Three\u201d of Novak Djokovic, Roger Federer,\nand Rafael Nadal. The user does not have any specific com-\nparison metric in mind, so they use the system to retrieve a set\nof related documents, each containing a different ranking of"
    },
    {
        "vector_id": 5453,
        "text": "The Big Three based on different metrics (e.g., total number\nof match wins and number of weeks ranked first). The user\nexpects that Novak Djokovic, who recently surpassed Rafael\nNadal and Roger Federer in total Grand Slam wins, might be\nthe LLM\u2019s choice. But when asked with the combination of all\nretrieved documents, the LLM\u2019s answer is \u201cRoger Federer.\u201d\nCurious about why the LLM chose Federer, the user poses\nthe same query and documents to RAGE, requesting combi-\nnation insights. As illustrated in Figure 2, RAGE analyzes the\nanswers generated by the LLM using various combinations\nof the given documents, and discovers that the first document\nled the LLM to produce this answer. This document ranks\nvarious tennis players based on total match wins, with Federer\nranking first at 369. RAGE\u2019s answer rules formalize this\nexplicitly, asserting that this document was included in every\ncombination for which the LLM answered \u201cRoger Federer.\u201d\nThe user now comprehends why the LLM chose Federer"
    },
    {
        "vector_id": 5454,
        "text": "but remains curious about the document\u2019s relative signifi-\ncance. Reviewing the original ranking, they notice that this\ndocument has prominent placement at the beginning of the\ncontext. To investigate the impact of this position, the user\nrequests permutation-based explanations for the same inputs.\nSurprisingly, RAGE reveals that moving the document to the\nsecond position altered the answer to \u201cNovak Djokovic.\u201d In\nshort, these explanations have enabled the user to promptly\nidentify the document that influenced the LLM\u2019s answer, and\nto understand the impact of its relative position.\nC. Use Case #2: Inconsistent Sources\nThe user turns to an LLM for help in determining the most\nrecent winner of the US Open women\u2019s tennis championship.\nA small set of documents is retrieved, each containing relevant\nstatistics about US Open championships. The documents share\nsimilar format, but some may be more current than others.\nHoping that the LLM will pinpoint the most recent winner"
    },
    {
        "vector_id": 5455,
        "text": "across all documents, the user requests combination insights\nin RAGE, and observes how the combination containing all\nsources produces the response \u201cCoco Gauff.\u201d With no further\nexplanation, the user aims to verify this result by identifying\nthe source document behind the answer, and discovers that the\nlast context document recognizes Gauff as the 2023 champion.\nCurious whether other out-of-date documents could have\nbeen mistakenly sourced for an incorrect answer, the user\nasks RAGE to derive permutation insights. By reordering the\ncontext documents in various configurations and analyzing the\nresulting answers, RAGE discovers that the LLM incorrectly\nidentifies the 2022 champion \u201cIga Swiatek\u201d whenever the last\ndocument is moved towards the middle of the sequence. Using\nRAGE, the user has identified the up-to-date document that\noffers the correct answer, and has gleaned insights about out-\nof-date documents and their ability to confuse the LLM.\nD. Use Case #3: Timelines"
    },
    {
        "vector_id": 5456,
        "text": "D. Use Case #3: Timelines\nThe user consults an LLM to determine how many times\nNovak Djokovic won the Tennis Player of the Year award\nbetween 2010 and 2019. The user gathers relevant documents\nfrom the system, each corresponding to one year\u2019s winner.\nCollectively, the documents form a timeline for the three win-\nners: Rafael Nadal (2010, 2013, 2017, 2019), Novak Djokovic\n(2011, 2012, 2014, 2015, 2018), and Andy Murray (2016).\nThe user poses their question to RAGE, which reports that\nthe LLM produces the expected answer of 5 when incorpo-\nrating the combination of all retrieved documents. To validate\nthe LLM\u2019s response, the user expects an explanation listing\neach year Djokovic won the award, along with a citation to\na supporting document. To achieve this, the user reviews the\ncombination counterfactual generated by RAGE to determine\nthe minimal set of documents (and thus the exact years)\nrequired to infer the correct answer. RAGE cites five separate"
    },
    {
        "vector_id": 5457,
        "text": "documents from those provided, each documenting a different\nyear in which Djokovic won Player of the Year.\nHoping to ensure that the LLM has not overlooked any\ntime period covered by the documents, the user asks RAGE to\nderive permutation insights over the same inputs. By analyzing\na sample of permutations, the user is presented with a pie\nchart and answer table that indicate a consistent answer of 5.\nThe user observed that no rules were found, and concludes\nthat the LLM consistently comprehends the entire timeline\nof the twenty-tens, regardless of the specific order of the\ntimeline\u2019s constituent documents. Through RAGE, the user has\nsuccessfully discovered which segments of the timeline were\ncrucial in determining the correct answer.\nREFERENCES\n[1] J. Wei, J. Wei, Y . Tay, D. Tran, A. Webson, Y . Lu, X. Chen, H. Liu,\nD. Huang, D. Zhou et al., \u201cLarger language models do in-context learning\ndifferently,\u201d arXiv preprint arXiv:2303.03846 , 2023."
    },
    {
        "vector_id": 5458,
        "text": "[2] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and\nP. Liang, \u201cLost in the middle: How language models use long contexts,\u201d\n2023, arXiv:2307.03172.\n[3] J. Rorseth, P. Godfrey, L. Golab, M. Kargar, D. Srivastava, and J. Szlichta,\n\u201cCredence: Counterfactual explanations for document ranking,\u201d in ICDE,\n2023, pp. 3631\u20133634.\n[4] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama 2: Open\nfoundation and fine-tuned chat models,\u201d arXiv:2307.09288, 2023.\n[5] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira,\n\u201cPyserini: A Python toolkit for reproducible information retrieval research\nwith sparse and dense representations,\u201d in SIGIR, 2021, pp. 2356\u20132362.\n[6] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural\naad medical research. Edinburgh: Oliver and Boyd, 1938.\n[7] C. R. Chegireddy and H. W. Hamacher, \u201cAlgorithms for finding k-best"
    },
    {
        "vector_id": 5459,
        "text": "perfect matchings,\u201d Discrete Applied Mathematics , vol. 18, no. 2, pp.\n155\u2013165, 1987."
    },
    {
        "vector_id": 5460,
        "text": "Usable XAI: 10 Strategies Towards Exploiting Explainability\nin the LLM Era\nXuansheng Wu1\u2217\nxuansheng.wu@uga.edu\nHaiyan Zhao2\u2217\nhz54@njit.edu\nYaochen Zhu3\u2217\nuqp4qh@virginia.edu\nYucheng Shi1\u2217\nyucheng.shi@uga.edu\nFan Yang4 yangfan@wfu.edu\nTianming Liu1 tliu@uga.edu\nXiaoming Zhai1 xiaoming.zhai@uga.edu\nWenlin Yao5 wenlinyao@global.tencent.com\nJundong Li3 jundong@virginia.edu\nMengnan Du2 mengnan.du@njit.edu\nNinghao Liu1 ninghao.liu@uga.edu\n1University of Georgia 2New Jersey Institute of Technology 3University of Virginia 4Wake Forest University\n5Tencent AI Lab (Seattle)\nAbstract\nExplainable AI (XAI) refers to techniques that provide human-understandable insights into\nthe workings of AI models. Recently, the focus of XAI is being extended towards Large Lan-\nguageModels(LLMs)whichareoftencriticizedfortheirlackoftransparency. Thisextension\ncalls for a significant transformation in XAI methodologies because of two reasons. First,"
    },
    {
        "vector_id": 5461,
        "text": "many existing XAI methods cannot be directly applied to LLMs due to their complexity and\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse industry\napplications, the role of XAI shifts from merely opening the \u201cblack box\u201d to actively enhanc-\ning the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike\ntraditional machine learning models that are passive recipients of XAI insights, the distinct\nabilitiesofLLMscanreciprocallyenhanceXAI.Therefore, inthispaper, weintroduceUsable\nXAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies,\nintroducing the key techniques for each and discussing their associated challenges. We also\nprovide case studies to demonstrate how to obtain and leverage explanations. The code\nused in this paper can be found at:https://github.com/JacksonWuxs/UsableXAI_LLM.\n*Equal contribution\n1"
    },
    {
        "vector_id": 5462,
        "text": "*Equal contribution\n1\narXiv:2403.08946v1  [cs.LG]  13 Mar 2024 Contents\n1 Introduction 4\n2 LLM Diagnosis via Attribution Methods 6\n2.1 Literature Review of Attribution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2 Case Studies: Usability of Attribution Methods for LLMs . . . . . . . . . . . . . . . . . . . . 8\n2.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3 LLM Diagnosis and Enhancement via Interpreting Model Components 11\n3.1 Understanding the Self-Attention Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Understanding the Feed-Forward Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 LLM Debugging with Sample-based Explanation 14\n4.1 Literature Review of Sample-based Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . 14"
    },
    {
        "vector_id": 5463,
        "text": "4.2 Case Study: EK-FAC-based Influence Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5 Explainability for Trustworthy LLMs and Human Alignment 18\n5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2 Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n5.4 Toxicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.5 Truthfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.6 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n6 LLM Enhancement via Explainable Prompting 22"
    },
    {
        "vector_id": 5464,
        "text": "6 LLM Enhancement via Explainable Prompting 22\n6.1 Chain of Thoughts (CoT) Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.2 Extended Methods of Explainable Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable? . . . . . . . . . . . . . . . . 23\n6.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n7 LLM Enhancement via Knowledge-Augmented Prompting 26\n7.1 Preliminaries: Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n7.2 Enhancing Decision-Making Control with Explicit Knowledge . . . . . . . . . . . . . . . . . . 27\n7.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n8 Training Data Augmentation with Explanation 28\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts . . . . . . . . . . . . . . . . 29"
    },
    {
        "vector_id": 5465,
        "text": "2 8.2 Explanation-enhanced Data Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n8.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n9 Generating User-Friendly Explanation for XAI 31\n9.1 User-friendly Data Explanation with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.2 Explaining Small Models with LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n9.3 Self-Explanation of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n9.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n10 LLMs for Interpretable AI System Design 33\n10.1 Designing Interpretable Network Architectures with LLMs . . . . . . . . . . . . . . . . . . . . 34\n10.2 Designing Interpretable AI Workflows with LLM Agents . . . . . . . . . . . . . . . . . . . . . 34"
    },
    {
        "vector_id": 5466,
        "text": "10.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n11 Emulating Humans with LLMs for XAI 35\n11.1 Emulating Human Annotators for Training Explainable Models . . . . . . . . . . . . . . . . . 35\n11.2 Emulating Human Feedback for Evaluating Explainable Models . . . . . . . . . . . . . . . . . 36\n11.3 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n12 Discussion and Conclusion 37\n3 1 Introduction\nExplainability holds great promise in understanding machine learning models and providing directions for\nimprovement. In practice, users have high expectations for model explainability:\n1. Through explanation, can we know if a model works properly?\n2. Does explainability help developing better models?\nFirst, explanations are expected to illuminate whether a model operates in accordance with human expecta-"
    },
    {
        "vector_id": 5467,
        "text": "tions. For example, does the model leverage reliable evidence and domain knowledge in its decision-making?\nDoes the model contain bias and discrimination? Does the model show any vulnerabilities to potential at-\ntacks? Will the model output harmful information? Second, in recognition of model imperfections, we aspire\nfor explainability to inform the development of better models. For example, how to adjust the behaviors of\na model if we find it is using unreliable or unreasonable features in making predictions? Can we improve the\nperformance of a model by aligning its behavior with human preferences?\nTherefore, the question arises: Have these expectations been met? In recent years, the body of\nliterature on Explainable AI (XAI) has expanded rapidly to improve model transparency (Du et al., 2019a;\nMurdoch et al., 2019; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018; Rudin et al., 2022), encompassing a"
    },
    {
        "vector_id": 5468,
        "text": "wide array of methods customized for different data modalities, including visual (Zhang & Zhu, 2018),\ntextual (Danilevsky et al., 2020), graph (Yuan et al., 2022), and time-series data (Zhao et al., 2023c). Some\nliterature delves into specific techniques, such as attention methods, generalized additive models, and causal\nmodels. Additionally, some offer reviews on general principles and categorizations or initiate discussions on\nevaluating the faithfulness of explanations (Yang et al., 2019).Despite the progress, the last mile of\nXAI \u2013 making use of explanations \u2013 has not received enough attention.In many cases, we seem\nto be satisfied with just acquiring explanations and their associated visualizations, sometimes followed by\nqualitative analysis of the model\u2019s strengths and weaknesses. While these explanations can reveal a model\u2019s\nimperfections, quantifying model properties (e.g., fairness, security, rationality) or taking the next concrete"
    },
    {
        "vector_id": 5469,
        "text": "steps toward model improvement remains a difficult task.\nThe challenges in achieving usable explainability are twofold. First, there is an inherent conflict between\nAI automation and human engagement in XAI. On one hand, humans need to define explainability that\nthe model should follow, or scrutinize explanation to identify if any vulnerabilities exist in the model. On\nthe other hand, the requirement for human oversight introduces substantial costs, posing challenges to the\nscalability and practical implementation of model debugging and improvement in AI workflows. Second,\nmany of the current approaches view explainability as a purely technical matter, ignoring the needs of\npractitioners and non-technical stakeholders. Existing XAI methods are mainly developed as statistical and\nmathematical tools. However, there exists a noticeable disparity between the objectives of these tools and the"
    },
    {
        "vector_id": 5470,
        "text": "expectations of practitioners across various application domains (Malizia & Patern\u00f2, 2023). An explanation\nthat satisfies a technical audience might offer little value to a non-technical audience.\nWhile the opacity issues have not yet been fully resolved for traditional deep models (e.g., multi-layer per-\nceptrons, convolutional and recurrent neural network), the recent advancements of Large Language Models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023b; Chiang et al., 2023) appear to have\nexacerbated the challenge we are facing. Firstly, LLMs typically possess a significantly larger model size\nand a greater number of parameters. This increased model complexity intensifies the difficulty of explaining\ntheir inner workings. Second, different from traditional ML models that primarily focus on low-level pattern\nrecognition tasks such as classification and parsing, LLMs can handle more complex tasks such as generation,"
    },
    {
        "vector_id": 5471,
        "text": "reasoning and question answering. Understanding the exclusive abilities of LLMs presents novel challenges\nfor XAI techniques. Considering the transformative impact of LLMs across various applications, ensuring\nthe explainability and ethical use of LLMs has become an imminent and pressing need. Meanwhile, the\nemergent capabilities of LLMs also present new opportunities for XAI research. Their human-like commu-\nnication and commonsense reasoning skills offer prospects for achieving explainability in ways that could\npotentially augment or replace human involvement.\nDefining \u201cUsable XAI\u201d.In light of the above considerations, in the context of LLMs, we define Usable XAI\nwhich includes two aspects as follows.(1) Utilizing Explainability to Enhance LLM and AI Systems.Beyond\n4 Attribution Methods\n(Sec 2)\nLLM Components \nInterpretation \n(Sec 3)\nSample-based \nExplanation \n(Sec 4)\nLLM Diagnosis\n\u2022 Accuracy\n\u2022 Factuality\n\u2022 \u2026 \u2026\nModel Probing\nModel Adjustment\nLLM Debugging\n\u2022 Influence function"
    },
    {
        "vector_id": 5472,
        "text": "LLM Debugging\n\u2022 Influence function\n\u2022 Embedding similarity\nExplainability for Trustworthy LLMs & \nHuman Alignment \n(Sec 5)\nSecurity Privacy Fairness\nToxicity\nLLM Enhancement \nvia Explainable \nPrompting\n(Sec 6)\nLLM Enhancement \nvia Knowledge-\nEnhanced Prompts \n(Sec 7)\nTraining Data \nAugmentation \n(Sec 8)\nEnhance Reasoning\nControllable Generation\nReduce Hallucination\nKnowledge Updating\nDomain Adaptation\nShortcut Mitigation\nData Enrichment\nLLM\nTraining\nData\nInference\nData\nPrediction\nXAI\nHuman\nUsers\nInterpretation\nUser-Friendly \nExplanation Generation\n(Sec 9)\nInterpretable AI System \nDesign with Explanation\n(Sec 10)\nEmulating Humans \nfor XAI \n(Sec 11)\nData Explanation\nSmall Model Explanation\nLLM Explanation\nInterpretable Architecture\nInterpretable AI Workflow\nHuman Annotation\nHuman Feedback\nHonesty Hallucination\nFigure 1: The contributions and outline of this paper. We define Usable XAI in the context of LLMs with"
    },
    {
        "vector_id": 5473,
        "text": "seven strategies of enhancing LLMs with XAI, andthree strategies of enhancing XAI with LLMs.\njust producing explanations or enhancing the transparency of LLMs, we explore whether these explanations\ncan pinpoint issues for model debugging or improve the overall performance of LLMs or AI models at large,\nsuch as accuracy, controllability, fairness, and truthfulness.(2) Utilizing LLMs to Enhance XAI Frameworks.\nThe human-like communication ability of LLMs can enhance model explanations in terms of user-friendliness,\nby converting the numerical values into understandable language. Also, the commonsense knowledge stored\nin LLMs can significantly boost the practicality of existing XAI frameworks, by playing the role of humans\nand alleviating the need for real human involvement in AI workflows.\nContribution of this paper.In this paper, we investigate 10 strategies towards usable XAI techniques in"
    },
    {
        "vector_id": 5474,
        "text": "the context of LLMs. These strategies are organized into two major categories: (1) Usable XAI for LLMs;\n(2) LLM for Usable XAI, as shown in Figure 1. Additionally, we conduct case studies to substantiate the\ndiscussion on selected techniques. For each strategy, we also explore the open challenges and areas that\nrequire further investigation in future work.\n\u2022 Usable XAI for LLMs. We introduce how interpretation can be utilized to enhance AI pipelines,\nincluding LLMs and small models. First, we investigate how explanations could be utilized to diagnose\nand enhance LLMs in terms of utility. We study three types of post-hoc explanation methods, target-\ning LLM predictions (Section 2), LLM components (Section 3), and training samples (Section 4),\nrespectively. Second, we focus on how explanations could be leveraged to scrutinize and boost model\ntrustworthiness (Section 5), including security, fairness, toxicity, and truthfulness, which is crucial to"
    },
    {
        "vector_id": 5475,
        "text": "achieving human alignment. Third, we discuss how explainability could guide the augmentation of data,\nincluding both inference data (i.e., prompts) and training data. Specifically, we discuss two strategies of\ncrafting explainable prompts for LLMs: Chain-of-Thought prompts (Section 6) and knowledge-enhanced\nprompts (Section 7). Furthermore, we introduce leveraging LLM explanations to augment training data\nfor improving small models (Section 8).\n\u2022 LLM for Usable XAI.In this part, we investigate strategies for leveraging the advanced capabilities\nof LLMs to address the challenges in traditional XAI domains, thus enhancing the usability of XAI in\npractice. First, we examine ways to enhance the user-friendliness of explanations through the generative\ncapabilities of LLMs (Section 9). Second, we introduce how to automate the design of interpretable\nAI workflows by leveraging the planning abilities of LLMs (Section 10). Third, we introduce how to"
    },
    {
        "vector_id": 5476,
        "text": "facilitate the evaluation of XAI methods by utilizing the unique property of LLMs in emulating human\ncognition processes (Section 11).\n5 Differences between this paper and existing surveys.Many surveys have been conducted to exam-\nine Explainable AI (Du et al., 2019a; Tjoa & Guan, 2020; Do\u0161ilovi\u0107 et al., 2018) or Interpretable Machine\nLearning (Murdoch et al., 2019). This paper differs from existing work as we focus on explanation methods\nfor large language models. Meanwhile, different from the existing survey (Zhao et al., 2023b) that mainly\nreviews explanation methods for LLMs, our paper puts an emphasis on the XAI usability in LLM studies.\nTo the best of our knowledge, the most related paper to our survey is (Luo & Specia, 2024), which also\ndiscusses several aspects where explanations can improve LLM performance. Nevertheless, this light-weight\ninvestigation lacks a thorough examination of XAI methods (e.g., sample-based explanation, interpretable"
    },
    {
        "vector_id": 5477,
        "text": "workflows, explainable prompts) and how LLMs can benefit existing XAI frameworks (e.g., data augmen-\ntation, improving user-friendliness, XAI evaluation). Finally, our paper contributes further by providing\ndetailed case studies and open-sourced codes, fostering future research in applying explanations effectively\nwithin the LLM context.\n2 LLM Diagnosis via Attribution Methods\nThis section introduces attribution methods as post-hoc explanations for LLMs, and how we can discover\nmodel defects with attribution scores. We start with revisiting existing attribution methods, and then\ndiscuss which methods are still suitable for explaining LLMs. Since LLMs widely serve both classification\nand generation tasks, our discussion categorizes the attribution methods accordingly. After that, we explore\ncase studies of applying attribution methods to assess LLM-generated output quality. Finally, we discuss\nfuture work of designing novel post-hoc explanation methods for LLMs."
    },
    {
        "vector_id": 5478,
        "text": "2.1 Literature Review of Attribution Methods\nThe attribution-based explanation quantifies the importance of each input feature that contributes to making\npredictions. Given a language modelf with a prediction\u02c6y = f(x) according to theN-words input prompt\nx, the explainer g assesses the influence of input words inx as a = g(x,\u02c6y,f) \u2208RN. Typically, the sign\nof an \u2208a indicates word xn positively or negatively influences\u02c6y, and a greater value of|an|indicates a\nstronger impact. In text classification, \u02c6y denotes a specific class label. In text generation, \u02c6y represents a\nvarying length of generated text.\nMany existing attribution-based explanation methods focus on classification tasks and cannot be directly\napplied to the generation task. The primary distinction between them is that: classification is limited to a\nspecific set of predictions, while generation encompasses an endless array of possibilities. For instance, in"
    },
    {
        "vector_id": 5479,
        "text": "sentiment analysis, a language model can be instructed to output a number between 0 and 1 that indicates\nthe positivity of input text by adding a linear layer and a sigmoid function on top of the language model.\nHowever, in the generative setting, the model can express this positivity in numerous expressions, such as\n\u201cthe reviewer definitely loves this movie\u201d and \u201cit is a strong positive movie review\u201d. This distinction poses a\nunique challenge in adapting explanation methods from classification to generation tasks. In the following,\nwe review related works based on the scenarios they are applicable to.\n2.1.1 Attributing Inputs for Label Classification\nCommon attribution methods (Du et al., 2019a; Murdoch et al., 2019) developed for traditional deep models\ninclude gradient-based methods, perturbation-based methods, surrogate methods, and decomposition meth-\nods. We introduce the general idea and representative examples for each category, followed by the analysis"
    },
    {
        "vector_id": 5480,
        "text": "of their suitability for explaining large language models.\nPerturbation-based Explanation. Perturbation-based methods assess the importance of input features\nby perturbing them and monitoring changes in prediction confidence, i.e.,an = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn), where\u02dcxn\nrefers to the input sequence with then-th feature being perturbed. Each feature could refer to a word (Li\net al., 2016a), a phrase (Wu et al., 2020b), or a word embedding (Li et al., 2016b). The underlying principle\nis that perturbing a more important feature should result in a more pronounced alteration in the model\u2019s\nprediction confidence. However, this method has limitations, particularly in its assumption that features are\n6 Table 1: Time complexity analysis on different attribution methods for the generative task.\nMethod Forward Backward Notes\nMask Perturbation O(N) 0 -\nGradient\u00d7Input O(1) O(M) -\nIntegrated Gradients O(Nstep) O(Nstep \u00b7M) Nstep is the number of steps for integrating gradients."
    },
    {
        "vector_id": 5481,
        "text": "LIME O(Naug) 0 Naug is the number of augmented samples.\nSHAP O(2N ) 0 -\nindependent, which is not always the case with textual data due to word inter-dependencies. Additionally,\nit is computationally intensive for explaining LLMs, requiringN inferences for an input ofN words.\nGradient-based Explanation. Gradient-based methods offer a computationally efficient approach for\nestimating model sensitivity to input features based on gradients\u2202p(\u02c6y|x)\n\u2202xn\n, wherexn refers to the embedding\nof wordxn. Some methods employ theL2-norm of gradients to assess word importance (Li et al., 2016a),\ni.e., an = \u2225\u2202p(\u02c6y|x)\n\u2202xn\n\u22252. This approach only requires a single inference and one backpropagation pass. Some\nextended methods multiply the gradient with the word embedding (Kindermans et al.; Ebrahimi et al., 2018;\nMohebbi et al., 2021), i.e.,an = \u2202p(\u02c6y|x)\n\u2202xn\n\u00b7xn. These methods may yield explanations with limited faithfulness"
    },
    {
        "vector_id": 5482,
        "text": "for deep models (Shrikumar et al., 2017), as gradients only reflect the local relationship between input\nvariation and output variation. To address this, Integrated Gradients (IG) has been proposed (Sundararajan\net al., 2017; Sikdar et al., 2021; Sanyal & Ren, 2021; Enguehard, 2023), which accumulates gradients as\ninput transitions from a reference point to the actual data point. Nevertheless, IG entails multiple rounds\nof inference and backpropagation, thus significantly increasing computational demands.\nSurrogate-basedExplanation. Surrogate-basedexplanationmethodsunderstandcomplexmodelsbycon-\nstructing a simpler modelg trained onD(x,\u02c6y) ={(\u02dcxk,\u02dcyk)}K\nk=1, whereD(x,\u02c6y) denotes a dataset constructed\nfor the target instance(x,\u02c6y); \u02dcxk is usually obtained by perturbingx, and\u02dcyk = f(\u02dcxk). The surrogate model\ng, ranging from basic linear models to sophisticated decision trees, serves as a proxy to approximate the de-"
    },
    {
        "vector_id": 5483,
        "text": "cision boundary of the target modelf for a specific instance(x,\u02c6y). Notable examples include LIME (Ribeiro\net al., 2016), SHAP (Lundberg & Lee, 2017), and TransSHAP (Kokalj et al., 2021), where the first two\nare designed for general deep neural networks and the last one is tailored for Transformer-based language\nmodels. Nevertheless, a significant limitation of them is their intensive reliance on repeated interactions with\nthe target model, a process that is impractical for LLMs.\nDecomposition-based Explanation. Decomposition-based methods assign linearly additive relevance\nscores to inputs, effectively breaking down the model\u2019s prediction. Layer-wise Relevance Propagation (Mon-\ntavon et al., 2019) and Taylor-type Decomposition (Montavon et al., 2017) are well-known techniques for\ncomputing these relevance scores. These methods have been adapted for Transformer-based language models"
    },
    {
        "vector_id": 5484,
        "text": "in various research (Voita et al., 2019; 2020; Wu & Ong, 2021). However, a primary challenge in implementing\ndecomposition-based explanations is the need for tailored decomposition strategies to accommodate different\nmodel architectures. Although many large language models are based on the Transformer framework, there\nare key variations between them, such as LLaMA (Touvron et al., 2023a) and GPT (OpenAI, 2023), partic-\nularly in aspects like positional encoding strategy and feed-forward network design. This challenge poses a\nlimitation on the universal applicability of decomposition methods for general-purpose interpretation.\nTo summarize, the traditional explanation methods arenot always suitable for LLMs. In particular, the\nperturbation-based and gradient-based explanations are relatively easy to extend for attributing LLM re-\nsponses to the input prompts, while the surrogate-based and decomposition-based methods become signifi-"
    },
    {
        "vector_id": 5485,
        "text": "cantly challenging to do so. Specifically, surrogate-based methods suppose that an explainable small model\ncould approximate the decision boundary of the target model around a local example, but there are limited\nexplainable models for the text generation task. Meanwhile, decomposition-based methods require designing\ndecomposition strategies for different layers, which is challenging for big LLM architectures. Another pri-\nmary concern is their significant demand for computing resources. Given anN-words input prompt and an\nM-words output response, we present the time complexity of several representative explanation methods in\nTable 1. It demonstrates that existing methods either require a large number of forward operations or back-\n7 ward operations. Therefore, improving the efficiency of the attribution-based explanation is an important\ndirection for future research and development.\n2.1.2 Attributing Inputs for Text Generation\nTherewas no\npresident\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe"
    },
    {
        "vector_id": 5486,
        "text": "president\nin\nFrance\nin\n 1 2 5 0 .\nWho\nwas\nthe\npresident\nin\nFrance\nin\n1\n2\n5\n0\n?\nFigure 2: An example of attribu-\ntion saliency map between input\nand output tokens.\nThe explanation of generative models can be defined as attributing the\noverall confidencep(\u02c6y|x) to the inputx, where\u02c6ydenotes the generated\nresponse \u02c6y= [\u02c6y1,..., \u02c6yM] with M words. One method to achieve this is\nby treating the text generation process as a sequence of word-level clas-\nsification tasks. This perspective allows for the application of existing\nclassification-based explanation techniques to assess the influence of\neach input wordxn in relation to each output word\u02c6ym, resulting in a\ncorresponding attribution scorean,m. After gathering the attributions\nan,m for m = 1,...,M , we perform an aggregation to determine the\noverall contribution of each input wordxn. This is accomplished by ag-\ngregating the individual attributions for all output words correspond-"
    },
    {
        "vector_id": 5487,
        "text": "ingtotheinputword, denotedas an = Aggregate([an,1,...,a n,M]). The\nsimplest approach for this aggregation is to average the attributions as-\nsigned to each input word across the different output words (Selvaraju\net al., 2016). However, Wu et al. (2023) observe that attribution scores\nfrom different output words are not inherently comparable. For exam-\nple, the attribution scores for function words (e.g., \u201cthe\u201d, \u201cis\u201d, \u201chave\u201d) are often disproportionately larger\nthan the scores for content words with clear semantic meaning (e.g., verbs and nouns). Therefore, it is nec-\nessary to normalize the scores prior to the aggregation, so that the scores[an,1,...,a n,M] become comparable\nfor 1 \u2264m \u2264M. Figure 2 plots the normalized scores of an example case, where each index in the Y-axis\nrefers to an input prompt token, while that in the X-axis is an output response token. A greater normalized"
    },
    {
        "vector_id": 5488,
        "text": "attribution score is brighter. In this example, the user attempts to direct the model to output information\nthat does not exist, namely the French president in 1250. The model successfully realizes that this thing\ndoes not exist and refuses to answer. The model response can be realized as three parts, \u201cThere was no\u201d,\n\u201cpresident in France\u201d, and \u201cin 1250\u201d. According to the figure, the first span is generated heavily because of\nthe tokens \u201cWho\u201d and \u201cpresident\u201d, while the model uses both \u201cFrance\u201d and \u201c1250\u201d to respond to the second\nspan \u201cpresident in France\u201d. Finally, the model emphasizes the date \u201c1250\u201d again by referencing the same\ninformation from the prompt. Overall, these explanations align with human understanding and highlight the\nusage of this method in the future. However, current research on attribution-based explaining for generative\nLLMs is still in its early stages, and only a limited number of methods have been proposed."
    },
    {
        "vector_id": 5489,
        "text": "2.2 Case Studies: Usability of Attribution Methods for LLMs\nLanguage Model\nPrompt\nResponse\nAttribution Map\nFeature Vector\nBehavior Detector\nHallucination\nJailbroken\nMisalignment\n\u2026\nInput Prompt\nOutput Response\nFigure 3: A general pipeline of model diag-\nnosis with attribution explanations.\nThe attribution map offers a partial insight into the opera-\ntional mechanics of LLMs (Chandrasekaran et al., 2018; Hase\n& Bansal, 2020; Ye & Durrett, 2022a). Accordingly, we propose\na general pipeline that leverages attribution scores to analyze\nLLM behaviors, as shown in Figure 3. First, given the target\nLLM and an input prompt, we compute attribution scores of\ninput tokens relative to the output tokens. Second, we extract\na feature vector from the attribution map, tailored to the re-\nquirements of the diagnostic task at hand. Third, we train a\nlight-weight predictor (e.g., a classifier) to diagnose whether\nthe model behaves appropriately based on the feature vector."
    },
    {
        "vector_id": 5490,
        "text": "In the following, we provide case studies to illustrate how at-\ntribution scores could be utilized to assess LLM response qual-\nity (Adlakha et al., 2023).\n8 2.2.1 LLM Response Quality Evaluation with Explanations\nThis case study explores the use of attribution-based explanations as evidence for assessing the quality of\nLLM-generated responses. Here, \u201cquality\u201d is specifically measured by the responses\u2019 accuracy. We hypoth-\nesize that responses generated from correct rationales are likely to be more accurate. Our method involves\ncomparing the model\u2019s explanations with underlying rationales as a means to evaluate response accuracy.\nThis approach could enhance the reliability of conversational systems, which is crucial for applications such\nas medical question answering.\nDataset. We employ the Multi-Sentence Reading Comprehension (MultiRC) dataset (Khashabi et al.,\n2018) in this case study. MultiRC presents a more significant challenge than other datasets: it requires"
    },
    {
        "vector_id": 5491,
        "text": "the system to answer questions based on multiple sentences from a given paragraph. The answers may not\ndirectly correspond to specific sentence spans, reflecting a more realistic scenario of modern LLMs. Each\nMultiRC instance is associated with a human-annotated rationale indicating which sentences are relevant\nfor answering the question. Our study specifically employs its test subset with 950 samples in total. We\nconsider 80% in which are the training set and we report the results evaluated on the rest samples.\nImplementation. We apply the Importance Density Score method (Wu et al., 2023) to estimate the\nimportance of each word in the provided paragraph. Specifically, givenN-word paragraphX and the M-\nword response \u02c6Y, the attribution from each input wordxn to each output word\u02c6ym is defined asan,m =\n\u2202f(ym|Zm)\n\u2202Ei[xn] \u00b7Ei[xn]\u22a4, where Zm indicates the formatted input prompt concatenated with the firstm\u22121"
    },
    {
        "vector_id": 5492,
        "text": "response word,f is the language model, andEi[xn] indicates the input static embedding ofxn. This pairwise\nattribution score is then normalized as\u02dcan,m = \u2308L\u00d7an,m/maxn\u2032(an\u2032,m)\u2309. Any normalized attribution\u02dcan,m is\nequal or less thanbwill is forced to 0. The overall attribution score on wordxn is defined as its attribution\ndensity, i.e.,an = ||[an,1,...,a n,M||1/||[an,1,...,a n,M]||p. In our experiments, we letL = 10, b = 2, p = 5,\nand consider Vicuna-7B-v1.1 as our language modelf. By averaging these word attribution scores, we reach\nthe importance of each sentence. The top-K sentences with the highest importance scores are selected as\nthe explanation for each instance. The explanation is then concatenated with the output response and fed\nto a classifier to predict whether the output is correct or not. We initialize the classifier with DistillBERT-\nbase(Sanhetal.,2019), andtrainitover3epochswithlearningrate 5e\u22125 andweightdecay 1e\u22123. Tocompare"
    },
    {
        "vector_id": 5493,
        "text": "with this, we also consider the Full Paragraph or the human-annotated rationales as the explanation part to\ntrain the classifier. Following previous studies (Khashabi et al., 2018; DeYoung et al., 2019), we evaluate the\naccuracy of the generated responses by exactly matching all correct answers with the generated responses.\nTable 2 reports the macro precision, recall, F1, and AUC scores on the test set over five random seeds.\nTable 2: Utilizing attribution for response quality evaluation\nSetting Precision Recall F1 AUC\nRandom 49.40 51.79 49.61 49.03\nHuman Rationale 68.73 66.88 67.57 73.11\nFull Paragraph 58.02 58.47 56.89 63.44\nAttribution (ours) 63.25 67.69 64.12 71.53\nResults. In Table 2, we observe that iso-\nlating the rationales from the full context\ncould best help the classifier identify the\nresponse quality. It is evident that ex-\nplanations align more closely with human-\nannotated rationales when associated with\ncorrect responses. In particular, the pre-"
    },
    {
        "vector_id": 5494,
        "text": "correct responses. In particular, the pre-\ncision and recall metrics for explanations\nderived from correct answers surpass those\nassociated with incorrect responses. This finding serves as empirical evidence underscoring the effectiveness\nof attribution-based explanations in estimating the accuracy of responses generated by LLMs.\n2.2.2 Hallucination Detection with Attribution Explanations\nThis case study explores a different aspect of LLM generation quality, focusing on the presence ofhallucina-\ntions in LLM-generated responses. We show that attribution-based explanations can serve as indicators to\ndetect LLM hallucinations. Hallucinations are defined as responses that contain information conflicting with\nor unverifiable by factual knowledge (Li et al., 2023c; Ji et al., 2023). For instance, if a model is asked about\na fictitious entity like \u201cKing Renoit\u201d and responds with a narrative about \u201cThe Three Musketeers\u201d, claiming"
    },
    {
        "vector_id": 5495,
        "text": "9 Table 3: Utilizing attribution for hallucination detection on ChatGPT responses.\nMethod Language Model Precision Recall F1 Accuracy\nRandom - 88.41 50.34 64.11 50.59\nFacTool GPT-4 95.30 72.93 82.62 73.04\nVectara DeBERTa-base 90.29 60.54 72.40 59.45\nAttrScore (ours) Vicuna-7B 90.15 74.21 81.36 70.20\nMistral-7B 88.74 75.04 81.26 69.57\nit pertains to the nonexistent king, it illustrates a hallucination. This tendency, particularly pronounced in\ninstruction-tuned models, stems from their earnest endeavor to fulfill user requests. The issue often arises\nwhen direct commands (\u201ctell me a story\u201d) significantly influence the generation process, while the instruc-\ntion\u2019s subject (\u201cabout King Renoit\u201d) is neglected. Based on this insight, we develop a hallucination detector\naccording to the distribution of attribution scores over different types of prompting words.\nDataset. We use the Hallucination Evaluation Benchmark (Li et al., 2024a) in this case study. Each"
    },
    {
        "vector_id": 5496,
        "text": "instance from this dataset includes an input prompt, a response generated by ChatGPT (Ouyang et al.,\n2022), and the knowledge related to the response. Each piece of knowledge has a human annotation about\nwhether the knowledge is valid or not. Each instance having at least one invalid knowledge is considered\na hallucination response. This study focuses on 632 less confused examples from the benchmark, each of\nwhich has almost all knowledge correct or almost all wrong. We randomly select 80% of the samples to form\na training set and the rest form the testing set.\nImplementation. Given a query prompt and its ChatGPT response, we aim to build a classifier to detect\nif the response contains hallucination. Since the gradients of ChatGPT is inaccessible, we apply the Vicuna-\n7B model as the substitute to compute the attribution scores. Specifically, we adopt Importance Density\nScores (Wu et al., 2023) to compute the attribution scores of Vicuna. We then use the NLTK package to"
    },
    {
        "vector_id": 5497,
        "text": "identify the part-of-speech (POS) tag of each query word. Finally, each query-response pair is represented\nwith an 82-dimensional vector, where each dimension indicates the average attribution scores over a certain\ntype of POS tagging. We develop a Support Vector Machine classifier based on the POS tagging attribution\nscores on the training set and report the precision, recall, F1 scores on the hallucination samples, as well\nas accuracy over all samples in Table 3. To compare with this, we also leverage a fine-tuned model and a\nprompting-based method to serve as the hallucination detector baselines.\nResults. In the table, we first observe that all methods have demonstrated a greater performance than the\nRandom strategy, indicating that the proposed methods and the baselines are valid hallucination detectors.\nIn addition, we observe that the proposed methods achieve a competitive performance with FacTool using"
    },
    {
        "vector_id": 5498,
        "text": "GPT-4, establishing a new benchmark in this domain. It is worth noting that, instead of using GPT-4,\nthe attribution score for our method is calculated using a smaller 7-billion-parameter language model. This\ndemonstrates the practicality and efficiency of our approach inweak-to-strong generalizationas we could\ndiagnose large language models with smaller models. Future work could consider extracting more effective\nfeatures and using more powerful classifiers.\n2.3 Challenges\n2.3.1 How to Identify and Explain the Semantics of Output?\nThe attribution functionan = p(\u02c6y|x) \u2212p(\u02c6y|\u02dcxn) is no longer faithfully attributing the model predictions in\nthe human-interested semantic level since the model could express the same semantic meaning with various\nresponses. Specifically, the model could assign a lower confidence to its original response than the new\none, while both responses share the same semantic meaning. This is a significant difference compared with"
    },
    {
        "vector_id": 5499,
        "text": "the traditional classification problem, where target label sets are manually designed so that a lowerp(\u02c6y|\u00b7)\nindicates the model is less confident in predicting a specific semantic concept. Taking the sentiment analysis\n10 Multi-Head\nSelf-Attention\nFeed Forward\nNetwork\nLinear\nSoftmax\nEmbedding\nN \u00d7 Transformers\nResponse\nProbabilities\nPrompt\nSelf-Attention Map \nExplanation\nMechanistic \nInterpretability\nExplanationExplanation\nKnowledge Tags\nText: \u2026 -> PosText: ... -> Neg Text: \u2026 ->\nReweight attentions scores.\nImproved Model Design\nUsability\nModel Pruning\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\n\u210e\ud835\udc57\ud835\udc57,1 \u210e\ud835\udc57\ud835\udc57,2 \u2026\n\u00b7\u00b7\u00b7\n \u00b7\u00b7\u00b7\nFeed Forward \nNetwork  \n\ud835\udc65\ud835\udc65\ud835\udc56\ud835\udc56\n \nData Samples\n \u2026\nConcepts\nModel Editing\nIreland\nDublin capital\nLocate and Edit\nModel Pruning\n1,2,3,4,5\nControllable Generation\n,6,7,8\nOriginal \nModel\nControlled:\n   + \u201cDNA\u201d.\nAGACCAGAUsability\nFigure 4: Review of interpretation methods for LLM components and their applications. We categorize"
    },
    {
        "vector_id": 5500,
        "text": "methods according to the target LLM modules: self-attention layers and feed-forward layers.\ntask as an example, an LLM may generate two different responses sharing the same predicted concepts,\nsuch as \u201cit is a positive review\u201d and \u201cthe audience thinks the movie is great\u201d. Current attribution-based\nexplanations concentrate on the literal changes in generated responses, but they do not study how the\nsemantic meanings of these responses change. Therefore, they do not provide sufficient explanations of\nmodel-generated responses at a semantic level. In this case, the semantic level is which words of the input\nreview lead the model to believe it is positive. Future work may tackle this challenge by proposing metrics\nto evaluate the semantic differences in responses.\n2.3.2 Explaining LLM Predictions Beyond Attribution\nThe versatility of LLM generation will inspire a variety of explanation paradigms beyond traditional attri-"
    },
    {
        "vector_id": 5501,
        "text": "bution methods. Attribution methods aim to explain model output with the contribution of input features.\nThis explanation task is meaningful for conventional machine learning (ML) models whose outputs are usu-\nally individual decisions with clear formats (e.g., classification, regression, object detection). The decisions\nare highly dependent on the input features. However, LLMs differ from traditional ML models in two as-\npects. First, the generative process of LLMs is stochastic, e.g., we may obtain different outputs by running\nan LLM twice even with the same input. Second, LLMs encode rich knowledge within their parameters,\nwhich are independent of inputs. These unique properties give rise to novel explanation paradigms. For the\nfirst aspect, an explanation task of interest would be to understand theuncertainty of LLM generation. For\nexample, researchers (Ahdritz et al., 2024; Varshney et al., 2023; Su et al., 2024) leverage the prediction per-"
    },
    {
        "vector_id": 5502,
        "text": "plexity to check whether the LLM is confident during generation, identifying potential errors in less confident\npredictions. Second, attributing LLM predictions to theirencoded knowledgeinstead of input patterns could\nprovide a new perspective. Some researchers (Yin et al., 2024a) propose the knowledge-boundary detection\ntask to detect whether a model has specific knowledge for a given question. Nevertheless, it did not attribute\nthe prediction to specific knowledge, so humans cannot verify the prediction process with their results yet.\n3 LLM Diagnosis and Enhancement via Interpreting Model Components\nThis section discusses the XAI methods that interpret the internal components of large language models.\nAdditionally, it delves into the insights these methodologies offer, which can be instrumental in refining and\nenhancing the design of language models. LLMs adopt transformers as the basic architecture, which typically"
    },
    {
        "vector_id": 5503,
        "text": "11 comprises two types of major components: self-attention layers and feed-forward layers. In the following, we\nreview the research that focuses on interpreting each of these components respectively.\n3.1 Understanding the Self-Attention Module\nA multi-head self-attention module contains multiple self-attention heads capturing different kinds of word-\nword relations, which are modeled with weightsWq,Wk \u2208RD1\u00d7D2 . Specifically, the relation of wordswi\nand wj is computed asAi,j \u221d(xiWq) \u00b7(xjWk)\u22a4, wherexi,xj \u2208R1\u00d7D1 are contextual embeddings of the\nwords. The most straightforward interpretation is analyzing the attention score matrixA given an input\nsequence to study the relations between words (Vig, 2019; Hoover et al., 2020). In practice, these intuitive\nexplanations would be majorly used to present case studies via visualization. With this strategy, Wang\net al. (2023b) conduct case studies on in-context sentiment analysis, where they find that the label words"
    },
    {
        "vector_id": 5504,
        "text": "from the in-context examples serve as anchors of the final prediction. In specific, these anchors aggregate\ninformation from the examples to generate informative representations from the lower layers, while the deeper\nlayers utilize these representations to make final predictions. This insight motivates them to reweight the\nattention scores of these anchors to achieve better inference accuracy. Some researchers (Dar et al., 2023;\nWu et al., 2023) extend this framework to globally analyze the attention weightsWq and Wk by feeding the\nstatic word embeddings of words from an interested vocabulary, instead of their contextual embeddings. For\nexample, with this approach, Wu et al. (2023) find that instruction tuning empowers LLMs to follow human\nintentions by encouraging them to encode more word-word relations related to instruction words. On the\nother hand, some mathematical models are proposed to theoretically explain the self-attention mechanism,"
    },
    {
        "vector_id": 5505,
        "text": "such as Sparse Distributed Memory(Bricken & Pehlevan, 2021) andTransformer Circuits(Elhage et al.,\n2021). In particular, the Transformer Circuits provide mechanistic interpretability to transformer-based\nmodels, which breaks models down into human-understandable pieces. Although these theoretical analyses\non self-attention solids a foundation for future research, their direct application is largely underexplored.\n3.2 Understanding the Feed-Forward Module\nA feed-forward network is formalized asx\u2032= \u03c3(xWu)W\u22a4\nv , wherex \u2208R1\u00d7D1 is the intermediate contextual\nrepresentation of an input word,\u03c3 is a non-linear operation, andWu,Wv \u2208RD1\u00d7D3 are model parameters.\nFeed-forward networks can be understood askey-value memories(Sukhbaatar et al., 2015; Geva et al., 2021),\nwhere each key or value is defined asWu[d] \u2208RD1 andWv[d] \u2208RD1 , respectively. That is, each feed-forward\nnetwork obtainsD3 key-value pairs, called memories. One simple way to interpret the semantic meaning of"
    },
    {
        "vector_id": 5506,
        "text": "memory is collecting the words that could maximally activate the key or value vector of that piece of the\nmemory (Geva et al., 2021; Dar et al., 2023), which has demonstrated strong interpretability of the extracted\nword lists. However, it is critical to be aware that the key or value vectors are polysemantic (Arora et al.,\n2018; Scherlis et al., 2022; Bricken et al., 2023), indicating that this simple approach might not provide\nconcise explanations for each key-value pair. It has been shown that the word list of each key-value pair has\n3.6 human interpretable patterns on average (Geva et al., 2021). To alleviate the limited interpretability\ncaused by the nature of polysemantic, Wu et al. (2023) propose to interpret the principal components of these\nkey or value vectors, leading to a more concise explanation for each word list, such as \u201cmedical abbreviations\u201d\nand \u201cprogramming tasks and actions\u201d. Other work examines individual memories by measuring the changes"
    },
    {
        "vector_id": 5507,
        "text": "of predictions after perturbing their corresponding activations, where it reveals that some memories encode\nspecific knowledge (Dai et al., 2022) and some others capture general concepts (Wang et al., 2022b). By\nleveraging the explanations of key-value memories, we could locate and update the memories associated with\na specific piece of knowledge to performmodel editing(Dai et al., 2022; Meng et al., 2022a;b; Hase et al.,\n2024), i.e., modifying outdated or incorrect knowledge. Another usage of these weight explanations ismodel\npruning, where LLMs can be condensed up to 66.6% of their initial parameters by exclusively maintaining\nredundant neurons, resulting in an approximate 1.4x increase in inference speed (Dalvi et al., 2020).\nBesides interpreting and analyzing model weights, some work studies model activations (e.g.,xWu or x\u2032)\nto interpret their functionality. Theprobing techniqueis the most popular way for this purpose (Belinkov"
    },
    {
        "vector_id": 5508,
        "text": "et al., 2018; Tenney et al., 2018; Jawahar et al., 2019; Rogers et al., 2021), identifying whether a specific\nconcept is encoded within the representations. The basic idea is developing an auxiliary classifierg to map\nfrom the representationsx\u2032to the interested concept spaceC, such as syntax and part-of-speech knowledge,\n12 and the performance ofg interprets how much information encoded inx\u2032 is related to the concepts inC.\nThis technique motivates developing better parameter-efficient (Chen et al., 2022), domain-specific (Das\net al., 2023), and robust (Bai et al., 2021; Wang et al., 2023a) LLMs. Recent studies (Chen et al., 2023a;\nAhdritz et al., 2024) also apply the probing method to detect the knowledge boundary of a LLM so that the\nhallucinated responses could be reduced. Some researchers (Bricken et al., 2023; Cunningham et al., 2023)\npoint out another direction to interpret the model hidden activations, calleddictionary learning, which is"
    },
    {
        "vector_id": 5509,
        "text": "motivated by the assumption ofsuperposition (Elhage et al., 2022; Sharkey et al., 2022). The superposition\nassumes that LLMs would learn an over-complete set of non-orthogonal features, thereby transcending\nthe limitations imposed by the dimensionality of the representation space. Therefore, the researchers aim\nto reconstruct and interpret these features to understand the internals of the model. Practically, they\ndevelop a sparse auto-encoderg to reconstruct the representations{xn}, which shows that humans could\nwell interpret the learned sparse features ofgaccording to their most activation words. Their research shows\nthat this method could be used for morecontrollable generation. Specifically, if forcing a sparse feature to be\nactivated, then the language modelf would change its response to perform the particular behavior of that\nsparse feature. For example, given \u201c1,2,3,4,5,6,7,8,9,10\u201d as input, the model originally generates numbers"
    },
    {
        "vector_id": 5510,
        "text": "as output. However, when they are forced to magnify the activations of a sparse feature called \u201cDNA\u201d,\nthe model changes its output to \u201cAGACCAGAGAGAAC\u201d. In general, while the explanation techniques for\nfeed-forward networks primarily offer insights for model development, they have also demonstrated promising\napplications in areas such as model editing and controllable generation.\n3.3 Challenges\nInterpreting the functionality of internal modules is still in its infancy, and we identify two challenges to be\ntackled in this direction.\n3.3.1 Complexity of Individual Models and Their Interactions\nThe transformer-based language model contains two types of modules that collaborate based on the residual\nmechanism (He et al., 2016), which enables later modules to utilize, enhance, and/or discard outputs from\npreceding modules. Formally, the output of thel-th module is denoted asxl = fl(xl\u22121) +xl\u22121, where fl"
    },
    {
        "vector_id": 5511,
        "text": "could be a self-attention module or a feed-forward network. Research in this area aims to interpret how\ndifferent modulesfi and fj work together fori\u0338= j. Pilot studies (Elhage et al., 2021; Olsson et al., 2022)\nfind that stacked self-attention modules could formInduction Heads, which demonstrate a strong correlation\nwith the in-context learning capability. Specifically, the induction head encourages the model to predict the\nword \u201cB\u201d followed by a sequence \u201cAB...A\u201d. Their study finds a specific phase during pre-training LLMs where\nboth induction heads and the in-context learning capability emerge from the model. Following this track,\nresearchers observe diverse functional heads within LLMs for different tasks, such as \u201cName Mover Head\u201d\nand \u201cDuplicate Token Head\u201d for the object identification task (Wang et al., 2022a), \u201cSingle Letter Head\u201d\nand \u201cCorrect Letter Head\u201d for the multiple-choice question answering task (Lieberum et al., 2023), and"
    },
    {
        "vector_id": 5512,
        "text": "\u201cCapitalize Head\u201d as well as \u201cAntonym Head\u201d for the general purpose tasks (Todd et al., 2023). Although\nthese studies have indeed deepened our understanding of cross-module effects, their analyses are grounded\non specific tasks or scenarios, leaving it uncertain whether these findings possess broad generalizability.\n3.3.2 Nature of Polysemantic and Superposition Assumption\nInterpreting the functionality of a single neuron (one row/column vector of a weight matrix) fails in an-\nalyzing large language models since a single neuron could be activated by multiple and diverse meanings,\ncalled polysemantic (Arora et al., 2018; Scherlis et al., 2022; Bricken et al., 2023). This nature leads to\npoor interpretability: explaining a single neuron usually does not reflect a concise human concept. Some\nresearchers (Elhage et al., 2022; Sharkey et al., 2022) assume that this phenomenon is caused by the super-"
    },
    {
        "vector_id": 5513,
        "text": "position of an over-complete set of features learned by the models. Based on this assumption, we may reach\nanother level of explanation by decomposing the model weights to reconstruct a large number of features.\nHowever, the two critical problems of this approach are still unclear: (1) How do we ensure our recon-\nstructed features faithfully represent the latent features learned by the models? (2) How do we interpret our\nreconstructed features with human language?\n13 4 LLM Debugging with Sample-based Explanation\nIn this section, we discuss sample-based explanation strategies for LLMs, which aim to trace back the answers\ngenerated by LLMs to specific training samples (i.e., documents) or document fragments in the corpora. The\nutility of sample-based explanations for LLMs is multifaceted. First, tracing back the predictions of LLM to\nthe training samples can provide evidence for the generation results, which facilitates model debugging in"
    },
    {
        "vector_id": 5514,
        "text": "cases of errors and increases the trustworthiness of the model from users when the outcomes are accurate.\nIn addition, it can also help researchers understand how LLMs generalize from training samples. If the\noutputs of LLMs can be traced back to exact subsequences directly spliced from the training data, it might\nsuggest that the LLM is simply memorizing the data. In contrast, if the generation results and the influencing\ntrainingsamplesareabstractlyrelated, itcouldindicatethatLLMscanunderstandtheconceptsandgenerate\nresponses by reasoning from input prompts.\nIn this section, we start by systematically reviewing traditional sample-based explanation strategies, in-\ncluding gradient-based methods and embedding-based methods, as well as some preliminary explorations to\ngeneralize them to LLMs. We then analyze the challenges associated with generalizing the above strategies\nto LLMs with unique transformer structures and unprecedented numbers of parameters. Finally, we discuss"
    },
    {
        "vector_id": 5515,
        "text": "the insights to address the challenges, as well as open challenges worthy of further investigation.\n4.1 Literature Review of Sample-based Explanation\nIn this section, we denote the input space and output space asX and Y, respectively. In the context of\nlarge language models (LLMs),X is the space of token sequences known as the prompts, andYcould be\nthe space of discrete labels in classification tasks or the space of token sequences as output in generation\ntasks*. Accordingly, we have a training datasetDtrain = {zi = (xi,yi)}N\ni=1 with N samples drawn from the\njoint spaceX\u00d7Y , on which an LLM modelf\u03b8 is trained with pretrained parameters\u02c6\u03b8\u2208RP. We also have\na test samplez = (x,y) of interest, where we want to explain the generation ofy from x based on training\nsamples inDtrain (which can be viewed as the information source). The goal of sample-based explanation\nis to measure the influence of a training samplezi \u2208Dtrain or a certain segment withinzi, such that the"
    },
    {
        "vector_id": 5516,
        "text": "generation of LLMs can be well-explained and backed up by the selected training samples.\n4.1.1 Influence Function-based Methods\nOne strategy to quantify the influence of a training samplezi in the datasetDtrain to a test samplez is\nthroughtheinfluencefunction(Koh&Liang,2017;Hanetal.,2020). Itmeasuresthechangeoftheprediction\nloss L(z,\u03b8) for the test samplez, when the training samplezi undergoes a hypothetical modification in the\ndatasetDtrain duringmodeltraining. Thismodificationresultsinanalteredsetofoptimalmodelparameters,\ndenoted as\u02c6\u03b8\u2212zi . The most common modification of a training sample is to remove it from the dataset, where\nthe influence of the removal of a training samplezi on the loss at test samplez can be computed as follows:\nI(zi,z) =\u2212\u2207\u03b8L(z,\u02c6\u03b8)\u22a4H\u22121\n\u02c6\u03b8 \u2207\u03b8L(zi,\u02c6\u03b8), (1)\nwhere\u2207\u03b8L(z,\u02c6\u03b8) isthegradientofthelossfunction Lonthetestsample zevaluatedattheoptimalparameters\n\u02c6\u03b8, andH\u02c6\u03b8\ndef\n= 1\nN\n\u2211N\ni=1 \u22072\n\u03b8L(zi,\u02c6\u03b8) denotes the Hessian matrix of the LLM model at parameter\u02c6\u03b8. If we denote"
    },
    {
        "vector_id": 5517,
        "text": "the number of parameters in\u02c6\u03b8 as P, the na\u00efve inversion of the Hessian matrixH\u02c6\u03b8 leads toO(NP2 + P3)\ntime complexity andO(P2) space complexity (Schioppa et al., 2022), which is clearly infeasible for large\nmodels. To improve efficiency, Koh & Liang (2017) adopt an iterative approximation process, i.e., LiSSA\n(Linear time Stochastic Second-Order Algorithm), to calculate the Hessian-Vector Product (HVP) in Eq.\n(1), where the memory complexity can be reduced toO(P) and time complexity toO(NPr) (ris the number\nof iterations). To further reduce the complexity, Pruthi et al. (2020) propose an alternative to Eq. (1), i.e.,\nTracIn, which measures the influence ofzi on zby calculating the total reduction of the loss onzwheneverzi\n*Here, we should note that, in the pretraining phase of LLM with language modeling, the model either predicts masked\ntokens in xi (i.e., masked language modeling), or autoregressively predicts the next token (i.e., causal language modeling) in"
    },
    {
        "vector_id": 5518,
        "text": "xi. Therefore, in some works,yi is omitted, and onlyxi is included for discussions.\n14 is included in the minibatch during model training. The TracIn measurement can be formulated as follows:\nITracIn (zi,z) =\n\u2211\nt:zi\u2208Bt\nL(z,\u03b8t) \u2212L(z,\u03b8t+1) \u22481\nb\n\u2211\nt:zi\u2208Bt\n\u03b7t\u2207\u03b8L(zi,\u03b8t) \u00b7\u2207\u03b8L(z,\u03b8t) , (2)\nwhere Bt is thet-th mini-batch fed into the model during training,\u03b8t is the parameter checkpoint at thet-th\nstep, \u03b7t is the step size, andb is the size of the mini-batch. According to the above equation, TracIn only\nleverages gradient terms, where HessianH\u02c6\u03b8 is removed from the influence measurement. This substantially\nimproves the efficiency. However, such complexity is still prohibitive for large models from both the compu-\ntational and memory perspectives. In addition, TracIn can only estimate the influence ofadding/removing\nthe sample to the loss, where variants of the vanilla influence function defined in Eq. (1) can measure the"
    },
    {
        "vector_id": 5519,
        "text": "influence of other modifications of the training samplezi, such as perturbation (e.g., masking out a segment\nof a documentxi). To adapt the vanilla influence function of Eq. (1) to explain transformers, Schioppa\net al. (2022) propose to use Alnordi iteration (Arnoldi, 1951) to find the dominant eigenvalues and eigenvec-\ntors of the Hessian matrix on randomly sampled subsetsDsub, with |Dsub|\u226a|D train|. In such a case, the\ndiagonalized Hessian can be cheaply cached and inverted, where thecomputational and memory complexity\ncan be substantially reduced. Previous work mainly focuses on reducing the complexity of calculating the\ninfluence of a single training sample. Observing that finding the most influential training sample onz needs\nto iterate Eq. (1) overall N training samples, Guo et al. (2021) propose to use fast KNN to pre-filter a\nsmall subset of influence-worthy data points fromDtrain as candidates to explain small pretrained language"
    },
    {
        "vector_id": 5520,
        "text": "models, whereas Han & Tsvetkov (2022) propose to iteratively find a small subsetDsub \u2282Dtrain whose\ngradient is the most similar to that of the downstream task examples. Recently, Grosse et al. (2023) pro-\npose to use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation\nto scale influence functions to LLMs up to 52B parameters. For adaptation, only influences mediated by\nthe multi-layer perceptron (MLP) layers are considered, whereas weights from the self-attention layers are\nfixed, as previous work has demonstrated that knowledge is mainly encoded in the MLP layers (Wang et al.,\n2023d). In addition, based on the assumption that weights from different MLP layers are independent, the\nEK-FAC approximated influence can be formulated as the sum of influences mediated by each layer:\nIEKFAC(zi,z) =\n\u2211\nl\n\u2207\u03b8(l) L(z,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121\u2207\u03b8(l) L(zi,\u02c6\u03b8), (3)"
    },
    {
        "vector_id": 5521,
        "text": "where \u03b8(l) denotes the weights of thel-th MLP layer, and\u02c6G\u02c6\u03b8(l) is the EK-FAC approximated Gauss-Newton\nHessian for \u03b8(l). Since the inversion of L small Kl \u00d7Kl matrices (i.e., O(L\u00d7K3\nl )) is substantially more\nefficient than the inversion of a largeLKl \u00d7LKl matrix (i.e.,O((LKl)3)), IEKFAC can be adaptable to very\nlarge models, let alone the HVP can be further simplified by utilizing the EK-FAC properties.\nRecently, influence function-based explanation has been used in efficient finetuning of LLMs, where influence\nfunction has been used to select a small subset of training samples given few-shot validation samples for a\nspecific downstream task, where the training overhead can be substantially improved (Xia et al., 2024).\n4.1.2 Embedding-based Methods\nAnother strategy for sample-based explanation involves leveraging the hidden representations within the\ntransformer architecture, which is recognized for encoding high-level semantics from textual data, to calculate"
    },
    {
        "vector_id": 5522,
        "text": "the semantic similarity betweenz and zi. The similarity can also be used to measure the influence ofzi on\nz as explanations (Rajani et al., 2019). Specifically, Akyurek et al. (2022) propose to represent the training\nsample zi and test samplez by concatenating the input and output aszcat\ni = [xi||yi], zcat = [x||y]. The\nconcatenation is feasible for generation tasks where the outputy lies in the same token sequence space as\nthe input promptx. The similarity betweenzi and z can then be calculated as follows:\nIemb (zi,z) =\nf(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\u00b7f(l)\n\u02c6\u03b8 (zcat)\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat\ni )\u22a4\n\ued79\ued79\ued79\n\ued79\ued79\ued79f(l)\n\u02c6\u03b8 (zcat)\n\ued79\ued79\ued79\n, (4)\nwhere f(l)\n\u02c6\u03b8 is the sub-network that outputs thel-th layer intermediate activation of the pretrained LLMf\u02c6\u03b8.\nThe Eq. (4) has a similar form as the vanilla influence function defined in Eq. (1) as well as its TracIn\n15 alternative defined in Eq. (2), which assigns a scoreIfor the explaineez for each training samplezi in the"
    },
    {
        "vector_id": 5523,
        "text": "dataset Dtrain as the explanation confidence of the samplezi.\nCompared with the influence function methods introduced in the previous part, embedding-based methods\nare computationally efficient, as for each explaineez, the explanation score from a training samplezi requires\nonly one forward pass of the transformer network. In addition, the calculation can be easily paralleled for\ndifferent training samples. However, the disadvantage is also evident: These methods lack a theoretical\nfoundation and may fail to identify important training samples that may not be semantically similar to the\ntest sample. Consider the following toy example: Training sampleszi = (\u201c1+1=\u201d, \u201c2\u201d) andzj = (\u201c2+2=\u201d,\n\u201c4\u201d) make the LLM gain the ability to conduct arithmetic calculation, which explains why prompting the\nmodel withx = \u201c100+100\u201d gives the resultsy=\u201c200\u201d. However, the embeddings between the test samplez"
    },
    {
        "vector_id": 5524,
        "text": "and the two training sampleszi and zj can be very different when calculated via Eq. (4) (Akyurek et al.,\n2022). Therefore, embedding-based methods may not be able to faithfully find the training samples where\nthe explanations require generalization ability beyond semantic similarity.\n4.2 Case Study: EK-FAC-based Influence Estimation\nInthispart, weimplementtheEK-FAC-approximatedinfluencefunctionproposedinGrosseetal.(2023), and\nverify its scalability and effectiveness on LLMs with billions of parameters, including GPT2-1.5B (Radford\net al., 2019), LLaMA2-7B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), and LLaMA2-13B.\n4.2.1 Experimental Design\nWe use the SciFact dataset (Wadden et al., 2020) as the corpora, which contains the abstract of 5,183\npapers from the domain of basic science and medicine. The explainee LLMs are obtained by finetuning the\npretrained LLMs for 20,000 iterations, where AdamW (Loshchilov & Hutter, 2018) is used as the optimizer,"
    },
    {
        "vector_id": 5525,
        "text": "and the learning rate and weight decay are set to 1e-5, 1e-3, respectively. Then, we use 500 samples from\nthe corpora to estimate the(i) uncentered covariance matrices of the activations and pre-activation pseudo-\ngradients Q(l)\nA , Q(l)\nS , and (ii) the variances of the projected pseudo-gradient\u039b(l) for each selected dense\nlayer l, and cache them on the hard disk (details see Eqs. (16) and (20) in Grosse et al. (2023)). We select\nthe c_fc layer for GPT2-1.5B, andgate_proj layer for LLaMA2-7B, Mistral-7B, and LLaMA2-13B*.\nFor evaluation, we randomly select 200 samples from the corpora to construct the test set, which we name\nSciFact-Inf. Specifically, for thej-th selected samplezj = (xj,xj) (here yj = xj as label equals the input\nin language modeling), we use the first three sentences inxj, i.e., \u02c6xj, to generate a completion\u02c6yj with the\nfinetuned LLM (here,\u02c6yj does not equal the remaining sentences inxj), and we aim to explain the generation"
    },
    {
        "vector_id": 5526,
        "text": "of \u02c6yi from \u02c6xj with the finetuned LLM with the training samples via EK-FAC approximated influence scores\ndefined in Eq. (3). Ideally, thej-th training samplezj itself should be the most influential sample w.r.t. the\ngeneration of\u02c6yj for test sample\u02c6zj, which facilitates quantitative analysis of the effectiveness of Eq. (3).\nIn our implementation, for each test sample\u02c6zj, we first calculate the EK-FAC approximated HVP part of\nthe influenceIEKFAC(zi,\u02c6zj), i.e.,\u2211\nl\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8)\u22a4( \u02c6G\u02c6\u03b8(l) + \u03bb(l)I)\u22121, which is shared for all training samples\nzi. Specifically, we record the layer-wise gradient\u2207\u03b8(l) L(\u02c6zj,\u02c6\u03b8) and calculate the HVP with the cachedQ(l)\nA ,\nQ(l)\nS as Eq. (21) in Grosse et al. (2023). We then go through candidate training samples (1 positive and\n99 negative), calculate the gradient\u2207\u03b8(l) L(zi,\u02c6\u03b8), and take inner-product with the approximate HVP as the\nlayer-wise influence. Finally, the layer-wise influences are summed up as Eq. (3) as the total influence"
    },
    {
        "vector_id": 5527,
        "text": "IEKFAC(zi,\u02c6zj). We rank the influence and calculate the top-K hit rate of the positive training sample.\n4.2.2 Results and Analysis\nThe experimental results are summarized in Table 4. From Table 4 we can find that, the EK-FAC ap-\nproximated influence function achieves a good accuracy in finding the training sample that has the greatest\ninfluence on the generation of a test sample, even if only the influences mediated by a small part of dense\n*All the implementation and layer names are based on the huggingface transformers, where the details can be found in\nhttps://huggingface.co/docs/transformers/en/index.\n16 Table 4: Effectiveness of EK-FAC approximated influence function on the established SciFact-Inf dataset.\nTime (Pre.) stands for the time for precomputing theQA, QS, and\u039b. Time (Inf.) stands for the time for\ncalculating the influence of 100 training samples per test sample. Experiments for GPT2-1.5B, LLaMA2-7B,"
    },
    {
        "vector_id": 5528,
        "text": "Mistral-7B are done on 4 A100 GPUs, whereas experiments for LLaMA2-13B are done with 8 A100 GPUs.\nStrategy LLM Recall@5 Recall@10 Time (Pre.) Time (Inf.)\nRandom - 0.0100 0.0200 - -\nInf. Func.\nGPT2-1.5B 0.6368 0.7363 0h 27min 0min 28sec\nMistral-7B 0.6418 0.6866 2h 05min 1min 47sec\nLLaMA2-7B 0.8063 0.8308 1h 37min 1min 34sec\nLLaMA2-13B 0.7811 0.8940 3h 11min 3min 08sec\nlayers are considered. In addition, we find that the main computational bottleneck in calculating the EK-\nFAC-based influence is to estimate the covariancesQ(l)\nA , Q(l)\nS and variance\u039b(l), which can take hours when\n500 training samples are used for the estimation. However, after the estimation, it is relatively cheap to\ncalculate the influence of 500 training samples for each test sample, which takes around 3 minutes for a 13B\nLLaMA2 model on 8 A100 GPUs. This demonstrates the scalability of EK-FAC-based influence by assuming\nthe independence of different dense layers and using EK-FAC to simplify the computation."
    },
    {
        "vector_id": 5529,
        "text": "4.3 Challenges\nOverall, explaining the generation of LLMs by tracing back to the training samples is still an emerging area.\nOpen questions need to be addressed to further advance the field. In this section, we identify three main\nchallenges as follows, which can serve as directions for future explorations.\n4.3.1 Strong Assumptions for Scalability\nThe unprecedented number of parameters in modern LLMs causes severe scalability issues for sample-based\nexplanation strategies. This is especially evident for the gradient-based methods, as the HVP in Eq. (1)\ninduces both high computational and space complexity. To address the bottleneck, strong assumptions are\nusually required to make it feasible for large models. For example, TracIn (Pruthi et al., 2020) simplifies\nthe second-order term in Eq. (1) via first-order approximation. Schioppa et al. (2022) assume the Hessian\nto be low rank. Grosse et al. (2023) that assume that the weights from different layers of the LLMs are"
    },
    {
        "vector_id": 5530,
        "text": "independent, as well as the tokens in different steps, such that EK-FAC can be appropriately applied to\napproximate the influence function. From the above analysis, we can find that while the method from\nGrosse et al. (2023) has the best scalability, it also has the strongest assumption, which may fail to hold\nin practice. While highly efficient to compute, embedding-based methods make the implicit assumption\nthat semantics similarly implies explainability, which we have demonstrated may not always be the case.\nTherefore, how to improve the scalability with weak assumptions needs to be investigated in the future.\n4.3.2 Explainability v.s. Understandability\nDespite the advantage of influence/embedding similarity to provide a qualitative measurement of a specific\ntraining sample as theexplanation for LLM generation, theunderstandability of the identified sample\ncan still be weak, where the connection between the selected training samples and the generation may not"
    },
    {
        "vector_id": 5531,
        "text": "be understandable to human beings. Specifically, Grosse et al. (2023) cautions that the sign of influence\nscore of the training tokens may be difficult for humans to connect to the positive or negative influence on\nthe generation results. This severely jeopardizes the usability of the identified training samples. In addition,\nGrosse et al. (2023) also found that, since LLMs are usually not trained to the minimum to avoid overfitting\n(and due to overparameterization, the number of local minimums may be large), the connection between\ninfluence defined in Eq. (1) with the counterfactual loss of removing the samplezi at z is also weak. For\nthe embedding-based methods, since most LLM models are black box transformer models, the similarity\nof embeddings can also be hard to interpret by human beings; therefore, it is imperative to improve the\ninterpretability of the identified training samples, such that tracing back becomes more meaningful.\n17 4.3.3 LLM-Oriented Sample-based Explanations"
    },
    {
        "vector_id": 5532,
        "text": "17 4.3.3 LLM-Oriented Sample-based Explanations\nFinally, we observed that both gradient-based and embedding-based methods are loosely connected to the\nLLM, as well as the backbone transformer networks. For example, algorithms like TracIn (Pruthi et al., 2020)\nare designed to scale up influence functions to large models, which are not specific for LLMs. Similarly, the\nembedding-based method proposed in Akyurek et al. (2022) is applicable to most machine learning models\nwith latent representations. Grosse et al. (2023) considers the specialty of LLMs by utilizing the knowledge\nneuron assumption of the backbone transformers (Wang et al., 2023d) to simplify the influence function,\nwhere the weights considered are constrained to the MLP layers, which may not fully utilize the property of\ntransformers. Therefore, how to further utilize the property of the LLM and the backbone transformer to"
    },
    {
        "vector_id": 5533,
        "text": "design LLM-tailored sample-based influence/similarity (either to reduce the computational/space overhead\nor to improve the explanation quality) is highly promising for future work.\n5 Explainability for Trustworthy LLMs and Human Alignment\nIn previous sections, we explore the use of explanation techniques for assessing and improving the perfor-\nmance of LLMs. In this section, we shift the focus towards examining LLM trustworthiness. As LLMs\nare increasingly integrated into various applications of daily life, including high-stakes areas like healthcare,\nfinance, and legal advice, it is crucial that their responses not only are accurate but alsoalign with human\nethical standards and safety protocols (Liu et al., 2023b; Li et al., 2023f). Thus, the need arises to extend\nthe scope of explanations from merely evaluating the accuracy of LLMs to scrutinizing their trustworthiness.\nHerein, we delve into how explanation techniques, discussed in the previous sections, can be instrumental in"
    },
    {
        "vector_id": 5534,
        "text": "assessing LLMs across key aspects of trustworthiness like security, privacy, fairness, toxicity, and honesty. It\nis worth noting that while explainability itself is an aspect of trustworthiness, it holds the promise of serving\nas a foundational tool for addressing other trustworthiness concerns.\n5.1 Security\nLLMs are known to be vulnerable to attacks and exploitation, such as spreading misinformation, launching\nphishing attacks, and poisoning training data (Derner et al., 2023). For enhanced safety, LLMs are designed\nto reject certain types of prompts that may result in the generation of harmful content, e.g., by precluding\nprompts likely to elicit unsafe outputs from the training phase. However, jailbreak techniques can circumvent\nthese restriction measures and manipulate LLMs into producing malicious contents. Malevolent users (i.e.,\nattackers) can craft special prompts that compel or induce LLMs to prioritize instruction following over"
    },
    {
        "vector_id": 5535,
        "text": "rejections (Liu et al., 2023c; Li et al., 2023a). For example, through Prefix Injection, attackers can use\nout-of-distribution prompt prefixes that are less likely to be rejected (walkerspider, 2022; Wei et al., 2023).\nAnother approach, called Refuse Suppression, involves directing or persuading models to ignore established\nsafety protocols (Wei et al., 2023; Zeng et al., 2024), where the instruction following ability is then employed\nto perform the attack.\nExisting methods mainly rely on prompt engineering for attacking LLMs, but they usually have low attack\nsuccess rates and significant time costs (Li et al., 2024c). Thus, by understanding and engineering latent\nrepresentations of LLMs, explanation methods provide a viable way to design advanced attacks and discover\nthe potential vulnerabilities of LLMs (Liu et al., 2021). For example, a recent work extracts \u201csafety patterns\u201d"
    },
    {
        "vector_id": 5536,
        "text": "via explaining the latent space of LLMs using representation engineering. Specifically, these patterns can be\ncaptured from the activation differences between malicious queries and benign queries. The salient portion\nof difference vectors\u2019 dimensions is localized and utilized to generate features of safety patterns. The safety\npatterns reflect the internal protection mechanisms within LLMs. Circumventing these patterns leads to\nnovel attacks, which helps exploring potential vulnerabilities of LLMs (Li et al., 2024c). Besides, a deeper\nunderstanding of fine-tuning can shed light on the reliability of existing safety measures. In particular, Jain\net al. (2023) use networking pruning, attention map activation, and probing classifiers to track the changes\nof model capabilities from pre-training to fine-tuning. These tools are helpful in finding salient weights\nthat identify crucial neurons to restore pre-training capabilities. These neurons have demonstrated that the"
    },
    {
        "vector_id": 5537,
        "text": "capabilities gained during fine-tuning can be removed easily through fine-tuning on other unrelated tasks.\nThis finding casts doubt on the robustness of current safety alignments in LLMs.\n18 5.2 Privacy\nRecent studies have revealed that LLMs such as ChatGPT can leak extensive amounts of training data\nthrough a method known as divergence attacks. These attacks utilize specially crafted prompts to lead the\nmodel away from its standard chatbot-style generation (Nasr et al., 2023). The risk of private data exposure\nthrough such means poses a serious challenge to the development of ethically responsible models. This issue\nis compounded by strategies similar to jailbreak attacks, where misalignment is exploited to induce LLMs\ninto operating in an unconventional \u201cdeveloper mode\u201d via out-of-distribution prompts (Li et al., 2023a).\nTraditional data pre-processing techniques, such as data sanitization (Lison et al., 2021), are impractical as"
    },
    {
        "vector_id": 5538,
        "text": "defenses for LLMs due to the enormous scale of training data.\nEnhancing LLM privacy involves two strategic approaches: (1) preventing the models from memorizing\nsensitive data, and (2) establishing safeguards against the release of sensitive information during content\ngeneration. The latter can employ techniques used in jailbreak defenses, treating prompts that solicit private\ninformationaspotentiallymalicious. TheformerapproachrequiresidentifyingwhetherLLMspossessspecific\nknowledge, which is traditionally tested by crafting question-answering (QA) tasks to determine if LLMs can\nprovide answers. However, this approach faces limitations due to LLMs\u2019 sensitivity to the phrasing of QA\nprompts, while the optimal prompt is usually unknown. To tackle the challenge, explanatory techniques can\nserve as a tool to confirm whether LLMs have internalized certain knowledge. For instance, via explaining the"
    },
    {
        "vector_id": 5539,
        "text": "relation between factual knowledge and neuron activations (Meng et al., 2022a; Dai et al., 2022; Hase et al.,\n2024), we may investigate whether and where a piece of factual knowledge is stored within transformers. In\naddition, Yin et al. (2024a) recently proposes the concept of \u201cknowledge boundary\u201d and develops a gradient-\nbased method to explore whether LLMs master certain knowledge independent of the input prompt.\n5.3 Fairness\nDespite LLMs\u2019 powerful generation capabilities, their widespread applications also bring concerns about\nexacerbating bias issues in society, as LLMs are able to learn social biases within human-generated cor-\npus (Gallegos et al., 2023). For example, in a gender bias case, \u201c[He] is a doctor\u201d is much more likely than\n\u201c[She] is a doctor\u201d because of the gender stereotype. In this subsection, we focus on fairness issues that\nrefer to biases related to race, gender, and age within human communities (Li et al., 2023g). There is a"
    },
    {
        "vector_id": 5540,
        "text": "rich body of literature quantifying fairness issues within LLMs through various tests (Adebayo et al., 2023).\nInterpretation complements these methods by providing a unique perspective for bias mitigation, focusing\non unraveling the mechanisms through which biases are embedded into LLMs. A research direction within\nthis domain is the examination of biased attention heads. For instance, Ma et al. (2023) detect stereotype\nencodings through probing attention heads and evaluating their attributions via Shapley values (Lundberg\n& Lee, 2017). The results unveil that approximately 15% to 30% of attention heads across six transformer-\nbased models are linked to stereotypes. These attention heads tend to specialize in maintaining various\nstereotypes, offering a potential pathway for developing effective de-biasing techniques. Moreover, scoring\nhead biases with gradient-based metrics offers another way for identifying biased heads (Yang et al., 2023a)."
    },
    {
        "vector_id": 5541,
        "text": "Furthermore, recent work has placed LLM representations under scrutiny (Zou et al., 2023a). Typically,\nspecific templates relevant to a given concept or function are designed beforehand. Then, representations\nclosely aligned with the concepts or functions are examined using principal component analysis (PCA). From\nthis analysis, a vector is derived from the first principal component to predict a certain bias.\nTo achieve fair model predictions, a diverse range of mitigation techniques have been proposed to debias\nmodels. One stream of work proposes to debias LLMs at the embedding level, refining embeddings through\nvariousapproaches. Forexample, arecentworkattemptstoalterbiasedembeddingswithminimalalterations\ntomakethemorthogonaltoneutralembeddings(Rakshitetal.,2024). Additionally, somestudiesconcentrate\non removing biases at the level of attention heads. Ma et al. (2023) address this by pruning attention heads"
    },
    {
        "vector_id": 5542,
        "text": "that significantly contribute to certain biases. Similarly, another study adapts movement pruning (Sanh\net al., 2020) to dynamically choose an optimal subset of low-bias attention heads (Joniak & Aizawa, 2022).\nBeyond modifying embeddings and pruning attention heads, another strategy involves targeting a specific\ngroup of neurons known to propagate biases. It unlearns the biases by retraining weight vectors for these\nneurons (Yu et al., 2023). Besides, bias mitigation can also be approached from a data-centric perspective\n19 using a few training samples (Thakur et al., 2023). This work uses a pre-trained model to find the most\nbiased training examples, and then modifies these examples to fine-tune the model.\n5.4 Toxicity\nToxicity is another form of harmful content that LLMs may produce. This issue arises because LLMs are\noften trained on extensive online text corpora that have not been thoroughly filtered, containing elements of"
    },
    {
        "vector_id": 5543,
        "text": "toxicity that can hardly be fully eliminated. Toxicity can be identified by interpreting LLM components like\nthe feed-forward layers and attention heads. For instance, recent work reveals how toxicity is represented\nwithin LLMs by identifying multiple vectors promoting toxicity within the MLP layers and further recognizes\nrelevant dimensions through singular value decomposition (Lee et al., 2024). Furthermore, the exploration\nof geometric structures in per-layer representations offers another way to detect toxicity. Balestriero et al.\n(2023) applied spline formulations within MLPs to extract seven characteristic input features, demonstrating\ntheir utility in describing the domain of prompts and classifying toxic remarks.\nThe insights into how toxicity manifests within LLMs also shed light on mitigation strategies. Motivated by\nthe finding that toxicity can be reduced by manipulating relevant vectors, Lee et al. (2024) develops a method"
    },
    {
        "vector_id": 5544,
        "text": "called direct preference optimization (DPO), where paired toxic and non-toxic samples are utilized to fine-\ntune models so that non-toxic content is promoted. By examining the changes in the parameter matrices\nduring the fine-tuning process, it substantiates that even minor adjustments to these critical vectors can\nreduce toxicity. Built on the observation that LLMs\u2019 representations are updated by outputs from attention\nlayers (Elhage et al., 2021), another work attempts to reduce toxicity by identifying the \u201ctoxicity direction\u201d\nand then adjusting representations in the opposite direction (Leong et al., 2023).\n5.5 Truthfulness\nOne prominent drawback of LLMs is their tendency to confidently produce false statements. These state-\nments fall into two main categories: 1) statements that contradict learned knowledge within models, a\nproblem often related to models\u2019honesty; 2) statements that are factually incorrect and appear to be fab-"
    },
    {
        "vector_id": 5545,
        "text": "ricated by models, a phenomenon commonly referred to ashallucination. In the following, we delve into\nvarious approaches that aim to understand aforementioned two behaviors by leveraging explainability tools.\n5.5.1 Honesty\nHonesty of LLMs describes models\u2019 ability to produce true statements based on their learned information,\nwhere dishonest behaviors significantly compromise the perceived trustworthiness of LLMs. Numerous stud-\nies have been undertaken to understand how and why dishonest behaviors happen by looking inside LLMs.\nOne notable work attempts to distinguish dishonesty by training a classifier to predict the accuracy of state-\nments (Azaria & Mitchell, 2023). The classifier is simply trained on top of activations from the hidden layers\nof LLMs. These activations are generated from true and false statements. The classifier reaches an accuracy\nrange between 60% and 80%, which suggests that LLMs may be internally aware of the truthfulness of their"
    },
    {
        "vector_id": 5546,
        "text": "outputs (Azaria & Mitchell, 2023). Furthermore, research by Campbell et al. (2023) localizes dishonesty\nbehaviors at the level of attention heads. This study employs specially crafted prompts to induce dishonest\nresponses, and then trains logistic classifiers on models\u2019 activations over true/false statements. It also em-\nploys activation patching to substitute lying activations with honesty ones. Intriguingly, both approaches\nhave witnessed the importance of layer 23\u221229 in flipping dishonesty behaviors. Besides, another popular\nmethod tries to study the geometric structure of true/false statements(Marks & Tegmark, 2023). Typically,\nthese structures are visualized by projecting representations of statements onto two principal components.\nA clear linear structure and the truth directions can be derived to mitigate the dishonest behaviors.\n5.5.2 Hallucinations\nHallucinations in LLMs can arise due to poor data quality, biases, outdated knowledge, and the lack of ex-"
    },
    {
        "vector_id": 5547,
        "text": "plicit knowledge (Xu et al., 2024; Zhu et al., 2023b). However, whether LLMs are aware of their hallucination\nbehaviors remains an open question. Recent work investigates this question by examining models\u2019 hidden\n20 representation space (Duan et al., 2024). It examines three hidden states involving a question, its correct\nanswer and its incorrect answer, which are used to compute an \u201cawareness\u201d score. This metric quantifies\nthe uncertainty of LLMs regarding their own answers, finding that adversarially induced hallucination can\nincrease models\u2019 awareness. Additionally, Li et al. (2024b) illustrates the major differences between models\u2019\noutput and their inner activations, identifying these discrepancies as a potential source of hallucination. By\ntraining linear probing classifiers on each attention head\u2019s activations, the most specialized attention head\nis identified. An orthogonal probe is subsequently trained with the identified specialized probe. The experi-"
    },
    {
        "vector_id": 5548,
        "text": "ments reveal that \u201ctruth\u201d might exist in a subspace instead of a single direction (Li et al., 2024b). Another\nwork investigates the source of hallucination by analyzing patterns of source token contributions through\nperturbations (Xu et al., 2023c). Their findings suggest that hallucinations may stem from the models\u2019\nexcessive dependence on a restricted set of source tokens. Besides, the static distribution of source token\ncontribution, termed as \u201csource contribution staticity\u201d, can be used as another indicator of hallucinations.\nBuilding on the above insights into LLM hallucinations, Duan et al. (2024) apply PCA to derive the direction\nofthecorrectanswer\u2019sfinalhiddenstate, andenhancethehiddenrepresentationswiththisdirectiontoreduce\nhallucinations. In contrast, Li et al. (2024b) adopts a different approach, by intervening on top-K specialized\nattention heads, while minimizing the influence of the rest attention heads within models. Different from"
    },
    {
        "vector_id": 5549,
        "text": "PCA that identifies a single principle direction, this work adopts two distinct techniques to find multiple\ndirections of intervention. First, they use orthogonal vectors of each probe\u2019s hyperplane, which is similar\nto PCA. Second, they leverage vectors that connect the mean of the true and false distributions (Li et al.,\n2024b). The vectors derived from mean shift has been demonstrated more effective than those from probe\nclassifiers, which presents another feasible strategy for identifying directions of truth.\n5.6 Challenges\nWe discuss the challenges in employing explanations to improve models\u2019 trustworthiness and enhance align-\nment from two aspects: 1) limitations of existing detection techniques, and 2) shortcomings of mitigation\nstrategies based on explanations.\n5.6.1 Challenges of Existing Detection Methods\nCurrent detection methods primarily focus on the level of LLM modules, such as layers, attention heads, and"
    },
    {
        "vector_id": 5550,
        "text": "representations. However, we still lack a finer-grained understanding of how knowledge is encoded within\nLLMs, such as at the level of individual neurons and weight parameters. Furthermore, we lack general and\nrobust strategies to identify these knowledge reliably. For instance, to identify gender biases, attention heads\nmight be examined and then the related heads pruned (Li et al., 2024b). This approach requires analyzing\neach model individually, rather than adopting a general approach. Moreover, existing localization approaches\nrely either on probing classifiers or casual scrubbing, which might not be reliable. For probing classifiers,\nthe pre-designed biases used to train these classifiers are crucial to their performance. On the other hand,\ncasual cleaning usually introduces new variables that complicate the analysis.\n5.6.2 Challenges of Mitigation Strategies\nSince LLMs are trained on vast corpora of data, it is impractical to mitigate the aforementioned trust-"
    },
    {
        "vector_id": 5551,
        "text": "worthiness issues from the data perspective. It is widely believed that XAI, i.e. understanding the inner\nmechanisms of LLMs, is a fundamental way to solve these issues. Thus, mitigation methods for LLMs are\ntypically developed based on explanations. Existing explanations are implemented using techniques from\nmechanistic interpretability and representation engineering (Zhao et al., 2024). While both streams of meth-\nods can alleviate these issues, they fail to fully address them. For example, principal component analysis\n(PCA) is often used to find geometric structures related to these issues, but cannot fix these issues. Another\npopular mitigation method is casual scrubbing, which introduces certain \"positive\" activations to replace\nthose demonstrated to be responsible for specific issues. However, the identified directions and patched\nactivations can only mitigate issues to a certain extent. Moreover, the changes to either representations or"
    },
    {
        "vector_id": 5552,
        "text": "activations could also influence other aspects of models\u2019 capabilities, which we are yet unable to evaluate.\n21 6 LLM Enhancement via Explainable Prompting\nA key distinction between LLMs and traditional machine learning models lies in the LLMs\u2019 ability to accept\nflexibly manipulated input data, namelyprompts, during model inference (Liu et al., 2023a). LLMs generally\ngive precedence to the information presented in these prompts when generating outputs. Therefore, to\nmitigate the opacity issue in LLM predictions, we can enhance prompts with understandable content, which\nis then prioritized over the LLMs\u2019 inherent and implicit knowledge. These enriched prompts can include\ndomain-specific insights, contextual information, or a step-by-step reasoning chain. In response, LLMs might\nreveal their decision-making processes during inference, which improves the explainability of their behaviors.\n6.1 Chain of Thoughts (CoT) Prompting"
    },
    {
        "vector_id": 5553,
        "text": "6.1 Chain of Thoughts (CoT) Prompting\nThe Chain of Thought (CoT) approach significantly enhances LLMs in tackling complex tasks (Wei et al.,\n2022). While LLMs are adept at generating human-like responses, they often lack transparency in their\nreasoning processes. This limitation makes it difficult for users to assess the credibility of the responses,\nespecially for questions that require detailed reasoning.\nTo bridge this gap, recent efforts have incorporated in-context learning with human-crafted explanations\ndirectly into prompts (Wei et al., 2022; Huang et al., 2023a; Yao et al., 2023b; Besta et al., 2023). Among\nthese approaches, the CoT method stands out by employing explicit knowledge to guide the reasoning\nprocess. Formally, we define the language model asf\u03b8, and input prompt asX = {x1,y1,x2,y2,...,x n},\nwhere x1,y1,x2,y2,...,x n\u22121,yn\u22121 denote the example question-response pairs for in-context learning, and"
    },
    {
        "vector_id": 5554,
        "text": "xn is the actual question. In a standard question-answering scenario, we have the model output asyn =\narg maxY p\u03b8(Y|x1,y1,x2,y2,...,x n). This approach, however, does not provide insights into the reasoning\nprocess behind the answeryn. Therefore, the CoT method proposes to include human-crafted explanations\nei for the i-th in-context example, resulting in a modified input formatX = {x1,e1,y1,x2,e2,y2,...,x n}.\nGiven the input, the model will output not onlyyn but also the generated explanationen:\nen,yn = arg max\nY\np\u03b8(Y|x1,e1,y1,x2,e2,y2,...,x n). (5)\nBesides allowing for a more transparent and understandable interaction with LLMs, the CoT approach is\nalso practically useful as it augments LLMs\u2019 functionality by opening a window for users to control the\nmodels\u2019 thought processes. Specifically, the usefulness of CoT methods lies in several key aspects:\n\u2022 Reducing Errors in Reasoning:By breaking down complex problems into a series of smaller tasks,"
    },
    {
        "vector_id": 5555,
        "text": "CoT reduces errors in complex calculations and logic-oriented tasks, leading to a more precise resolution\nof intricate problems (Wei et al., 2022; Qin et al., 2023; Zhang et al., 2023; Wang & Zhou, 2024b).\n\u2022 Providing Adjustable Intermediate Steps:CoT enables the outlining of traceable intermediate steps\nwithin the problem-solving process. This feature enables users to trace the model\u2019s thought process from\ninception to conclusion, and to adjust the prompts if undesirable model behaviors are observed (Lyu\net al., 2023; Wang et al., 2023d).\n\u2022 Facilitating Knowledge Distillation:The step-by-step reasoning processes derived from larger LLMs\ncan serve as a specialized fine-tuning dataset for smaller LLMs. It allows smaller models to learn complex\nproblem-solving by following explanations, effectively teaching them to tackle intricate questions with\nenhanced reasoning capabilities (Magister et al., 2022).\n6.2 Extended Methods of Explainable Prompting"
    },
    {
        "vector_id": 5556,
        "text": "6.2 Extended Methods of Explainable Prompting\nAdvanced techniques beyond CoT methods have been developed to broaden the range of reasoning paths\navailable to LLMs towards enhancing the transparency and understandability of the decision-making pro-\ncess (Yao et al., 2023b; Besta et al., 2023; Yao et al., 2023a; Dhuliawala et al., 2023; Lyu et al., 2023). We\nintroduce several notable examples below.\nTree-of-Thoughts (ToT).Proposed by Yao et al. (2023b), ToT advances beyond the traditional linear\nChain of Thought reasoning, offering a more versatile structure that allows models to navigate through\n22 multiple reasoning paths. ToT makes the reasoning process of LLMs more interpretable by closely aligning\nit with human thought processes, as humans naturally consider multiple options and possible outcomes in\nboth forward planning and retrospective analysis to reach conclusions (Sloman, 1996; Stanovich, 1999). This"
    },
    {
        "vector_id": 5557,
        "text": "capability enhances the capacity of LLMs to tackle complex challenges that require the ability to consider and\nreevaluate different strategies, such as devising game strategies or generating creative content. By simulating\nthe way humans think and make decisions, ToT not only makes their thought process more understandable\nto human users, but also improves the models\u2019 effectiveness in handling complex tasks.\nGraph of Thoughts (GoT). Proposed by Besta et al. (2023), GoT transforms the output of LLMs\ninto a graph format. This format visualizes information pieces as nodes and their connections as edges,\nenabling a more intricate and connected form of reasoning compared to previous methods CoT and ToT. By\norganizing data into nodes (individual concepts or pieces of information) and edges (relationship between\nthese concepts), GoT makes the logical connections within complex systems more understandable (Yao"
    },
    {
        "vector_id": 5558,
        "text": "et al., 2023a). This graphical representation brings several benefits for understanding complex information.\nFirstly, itenablesdynamicmodificationofrelationshipsbetweenconcepts, offeringaclearvisualizationofhow\nchanging one element affects the others. This is crucial in fields like legal reasoning (Cui et al., 2023; Boche\net al., 2024), scientific research (Ding et al., 2023; Choudhury et al., 2023), and policy analysis (Chen et al.,\n2023c), where the inter-dependencies between various factors can be intricate and subtle. Secondly, GoT\nenables an assessment of the significance of each node within the graph, providing insights into which pieces\nof information are most critical to the task. This level of adaptability and clarity makes GoT exceptionally\npowerful for analyzing and navigating complex information networks.\n6.3 Case Study: Is CoT Really Making LLM Inferences Explainable?\n6.3.1 Background and Experimental Settings"
    },
    {
        "vector_id": 5559,
        "text": "6.3.1 Background and Experimental Settings\nDespite the apparent intuitiveness of the CoT prompt design, a critical question remains unanswered:Does\nCoT really make LLM inferences explainable?In other words, can the information provided through CoT\nfaithfully reflect the underlying generation process of LLMs? We use multi-hop question-answering (QA) as\nthe scenario to investigate this problem.\nIn QA systems, answering multi-hop questions remains a significant challenge. Instead of leveraging a single\ninformation source, multi-hop questions require synthesizing information from multiple pieces or sources of\ndata into a coherent and logical sequence. While LLMs show good performance in single-hop QA tasks (Rad-\nford et al., 2019), their efficacy significantly declines in multi-hop situations (Tan et al., 2023; Kim et al.,\n2023a; Zhong et al., 2023). This discrepancy highlights the need for more advanced methods to effectively\nhandle the intricacy of multi-hop reasoning."
    },
    {
        "vector_id": 5560,
        "text": "handle the intricacy of multi-hop reasoning.\nCoT Prompts for Multi-hop QA. To address the above challenge, our case study applies the CoT\ntechnique. CoT relies on high-quality templates as the in-context learning prompt, where we give an example\nas below. Here, [x] denotes the test question. The \u201cThoughts\u201d following each \u201cQuestion\u201d are step-by-step\nproblem-solving statements for the multi-hop questions. The thoughts in the templates align the generation\nprocess of LLMs with human cognitive problem-solving patterns.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of the United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors"
    },
    {
        "vector_id": 5561,
        "text": "owned by General Motors .\nAnswer : General Motors\nQuestion : [x]\n\u0006 \u0005\n23 CoT Faithfulness for Explanation:To quantitatively measure the faithfulness of CoTs, we select fidelity\nas the corresponding metrics (Zhao et al., 2023b; Wachter et al., 2017):\nFidelity =\n\u2211N\ni=1\n(\n1 (\u02c6yi = yi) \u22121\n(\n\u02c6ymislead\ni = yi\n))\n\u2211N\ni=1 (1 (\u02c6yi = yi))\n\u00d7100%, (6)\nwhere yi denotes the ground truth label,\u02c6yi denotes the original model output with CoT, while\u02c6ymislead\ni\ndenotes the model output with misleading information inserted in the \"Thoughts\" section. In the following,\nwe give an example. Given the target question, the correct step-by-step thoughts should be: \u201cEllie Kemper\nis a citizen of the United States of America. The president of the United States of America is Joe Biden.\u201d\nTo mislead the model, we replace the thoughts with incorrect information (the underlined text) and ask the\nmodel to generate a new answer based on incorrect thoughts. If the model still generates the correct answer"
    },
    {
        "vector_id": 5562,
        "text": "after the modification, we believe that the CoT information does not faithfully reflect the true process of the\nanswer generation. On the other hand, if it generates an answer corresponding to the incorrect thoughts,\nthen we claim the thoughts are faithful.\n\u0007\nQuestion : What is the capital of the country where Plainfield Town Hall is located\n?\nThoughts : Plainfield Town Hall is located in the country of the United States of\nAmerica . The capital of United States is Washington , D.C.\nAnswer : Washington , D.C.\n...\nQuestion : Who has ownership of the developer of the Chevrolet Corvette (C4)?\nThoughts : The developer of Chevrolet Corvette (C4) is Chevrolet . Chevrolet is\nowned by General Motors .\nAnswer : General Motors\nQuestion : [ Who is the head of state of the country where Ellie Kemper holds a\ncitizenship ?]\nThoughts : Ellie Kemper is a citizen of Croatia. The head of state in Croatia is Zoran Milanovi\u0107.\nAnswer :\n\u0006 \u0005"
    },
    {
        "vector_id": 5563,
        "text": "Answer :\n\u0006 \u0005\nExperimental Settings.We evaluate the performance on the MQUAKE-CF dataset Zhong et al. (2023),\nwhich includes 1,000 cases for eachK-hop questions,K \u2208{2,3,4}, which totally consists of 3,000 questions.\nOur evaluation applies various language models, including GPT-2 (Radford et al., 2019) with 1.5 billion\nparameters, GPT-J (Wang & Komatsuzaki, 2021) with 6 billion parameters, LLaMA (Touvron et al., 2023a)\nwith 7 billion parameters, Vicuna-v1.5 (Chiang et al., 2023) with 7 billion parameters, LLaMA2-chat-hf (Tou-\nvron et al., 2023b) with 7 billion parameters, Falcon (Almazrouei et al., 2023) with 7 billion parameters,\nMistral-v0.1 (Jiang et al., 2023) with 7 billion parameters, and Mistral-Instruct-v0.2 (Jiang et al., 2023)\nwith 7 billion parameters. These models have demonstrated proficiency in both language generation and\ncomprehension.\n6.3.2 Experiment Results\nPerformance Improvement.The performance reported in Table 5 for multi-hop question answering high-"
    },
    {
        "vector_id": 5564,
        "text": "lights the effectiveness of the CoT across various models. GPT-J exhibits the most dramatic improvement,\nparticularly in 3-hop questions with over a 200% increase in accuracy, suggesting that CoT\u2019s coherent reason-\ning greatly enhances LLMs\u2019 question-answering ability. While GPT-2 shows modest gains, the performance\nof GPT-J and other models, such as LLaMA, Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and Mistral-v0.2,\nindicates that the CoT method may be more beneficial for more advanced models. LLaMA2 and Mistral-v0.2\ndisplay considerable improvements of 78.4% and 63.8%, respectively, in 3-hop questions, further supporting\nthis observation. Overall, the results underscore the potential of coherent reasoning techniques, i.e., CoT, in\nadvancing the question-answering capabilities of LLMs across different model architectures and sizes.\n24 Table 5: Multi-hop question answering performance on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops"
    },
    {
        "vector_id": 5565,
        "text": "Question Type 2-hops 3-hops 4-hops\nEdited Instances Base Enhanced Improve Base Enhanced Improve Base Enhanced Improve\nGPT-2 (1.5B) 13.6 15.9 16.9% \u2191 11.6 8.9 23.3% \u2193 7.0 8.4 20.0% \u2191\nGPT-J (6B) 23.1 51.9 124.7% \u2191 10.1 30.5 202.0% \u2191 21.3 49.8 133.8% \u2191\nLLaMA (7B) 47.7 65.1 36.4% \u2191 29.6 39.3 32.8% \u2191 52.4 62.9 20.0% \u2191\nVicuna-v1.5 (7B) 41.3 56.3 36.3% \u2191 22.7 29.7 30.9% \u2191 31.6 53.1 68.2% \u2191\nLLaMA2 (7B) 36.7 58.7 60.0% \u2191 17.0 30.3 78.4% \u2191 29.2 49.1 68.1% \u2191\nFalcon (7B) 42.3 61.7 45.7% \u2191 23.2 31.7 35.7% \u2191 33.3 48.6 45.7% \u2191\nMistral-v0.1 (7B) 49.0 69.3 41.5% \u2191 30.0 42.3 41.1% \u2191 48.7 63.2 29.9% \u2191\nMistral-v0.2 (7B) 44.0 56.3 28.0% \u2191 23.0 37.7 63.8% \u2191 32.9 56.2 70.9% \u2191\nFaithfulness Evaluation of CoT.Table 6 illustrates the impact of accurate versus misleading CoTs on the\nperformance of LLMs. The Fidelity metric indicates how faithfully the model\u2019s output reflects the reasoning\nprocess described in the CoT. Ideally, a high Fidelity score suggests that the model\u2019s final response is directly"
    },
    {
        "vector_id": 5566,
        "text": "based on the provided CoT, validating it as a faithful explanation of the model\u2019s reasoning pathway. However,\nas we will discuss below, a low Fidelity may not always imply a lack of faithfulness in the model\u2019s reasoning,\nwhich calls for developing more effective evaluation methods in future research.\nGPT-J and LLaMA exhibit high fidelity scores across different question types, indicating a strong adherence\nto the given reasoning paths. Conversely, models such as Vicuna-v1.5, LLaMA2, Falcon, Mistral-v0.1, and\nMistral-v0.2 show relatively high mislead accuracy scores with lower fidelity scores. In the experiments, we\nobserve that these models usually rely on their own generated thoughts instead of using incorrect informa-\ntion provided in the CoT. Mistral-v0.2, in particular, demonstrates the lowest fidelity scores and highest\nmisleading accuracy scores, suggesting a potential self-defense ability against false information. The lower"
    },
    {
        "vector_id": 5567,
        "text": "fidelity scores of later models may be attributed to their improved training processes on more diverse and\nhigh-quality datasets, enabling them to develop a better understanding of context and reasoning. As a result,\nthey are more likely to generate their own correct reasoning paths.\nIn conclusion, the generated thoughts can be generally viewed as faithful explanations of their output answer.\nWhile high fidelity scores generally indicate a model\u2019s adherence to the provided CoT, low fidelity scores\ndo not necessarily imply a lack of faithfulness, especially when the model demonstrates the ability to reject\nmisleading information. Further research on CoT faithfulness and the development of more sophisticated\nevaluation metrics could contribute to the advancement of interpretable and reliable language models.\nTable 6: CoT Faithfulness Evaluation on MQUAKE-CF.\nDatasets MQUAKE-CF\nQuestion Type 2-hops 3-hops 4-hops"
    },
    {
        "vector_id": 5568,
        "text": "Question Type 2-hops 3-hops 4-hops\nEdited Instances Correct Mislead Fidelity Correct Mislead Fidelity Correct Mislead Fidelity\nGPT-2 (1.5B) 15.9 5.2 67.3% 8.9 2.9 67.4% 8.4 1.3 84.5%\nGPT-J (6B) 51.9 7.3 85.9% 30.5 1.8 94.1% 49.8 2.0 96.0%\nLLaMA (6B) 65.1 9.9 84.8% 39.3 6.1 84.5% 62.9 6.0 90.5%\nVicuna-v1.5 (7B) 56.3 21.7 61.5% 29.7 12.7 57.3% 53.1 16.1 69.7%\nLLaMA2 (7B) 58.7 17.0 71.0% 30.3 8.3 72.5% 49.1 12.0 75.6%\nFalcon (7B) 61.7 24.0 61.1% 31.6 15.0 52.6% 48.6 23.1 52.4%\nMistral-v0.1 (7B) 69.3 24.0 65.4% 42.3 13.0 69.3% 63.2 18.4 70.8%\nMistral-v0.2 (7B) 56.3 47.9 14.8% 37.7 22.0 41.6% 56.2 37.3 33.6%\n6.4 Challenges\nWithin machine learning, explanation faithfulness refers to the degree to which an explanation accurately\nreflects the decision-making process of the model it is intended to elucidate (Li et al., 2022c). An explanation\n25 is considered as faithful if it causes the model to make the same decision as the original input. In this context,"
    },
    {
        "vector_id": 5569,
        "text": "the challenge faced by explainable prompting (e.g., CoT prompt) lies in two aspects: (1) directing language\nmodels to generate explanations that are genuinely representative of the models\u2019 internal decision-making\nprocesses, and (2) preventing language models from depending on potentially biased CoT templates.\nRegarding the first challenge, our case study has revealed that relatively small language models may generate\nanswers that do not align with the provided CoT rationales. Therefore, these rationales do not accurately\nrepresent the decision-making process within these models. Some efforts have been made to bolster the\nCoT capabilities of smaller language models by implementing instruction tuning with CoT rationales (Kim\net al., 2023b; Ho et al., 2022). These methods can help improve the explanation faithfulness of CoT for\nsmall language models, thereby addressing this issue to some extent. Nevertheless, it remains a challenging"
    },
    {
        "vector_id": 5570,
        "text": "problem of how to ensure the generated explanations (i.e., \u201cwhat the model says\u201d) are faithful to the internal\nmechanism (i.e., \u201cwhat the model thinks\u201d) of language models.\nRegarding the second challenge, recent research shows that explanations in the CoT can be heavily influenced\nby the introduction of biasing prompt templates into model input (Turpin et al., 2024). This is because\nexisting CoT requires carefully designed templates to prompt language models to produce explanations. If\nincorrect or biased information is encoded in such templates, the generated explanations could be misleading.\nRecently, Wang & Zhou (2024a) propose a novel decoding strategy to implement CoT with prompting, which\ncould mitigate this issue. However, how to effectively help language models get rid of the template reliance\nstill remains to be underexplored.\n7 LLM Enhancement via Knowledge-Augmented Prompting"
    },
    {
        "vector_id": 5571,
        "text": "Enhancing models with external knowledge can significantly improve the control and interpretability of\ndecision-making processes. While LLMs acquire extensive knowledge through pre-training on web-scale\ndata, this knowledge is embedded implicitly within the model parameters, making it challenging to explain\nor control how this knowledge is utilized during inference. Additionally, LLMs may not always encompass\nthe unique knowledge specific to certain domains, nor keep pace with the constantly evolving information in\nthe world. To address these limitations, this section discusses Retrieval-Augmented Generation (RAG) for\nthe explicit integration of external knowledge into the mechanism of LLM-based AI systems, aiming to yield\nmore interpretable predictions.\n7.1 Preliminaries: Retrieval-Augmented Generation\nBy fetching relevant information from external databases or the internet, RAG ensures that LLM outputs"
    },
    {
        "vector_id": 5572,
        "text": "are accurate and up-to-date. It addresses LLMs\u2019 limitation of relying on fixed and potentially outdated\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\nfrom an external source based on the user\u2019s query; (2)Generation: It incorporates this information into the\nmodel\u2019s generated response. Given an input queryx and the desired outputy, the objective function of\nRAG can be formulated as (Guu et al., 2020):\nmax\n\u03d5,\u03b8\nlog p(y|x) = max\n\u03d5,\u03b8\nlog\n\u2211\nz\u2208K\np\u03d5(y|x,z) \u00b7p\u03b8(z|x), (7)\nwherezstands for the external knowledge retrieved from a knowledge baseK. Thus, the target distribution is\njointly modeled by a knowledge retrieverp\u03b8(z|x) and an answer reasoning modulep\u03d5(y|x,z). The knowledge\nz serves as a latent variable. An RAG model is trained to optimize the parameters, so that it learns to\nretrieve relevant knowledgez and to produce correct answersy based onz and x. As LLMs possess stronger"
    },
    {
        "vector_id": 5573,
        "text": "text comprehension and reasoning abilities, they can directly serve as the reasoning modulep\u03d5 without\nfurther training. In this case, RAG can be treated as a data-centric problem:\nmax\nz\u2208K\nlog p(y|z,x) = max\nz\u2208K\np(z|x,y)\np(z|x) p(y|x), (8)\nwhere the goal is to find appropriate knowledge that supports the desired output. Theinterpretability of\nRAG-based models comes from the information inz: (1)zusually elucidates or supplements the task-specific\n26 information in x; (2) z could explain the generation of outputy. Unlike other deep models that directly\nestimate p(y|x) in an end-to-end manner, where the decision process is not comprehensible, the RAG process\nprovides justification or rationalez that supports the result.\nExisting Retrieval-Augmented Generation (RAG) approaches can be categorized based on when they inte-\ngrate external knowledge into the model\u2019s workflow. The first category incorporates external knowledge at"
    },
    {
        "vector_id": 5574,
        "text": "the inference stage. For instance, Karpukhin et al. (2020) employ dense vectors to identify related docu-\nments or text passages, enhancing the data retrieval step of RAG. Similarly, Lewis et al. (2020) refine the\ndata retrieval process to ensure only the most pertinent information influences the model\u2019s output. The sec-\nond category integrates external knowledge during the modeltuning stage. Some representative approaches\ninclude Guu et al. (2020); Borgeaud et al. (2022); Nakano et al. (2021). Generally, these methods embed\na retrieval mechanism into the model\u2019s training phase, enabling the model to utilize external data more\nefficiently from the outset.\n7.2 Enhancing Decision-Making Control with Explicit Knowledge\nThe incorporation of explicit external knowledge through RAG enhances the precision and controllability of\ndecision-making in LLMs. This method leverages real-time information from external databases to produce"
    },
    {
        "vector_id": 5575,
        "text": "responses that are not only accurate but also tailored to the specific requirements of each query. Below, we\nexplore the mechanisms by which RAG achieves a more controllable and directed content generation process,\nwith references to key papers that have contributed to these advancements.\n7.2.1 Reducing Hallucinations in Response\n\u201cHallucination\" in the context of LLMs refers to instances where these models generate information that,\nwhile coherent and contextually appropriate, is not based on factual accuracy or real-world evidence (Huang\net al., 2023c). This issue can lead to the production of misleading or entirely fabricated content, posing a\nsignificant challenge to the reliability and trustworthiness of LLMs\u2019 outputs. RAG offers a powerful solution\nto mitigate the problem of hallucinations in LLMs. By actively incorporating up-to-date, verified external\nknowledge at the point of generating responses, RAG ensures that the information produced by the model"
    },
    {
        "vector_id": 5576,
        "text": "is anchored in reality. This process significantly enhances the factual basis of the model\u2019s outputs, thereby\nreducing the occurrence of hallucinations. Shuster et al. (2021) applies neural-retrieval-in-the-loop archi-\ntectures to knowledge-grounded dialogue, which significantly reduces factual inaccuracies in chatbots, as\nconfirmed by human evaluations. Siriwardhana et al. (2023) introduces RAG-end2end, which joint trains re-\ntriever and generator components together. Their method demonstrates notable performance improvements\nacross specialized domains like healthcare and news while reducing knowledge hallucination.\n7.2.2 Dynamic Responses to Knowledge Updating\nRAG empowers LLMs with the ability to incorporate the most current information, keeping their decision-\nmaking processes aligned with the latest developments. This feature is especially vital in fast-evolving fields\nsuch as medicine and technology, where the need for timely and accurate information is paramount (Meng"
    },
    {
        "vector_id": 5577,
        "text": "et al., 2022b). For example, research by (Izacard & Grave, 2020) demonstrates significant enhancements\nin output relevance and accuracy through real-time information retrieval. Similarly, Han et al. (2023)\nsuggest using retrieved factual data to correct and update the knowledge within pre-trained LLMs efficiently.\nAdditionally, Wang et al. (2023e) introduce a method for integrating newly retrieved knowledge from a\nmultilingual database directly into the model prompts, facilitating updates in a multilingual context.\n7.2.3 Domain-specific Customization\nRAG enhances LLMs by incorporating knowledge from specialized sources, enabling the creation of models\ntailored to specific domains. Research by Guu et al. (2020) illustrates how integrating databases specific to\ncertain fields into the retrieval process can empower models to deliver expert-level responses, boosting their\neffectiveness in both professional and academic contexts. Shi et al. (2023) have applied this concept in the"
    },
    {
        "vector_id": 5578,
        "text": "medical domain with MedEdit, utilizing an in-context learning strategy to merge relevant medical knowledge\ninto query prompts for more accurate medical advice. Moreover, recent research finds that LLMs struggle\n27 to capture specific knowledge that is not widely discussed in the pre-training data. Specifically, Mallen\net al. (2023) observe that LLMs often fail to learn long-tail factual knowledge with relatively low popularity,\nfinding that simply increasing model size does not significantly enhance the recall of such information.\nHowever, they note that retrieval-augmented LLMs surpass much larger models in accuracy, particularly\nfor questions on well-known subjects, suggesting that this method can effectively bridge knowledge gaps.\nSimilarly, Kandpal et al. (2023) highlights LLMs\u2019 challenges with acquiring rare knowledge and proposes that\nretrieval augmentation offers a viable solution, minimizing reliance on extensive pre-training for capturing\nnuanced, less common information."
    },
    {
        "vector_id": 5579,
        "text": "nuanced, less common information.\n7.3 Challenges\nWe discuss the challenges in RAG that are relevant to its explainability aspects: (1) In the retrieval stage\np\u03b8(z|x), does the retrieved informationzalways elucidate the task-specific information contained in the input\nx? (2) In the generation stagep\u03d5(y|x,z), does z effectively serve as an explanation for the generation of\noutput y? Please note that our goal is not to exhaustively discuss all the limitations of RAG in this paper\nas RAG itself is a broad topic in NLP research. For a more detailed examination of the broader limitations\nof RAG, we direct readers to other reviews (Gao et al., 2023).\n7.3.1 Retrieval Accuracy Bottlenecks\nExistent RAG methods typically rely on similarity search to pinpoint relevant information (Lewis et al., 2020;\nGao et al., 2023), which represents a substantial improvement over basic keyword searches (Robertson et al.,"
    },
    {
        "vector_id": 5580,
        "text": "2009). However, these methods may struggle with complex queries that demand deeper comprehension and\nnuanced reasoning. The recent \u201clost-in-the-middle\u201d phenomenon (Liu et al., 2024) has revealed that an inef-\nfective retrieval can result in the accumulation of extraneous or conflicting information, negatively affecting\nthe generation quality. To address this challenge, recent RAG approaches have integrated adaptive learning\nprocesses (Asai et al., 2023). This advancement enables the retrieval system to refine their performance over\ntime through feedback, adapting to evolving language use and information updates, ensuring their responses\nremain relevant and accurate. Nonetheless, efficiently handling intricate and multi-hop questions remains a\nsignificant challenge, highlighting the need for ongoing research to enhance the capabilities of RAG systems.\n7.3.2 Controllable Generation Bottlenecks"
    },
    {
        "vector_id": 5581,
        "text": "7.3.2 Controllable Generation Bottlenecks\nIn-context learning stands out as the premier method for incorporating external knowledge to boost the\ncapabilities of LLMs such as GPT-4 (Asai et al., 2023; Gao et al., 2023). Despite its effectiveness, there\u2019s no\nsurefire way to ensure that these models consistently leverage the provided external knowledge within the\nprompts for their decision-making processes. In practice, to achieve thorough coverage, commonly used dense\nretrieval usually returns a large volume of content, including both relevant and redundant information to the\ninput question. Unfortunately, redundant information in the model prompt raises the computational cost and\ncanmisleadLLMstogenerateincorrectanswers. Recentresearchshowstheretrievedinformationcandegrade\nthe question-answering task performance (Yoran et al., 2023; Petroni et al., 2020; Li et al., 2022a). Some\nrecent work proposes to fine-tune the LLM to improve resilience to noise and reduce hallucinations. However,"
    },
    {
        "vector_id": 5582,
        "text": "such approach still cannot prevent oversized retrieval information decrease the system interpretability (Yoran\net al., 2023; Xu et al., 2023b). The challenge of optimizing the use of external explanations to achieve more\nprecise and controlled decision-making in LLMs is an ongoing issue that has yet to be fully addressed.\n8 Training Data Augmentation with Explanation\nThis section explores the generation of synthetic data from explanations using large language models, a tech-\nnique poised to enhance various machine learning tasks. In machine learning, limited data availability often\nconstrains model performance, presenting a significant challenge across many domains. A viable solution\nis data augmentation, where LLMs, with their robust generative capabilities, can be utilized for text data\naugmentation (Whitehouse et al., 2023), such as transforming existing text samples into new variants (Dai"
    },
    {
        "vector_id": 5583,
        "text": "et al., 2023). Nevertheless, there are several challenges to be tackled for effective text augmentation. First,\n28 for utility, the generated samples need to exhibit diversity compared to the original data. Second, these\nsamples should be exhibit useful patterns relevant to the downstream tasks. To address these challenges,\nexplanation methods offer a valuable tool to guide data augmentation by providing supplemental contexts\nand useful rationales (Carton et al., 2021). Using LLMs for explanation-guided data augmentation is a\nnascent but promising field. In this section, we aim to outline feasible frameworks and discuss potential\napplications, offering directions for future research in this field.\nExplanations can be particularly beneficial in data augmentation within two scenarios. In thefirst scenario,\nexplanationsareusedtodelineatedesiredmodelbehaviorsortoidentifyexistingdeficiencies, whicheffectively"
    },
    {
        "vector_id": 5584,
        "text": "guides the data augmentation process of LLMs. Thesecond scenarioinvolves employing LLMs to directly\nproduce explanatory texts, which serve as supplementary information to enrich the dataset.\n8.1 Explanation-guided Data Augmentation for Mitigating Shortcuts\nMachine learning models are prone to make predictions with spurious correlations, also known as short-\ncuts (Geirhos et al., 2020), which are misaligned with human reasoning processes. This dependency on\nshortcuts underlies various challenges in machine learning, notably diminishing a model\u2019s ability to general-\nize to out-of-distribution samples and weakening its resilience against adversarial attacks (Xu et al., 2020).\nThe extent to which a model depends on shortcut features is intrinsically linked to its interpretability. Es-\nsentially, if a model\u2019s predictions are predominantly based on such unreliable features, it indicates that the"
    },
    {
        "vector_id": 5585,
        "text": "underlying mechanisms of the model are not fully comprehensible or interpretable from a human perspective.\nTherefore, post-hoc explanation techniques usually play a crucial role in detecting undesirable correlations\nbetween input and predictions within deep models (Liu et al., 2018; 2021). For example, Du et al. (2021)\nadopt Integrated Gradient (IG) to attribute a model\u2019s predictions to its input features, showing that the\nmodel tends to treat functional words, numbers, and negation words as shortcuts and strongly rely on these\nshortcut features for prediction in natural language understanding tasks.\nData augmentation can be applied to training better models that are less sensitive to shortcut features.\nExplanatory information such as counterfactuals (Wang & Culotta, 2021) has been incorporated in data\naugmentation to improve model robustness. It generates counterfactual samples by first identifying critical"
    },
    {
        "vector_id": 5586,
        "text": "features (e.g., word tokens) and then replacing these features with their antonyms, along with reversing\ntheir associated labels. Subsequently, the generated samples are combined with the original ones to train\ndownstream models. Furthermore, these techniques can be extended to enhance the out-of-distribution\nperformance of smaller models (Sachdeva et al., 2023; Wen et al., 2022). Namely, large language models\ncould serve as an effective tool to augment data. For example, LLMs are able to synthesize examples\nthat represent outlier cases or rare occurrences, which helps smaller models generalize better on unseen\ndata (Xu et al., 2023a). This could be helpful in building robust models in scenarios where data are\nscarce or confidential (Tang et al., 2023a). Besides, LLMs are promising in improving models\u2019 safety by\ngenerating adversarial examples that are more valid and natural compared to conventional approaches (Wang"
    },
    {
        "vector_id": 5587,
        "text": "et al., 2023f). First, the most vulnerable words are identified with attribution-based methods. Then, these\nwords are replaced by LLMs in a manner that maintains the integrity of the original text. The quality of\nthese examples can be examined with an external classifier. Subsequently, these adversarial examples are\nemployed to train downstream models, effectively fortifying them against potential attacks and boosting their\nsecurity. Similarly, LLMs are also helpful in mitigating biases such as fairness issues in models (He et al.,\n2023). This work claims that it automatically identifies underrepresented subgroups, and chooses an optimal\naugmentation strategy that avoids hurting other groups. New group examples are generated using LLMs\nwith human-providing labels. The experiments observe improvements on both underrepresented groups and\noverall model performance. This approach prevents skewed datasets from leading to unfair outcomes for a"
    },
    {
        "vector_id": 5588,
        "text": "certain demographic, thereby potentially promoting fairness in society.\n8.2 Explanation-enhanced Data Enrichment\nAs a powerful generative tool, LLMs have been leveraged to directly generate natural language explanations\nas augmented data. This strategy relies on LLMs\u2019 understanding abilities to assist smaller models in their\nlearning tasks. One objective in such work is to add natural language explanation generated by LLMs to\n29 training data, so as to enhance the performance of small models. Li et al. (2022b) introduce explanations\nfrom LLMs that facilitate the training of smaller models to enhance their reasoning capabilities and ac-\nquire explanation generation abilities. Specifically, three explanation generation approaches from LLM are\nutilized to train smaller models, including (1) explanations generated through chain of thought prompting,\n(2) explanations by rationalization prompting conditioned on golden labels and (3) hybrid approach that"
    },
    {
        "vector_id": 5589,
        "text": "combines the first two approaches. With the best case scenario in a reasoning case, a 9.5% improvement\non accuracy compared to 60x larger GPT-3 has been seen (Li et al., 2022b). It is worth noting that LLMs\nincluding ChatGPT and GPT-4 are competitive in generating coherent and logical explanations, except for\nlimited languages (Whitehouse et al., 2023). Explanations from LLMs have also been utilized to mitigate\nspurious correlations in aspect-based sentiment analysis tasks (Wang et al., 2023c). This study proposes\nusing LLMs to generate explanations for the sentiment labels of aspects in sentences (Wang et al., 2023c).\nThese explanations provide reasoning grounded in contextual semantics rather than relying on spurious\ncorrelations between words and labels. The explanations are integrated into the training of aspect-based\nsentiment analysis models through two methods: augmenting the training data with the explanations or"
    },
    {
        "vector_id": 5590,
        "text": "distilling knowledge from the explanations through mimicking behaviors. By focusing on the explanations\ninstead of superficial textual cues, the models can better learn the true associations between text and sen-\ntiment and become more robust, improving both in-domain performance and generalization ability (Wang\net al., 2023c). Another line of work involves integrating LLM rationales as additional supervision to guide\nthe training of smaller models. Experiments have shown that this approach not only requires fewer training\ndata but also outperforms traditional fine-tuning and distillation methods (Hsieh et al., 2023).\nIn addition to the existing application of augmentation techniques summarized above, we envision that in-\ncorporating additional text information can also be practical and efficient in enhancing the performance of\nvarious models. For example, one promising application lies in the realm of guiding the parameter learning"
    },
    {
        "vector_id": 5591,
        "text": "process of small models by using automatically generated explanations. Previous research has investigated\nthis avenue by directing the attention of natural language inference models towards human-crafted explana-\ntions (Stacey et al., 2022). As human-crafted explanations are both arduous and non-transferable, utilizing\nLLMs as generators presents a more economic and versatile alternation. Another potential application is to\nenhance model performance on complex tasks using natural language explanations from LLMs. For instance,\ncode translation generation tasks incorporate explanations as an intermediate step, improving model perfor-\nmance by 12% on average (Tang et al., 2023b). The result shows that explanations are particularly useful in\nzero-shot settings. Apart from assisting in smaller models, LLMs have demonstrated their ability to improve\ntheir own reasoning abilities by generating reliable rationales (Huang et al., 2022). Further, Krishna et al."
    },
    {
        "vector_id": 5592,
        "text": "(2023) embed post-hoc explanations, attributing scores to all input features, into natural language rationales.\nThis approach improves model accuracy by 10-25% across multiple tasks via in-context learning. Another\nstudy explicitly investigates LLMs\u2019 ability to generate post-hoc explanations in natural language. The ex-\nperiments show that LLMs spot key features with 75% accuracy (Kroeger et al., 2023). These studies present\na novel strategy for leveraging post-hoc explanations, beyond traditional natural language explanations, to\nenrich training data so as to bolster model performance.\n8.3 Challenges\n8.3.1 Computational Overhead\nConventional post-hoc explanations, built on well-trained models, are often resource-intensive tasks. The\nfirst scenario mentioned above leverages interpretability techniques to accurately diagnose dataset issues.\nThis process typically requires multiple rounds of model training and applying interpretability methods to"
    },
    {
        "vector_id": 5593,
        "text": "develop fair and robust models. Consequently, the crafting process can be both time and energy-consuming.\nGiven these challenges, exploring the development of data-centric evaluation metrics is crucial. These metrics\ncan offer a more efficient way to assess data issues, bypassing traditional, cumbersome explanation methods.\nBy focusing on these data-centric measurements, data issues can be diagnosed and fixed before training.\nThe number of training rounds needed is then significantly reduced. This shift not only streamlines model\ndevelopment but also helps reduce computational overhead, making the whole process more practical and\nefficient.\n30 8.3.2 Data Quality and Volume\nDespite their advanced capabilities, LLMs still have limitations when dealing with highly specialized or niche\ncontexts. For example, one of the most prominent issues is \u201challucination\u201d, where models generate plausible"
    },
    {
        "vector_id": 5594,
        "text": "but incorrect or misleading responses. This could adversely affect the quality of augmented data, potentially\nintroducing more biases to which LLMs are also vulnerable. Another challenge is controlling the relevance of\nLLM-generated content. That is, the explanations or data points may seem reasonable but often lack factual\naccuracy or nuances specific to a domain. Currently, we lack robust metrics to effectively measure the quality\nand relevance of these generated data relative to the original tasks. Determining the precise amount of data\nrequired is also challenging, often leading to new dataset imbalances. Managing the quality of LLM-generated\ndata is an immense challenge, as augmented data can introduce other biases (Zhao & Vydiswaran, 2021).\nThis stems from LLMs\u2019 limited ability to accurately control the quantity and distribution of generated data.\nMoreover, crafting effective prompts is more of an art than a science, adding uncertainty around generated"
    },
    {
        "vector_id": 5595,
        "text": "data quality. Together, these factors underscore the complexities and challenges in fully harnessing LLMs\u2019\npotential for data augmentation and related tasks.\n9 Generating User-Friendly Explanation for XAI\nPrevious sections mainly focused on quantitative explanations with LLM via numerical values. For example,\nsample-based explanation discussed in Section 4 aims to assign each training sample an influence score (see\nEqs.1-4) that measures the confidence that we can use that training sample to explain the prediction of\na test sample. However, using numerical values for explanations is not intuitive, which can be difficult to\nunderstand by practitioners with little domain knowledge (Latif & Zhai, 2024; Lee et al., 2023; Li et al.,\n2020). User-friendly explanations, on the contrary, aim to generate human-understandable explanations,\ne.g., natural language-based descriptions, regarding certain data, the reason why a model makes certain"
    },
    {
        "vector_id": 5596,
        "text": "predictions, or what role a neuron plays in the network, such that the generated explanations can be well-\nunderstood by both researchers and practitioners.\nGiven an explaineee, which can be a data sample(xi,yi), a neuron\u03b8i from a pretrained modelf\u03b8, or a\nprediction result \u02c6y based on the inputx, generating user-friendly explanation aims to map the explainee\ne to a sequence of natural language tokens as the explanation for the explaineee, such that the generated\nexplanations can be easily comprehended by human beings.\n9.1 User-friendly Data Explanation with LLMs\nData explanation refers to the process of translating difficult materials (e.g., program codes, long documents)\ninto concise and straightforward language so that they are easy to understand by humans. Language models\nhave long been used to generate explanations for textual data (Dai & Callan, 2019). Since modern LLMs are\ntrained on corpora composed of codes, math, and papers, they can be leveraged to explain data beyond pure"
    },
    {
        "vector_id": 5597,
        "text": "textual content. For example, Chen et al. (2021) have demonstrated that pretrained GPT models possess\nthe ability to understand and generate codes, where explanatory comments are generated simultaneously\nthat facilitate the understanding of programmers. In addition, Welleck et al. (2022) propose to explain math\ntheorems by providing detailed derivations, so that the theorems are easier to understand. Recently, LLMs\nhave also been used to elucidate academic papers (Castillo-Gonz\u00e1lez et al., 2022), making difficult content\nto be easily understood by individuals with little domain knowledge.\n9.2 Explaining Small Models with LLMs\nRecently, there has been growing interest in leveraging LLMs to generate free-text explanations for small\nmodels. For example, to explain black-box text classifiers, Bhattacharjee et al. (2023) propose a prompting-\nbased strategy to identify keywordsK = {k1,k2,...,k n}in the input textsx with pretrained LLMs that"
    },
    {
        "vector_id": 5598,
        "text": "are informative for the label y, and ask LLMs to substitute them with another set of keywords K\u2032 =\n{k\u2032\n1,k\u2032\n2,...,k \u2032\nn}, such that changed textx\u2032changes the label prediction toy\u2032. They view the textual mapping\nrule \u201cif we changeK into K\u2032 in x, theny will be classified asy\u2032\u201d as the counterfactual explanation for the\nmodel. In addition, to explain the neuron of a pretrained language model (e.g., GPT2), Bills et al. (2023)\n31 propose to summarize the neuron activation patterns intotextual phraseswith a larger language model (e.g.,\nGPT4), where the neuron activation patterns are expressed as a sequence of (token, attribution score) pairs.\nTo verify the identified patterns, they generate activation patterns according to the phrases via the same\nLLM and compare their similarity with the true activation patterns of the neuron, where the phrases with\nhigh scores are considered more confident to serve as the explanation for the neuron."
    },
    {
        "vector_id": 5599,
        "text": "The explaining ability of LLMs is not necessarily limited to text models. For example, Zhao et al. (2023a)\npropose using pretrained vision-language models to generate explanations for a neuron\u03b8i of an image clas-\nsification model. Specifically, for each class y = yc, they first find regions in images with labelyc that\nhave maximum activation of the neuron\u03b8i as the surrogate explainees for\u03b8i, and prompt LLMs such as\nChatGPT to generate candidate explanations (words, short phrases) for the class labelyc. Then, they use\nthe pretrained vision-language model CLIP (Radford et al., 2021) to match the candidate explanations with\nthe surrogate explainees as the explanations for the neuron\u03b8i. Recently, LLMs have also found applications\nin explaining recommender systems Zhu et al. (2023a). Specifically, Yang et al. (2023c) found that LLMs\ncan well interpret the latent space of sequential recommendation model after alignment, whereas Lei et al."
    },
    {
        "vector_id": 5600,
        "text": "(2023) propose to align user tokens of LLMs with the learned user embeddings of small recommendation\nmodel to generate explanations of user preferences encoded in the embeddings. Recently, Schwettmann\net al. (2024) propose a unified framework to explain all models where inputs and outputs can be converted\nto textual strings. Specifically, the explainer LLM is used as an agent to interact with the explainee model\nby iteratively creating inputs and observing outputs from the model, where the textual explanations are\ngenerated by viewing all the interactions as the context.\n9.3 Self-Explanation of LLMs\nDue to the black-box nature of LLMs, it is promising to generate user-friendly explanations for the LLMs\nthemselves, such that the operational mechanics and the predictions of LLMs can be well-understood by\nhuman experts. Based on whether the LLM needs to be retrained to generate explanations for themselves,"
    },
    {
        "vector_id": 5601,
        "text": "the self-explanation of LLM can be categorized into two classes:fine-tuning basedapproach andin-context\nbased approach, which will be introduced in the following parts.\nFine-tuning based approaches.Given sufficient exemplar explanations on the labels of the training data\n(e.g., in recommendation datasets such as the Amazon Review datasets (He et al., 2017) or the Yelp dataset\n(Zhou et al., 2020), users have provided explanations on why they have purchased certain items, which can\nbe viewed as explanations for the ratings), LLMs can learn to generate explanations for their predictions\nas an auxiliary task through supervised learning. One exemplar method is P5 (Geng et al., 2022), which\nfine-tunes the pre-trained language model T5 (Raffel et al., 2020) on both the rating and explanation data\nto generate an explanation alongside the recommendations. Recently, several works have improved upon"
    },
    {
        "vector_id": 5602,
        "text": "P5 (Cui et al., 2022; Zhu et al., 2024), which fine-tunes different LLMs such as GPT2, LLaMA, Vicuna,\netc., and propose different prompt learning strategies (Li et al., 2023d) with generating explanation as the\nauxiliary task. With explanations introduced as additional supervision signals to fine-tune pretrained LLMs\nfor recommendations, the performance can be improved with good explainability.\nIn-context based approaches.In many applications, there is often a lack of sufficient exemplar explana-\ntions. However, the unique capability of modern LLMs to reason and provide answers through human-like\nprompts introduces the potential for in-context based explanations. Here, explanations for predictions are\ncrafted solely based on the information within the prompt. A leading approach in this domain is the\nChain-of-Thoughts (CoT) prompting (Wei et al., 2022), which provides few-shot examples (with or without"
    },
    {
        "vector_id": 5603,
        "text": "explanations) in the prompt and asks the LLM to generate answers after reasoning step-by-step, where the\nintermediate reasoning steps that provide more context for generating the final answer can be viewed as\nexplanations. However, CoT generates reasoning first and then based on which generates predictions, where\nthe reasoning steps can influence prediction results (Lyu et al., 2023). If explanations are generated after\nthe prediction, since the explanation is conditioned on the predicted label, it can provide a more faithful\npost-hoc explanation of why the model makes certain decisions (Lanham et al., 2023). The application of\nin-context based self-explanation of LLMs is broad. For example, Huang et al. (2023d) explore generating\nzero-shot self-explanation of sentiment analysis with LLMs by directly asking them to generate explanations\nalongside the predictions. In addition, Huang et al. (2023a) propose a chain-of-explanation strategy that"
    },
    {
        "vector_id": 5604,
        "text": "32 aims to explain how LLMs can detect hate speech from the textual input. Lu et al. (2022) find that CoT\ncan generate well-supported explanations for question answering with scientific knowledge.\n9.4 Challenges\n9.4.1 Usability v.s. Reliability\nMany existing methods rely on prompts to generate user-friendly explanations, which are not as reliable as\nnumerical methods with good theoretical foundations. Ye & Durrett (2022b) find that the explanations by\nCoT may not be factually grounded in the inputs. Therefore, they believe that these explanations are more\nsuitable as post-hoc explanations regarding why the LLM makes certain predictions (no matter whether the\npredictions are right or wrong). However, the validity of viewing CoT explanations as post-hoc justifications\nhas been questioned by recent findings from Turpin et al. (2024), which uses biased datasets (e.g., the few-\nshot examples in the prompt always answer \u201cA\u201d for multiple choice questions) to show that the generated"
    },
    {
        "vector_id": 5605,
        "text": "explanations may be plausible, but systematically unfaithful to represent the true reasoning process of the\nLLMs. This issue arises from biased reasoning steps which inadvertently skew the predictions. Thus, there\u2019s a\ngrowing need for more theoretical scrutiny of user-friendly explanations to ensure faithfulness and credibility.\n9.4.2 Constrained Application Scenarios\nCurrently, the utilization of LLMs to explain smaller black-box models is mainly limited to those that deal\nwith data with rich textual information (Bhattacharjee et al., 2023; Lei et al., 2023). Although Zhao et al.\n(2023b) propose a strategy to explain image classifiers, the ability to match candidate textual explanations\nwith image patterns still relies on the pretrained vision-language model CLIP. This method may not be\napplicable to other domains such as graph machine learning (e.g., graph neural networks), or time-series"
    },
    {
        "vector_id": 5606,
        "text": "analysis (recurrent neural networks), where large pretrained models have demonstrated little progress com-\npared to natural language processing and computer vision. Therefore, there is a compelling need to devise\nmore versatile strategies for explaining models across a wider range of fields. This endeavor could depend\non the fundamental research on combining LLM with other domain-specific tasks, such as the development\nof Graph-Language Models that are applicable to unseen graphs in a zero-shot manner.\n10 LLMs for Interpretable AI System Design\nAn intriguing but challenging problem in XAI is creating model architectures or even AI systems that are\ninherently interpretable (Rudin, 2019), where different model components represent clear and comprehen-\nsible concepts or functionalities that are easily distinguishable from one another. Machine learning models\nsuch as support vector machines (Hearst et al., 1998) and tree-based models (Song & Ying, 2015) were clas-"
    },
    {
        "vector_id": 5607,
        "text": "sical techniques for achieving model interpretability. In the deep learning era, typical research areas in this\ncontext include concept-bottleneck models (Koh et al., 2020; Yuksekgonul et al., 2022), disentangled repre-\nsentation learning (Denton et al., 2017; Higgins et al., 2016), and network dissection (Bau et al., 2017; 2018).\nNevertheless, under the traditional deep learning setting, the usability of these techniques remains limited\nbecause of two major challenges. First, it is difficult to define the spectrum of concepts or functionalities\nthe model is expected to capture. Second, the efficacy of interpretable models often falls short compared to\nblack-box models, thereby constraining their practical utility.\nLarge foundation models, such as large language models (LLMs) and vision language models (VLMs), pro-\nvide opportunities to bridge the gap. By leveraging the common-sense knowledge embedded within them,"
    },
    {
        "vector_id": 5608,
        "text": "foundation models candesign interpretable architecturesby providing cues that encourage creating and using\nthe features or procedures within AI workflows. This is different from traditional deep learning pipelines\nwhere the deep models automatically discover the features during the training process, which may not end up\nwith model components with clear meanings. Furthermore, LLMs can decompose complex tasks into simpler\nand collaborative sub-tasks, enhancing both the system\u2019s interpretability and its overall performance.\n33 10.1 Designing Interpretable Network Architectures with LLMs\nRepresentative methods for developing interpretable deep architectures include Generalized Additive Models\n(GAMs) (Zhuang et al., 2021; Lou et al., 2012) and Concept Bottleneck Models (CBMs) (Koh et al., 2020;\nYuksekgonul et al., 2022). These models map inputs into a human-understandable latent space, and then"
    },
    {
        "vector_id": 5609,
        "text": "apply a linear transformation from this space to the target label. For example, to build a classifier that\ndiagnoses arthritis, we can let the model identify features such as \u201cbone spurs\u201d and \u201csclerosis\u201d, and then use\nthese interpretable features for the final decision. However, these approaches often require the involvement of\nexperts to define the latent space, which can limit the learning capabilities of deep models. Some work tries\nto automate the discovery of semantic concepts during model training, such as by requiring independence\nbetweenconcepts(Higginsetal.,2016;Yuetal.,2020)orclusteringdata(Ghorbanietal.,2019), buttheylack\ndirect control over the outcomes and does not ensure the clarity of the concepts. One promising strategy\nis to utilize LLMs to provide comprehensible concept candidates. Menon & Vondrick (2022) use human\nlanguage as an internal representation for visual recognition, and create an interpretable concept bottleneck"
    },
    {
        "vector_id": 5610,
        "text": "for downstream tasks. By basing the decision on those comprehensible concepts, the model architecture\nitself is provided with better transparency. Similarly, a recent approachLabo(Yang et al., 2023b) constructs\nhigh-performance CBMs without manual concept annotations. This method controls the concept selection\nin bottlenecks by generating candidates from the LLMs, which contain significant world knowledge (Petroni\net al., 2019) that can be explored by prompting a string prefix. Human studies further indicate that those\nLLM-sourced bottlenecks are much factual and groundable, maintaining great inherent interpretability for\nmodel designs. Besides the concept-based models, another promising strategy is to employ LLMs to enhance\nthe conventional architectures that are inherently interpretable, such as GAMs and Decision Trees (DTs).\nSingh et al. (2023) leverages the knowledge captured in LLMs to enhance GAMs and DTs, where LLMs are"
    },
    {
        "vector_id": 5611,
        "text": "only involved during the augmented model training instead of the inference process. For GAMs training,\nLLMs can provide decoupled embeddings for enhancement. For DTs training, LLMs are able to help generate\nimproved features for splitting. The LLM-augmented GAMs and DTs enable full transparency, where only\nthe summing coefficients and input key phrases are required for interpretation. With the extra information\nfrom LLMs, augmented GAMs and DTs are capable of achieving better generalization performance compared\nwith non-augmented ones.\n10.2 Designing Interpretable AI Workflows with LLM Agents\nTraditional deep models are usually designed in an end-to-end manner. The internal workflows are not\nquite understandable to general users. By utilizing common-sense world knowledge, LLMs can break down\ncomplex problems into smaller ones and organize the workflows among them, leading to more interpretable"
    },
    {
        "vector_id": 5612,
        "text": "design of AI systems (Feng et al., 2023). A recent example on interpretable AI workflow design comes from\nShen et al. (2024), where an LLM-powered agent leverages ChatGPT to integrate various off-the-shelf AI\nmodels (e.g., from Hugging Face (Jain, 2022)) to handle different downstream application tasks. In order\nto handle the complicated tasks in a transparent workflow, LLMs serve as a pivotal role in coordinating\nwith external models with language mediums to harness their powers. By planning the target task, selecting\ncandidate models, executing decomposed subtasks and summarizing responses, LLMs can help disassemble\ntasks based on user requests, and assign appropriate models to the tasks based on the model descriptions.\nSimilarly, to transparentize the workflow, Liu et al. (2023d) introduces a task decomposer to analyze the\nuser prompts and break it down into a number of subtasks for solving using LLMs. Each subtask is well"
    },
    {
        "vector_id": 5613,
        "text": "managed and attributed withdescription, domain, inputs, and outputs. In this way, the AI systems are\nthen capable of handling intricate user prompts with a step-by-step understandable workflow. Under the\nprompting paradigm, Khot et al. (2022) also employs LLMs to solve complex tasks by decomposition.\nDrawing inspiration from software libraries where the workflows are trackable, the decomposer and shared\nsubtasks are designed in a modular manner. One step further, Wang et al. (2024) introduces an interactive\nplanning approach for complex tasks, which enhances the error correction on initial LLM-generated plans\nby integrating plan execution descriptions and providing self-explanation of the feedback. Such interactive\nnature enables better workflow transparency in long-term planning and multi-step reasoning task scenarios.\n34 10.3 Challenges\n10.3.1 Planning Feasibility in Complicated Scenarios"
    },
    {
        "vector_id": 5614,
        "text": "Despite the task planning capability of LLMs, it is still challenging to be applied to certain scenarios in\nreal-world applications due to the feasibility issues. One typical scenario is the few-shot planning cases (Guo\net al., 2023), where acquiring large datasets for training is either impractical or cost-prohibitive thus making\nfeasible planning about unseen cases from sparse exemplars extremely challenging. To better assist the\ninterpretable designs, LLM planning needs to generalize well without extensive supervision and is expected\nto have the ability to integrate information from prior experiences as well as knowledge. Besides, another\nimportant scenario lies in the dynamic planning settings (Dagan et al., 2023), in which LLMs integrate\nfeedback from the environment iteratively, letting the agent take thinking steps or augment its context with\na reasoning trace. Dynamic scenarios urgently and frequently involve high computational costs resulting"
    },
    {
        "vector_id": 5615,
        "text": "from the iterated invocations of LLMs, and still face challenges in dealing with the limits of the context\nwindow and recovering from hallucinations on planning.\n10.3.2 Assistance Reliability with Knowledge Gaps\nLLMs exhibit remarkable proficiency in encapsulating real-world knowledge within their parameters, but\nthey resort to hallucinations and biases with high confidence when certain knowledge is missing or unreliable.\nAlthough a growing number of techniques has been proposed, such as retrieval augmentation (Guu et al.,\n2020), searching integration (Nakano et al., 2021) and multi-LLM collaboration (Feng et al., 2023), to expand\nLLM knowledge, such discrepancy in knowledge may perpetually exist owing to the continuously evolving\ncharacter of human understanding (Ji et al., 2023). As a result, a crucial research challenge keeps rising,\ni.e., how to effectively detect and mitigate the LLM knowledge gaps from humans when employing LLMs"
    },
    {
        "vector_id": 5616,
        "text": "for designs. We will need further research on evaluating and developing robust LLM mechanisms to address\nthe knowledge-gapping problems, with the goal of helping improve LLM reliability, reducing hallucinations\nand mitigating biases. Furthermore, the intersections between the knowledge gaps and the safety aspects\nare also of great challenges to be solved, which may pose some security concerns especially when using LLMs\nfor downstream models or workflow designs.\n11 Emulating Humans with LLMs for XAI\nThis section discusses how LLMs can be leveraged to serve XAI by playing the role of humans. Building\nexplainable models requires two main steps where humans are in the loop: (1) collecting a dataset with\nhuman-annotated rationales to train the models; (2) collecting human feedback on the quality of explanations\nproduced by the models for evaluation. The significant cost and time required for human involvement raise"
    },
    {
        "vector_id": 5617,
        "text": "the main challenge in scaling up this procedure. LLMs emerge as a promising solution to this challenge,\nthanks to their capability to emulate human reasoning and produce responses that closely resemble human-\ngenerated content. In the following, we introduce the methods that demonstrate LLMs\u2019 ability to generate\nhuman-like annotations and feedback, contributing to the creation of explainable models.\n11.1 Emulating Human Annotators for Training Explainable Models\nIncorporating human-understandable rationales into model development has shown its effectiveness in en-\nhancing both the transparency and performance of the system for various NLP tasks, such as question\nanswering (Li et al., 2018; Wu et al., 2020a), sentiment analysis (Du et al., 2019b; Antognini & Faltings,\n2021), and common sense reasoning (Rajani et al., 2019; Camburu et al., 2021). We use the termrationalesto\ndescribe supportive evidence that justifies the connection between inputs and outputs (Gurrapu et al., 2023)."
    },
    {
        "vector_id": 5618,
        "text": "Traditionally, the rationales are collected by leveraging human annotations (Camburu et al., 2018; Wang\net al., 2019) or applying expert-designed rules (Alhindi et al., 2018; Li et al., 2018), resulting in expensive\ncosts or limited quality. Recently, researchers in automatic annotation (Ding et al., 2022; Belal et al., 2023;\nGilardi et al., 2023) have begun to explore the potential of leveraging advanced LLMs to emulate human\nannotators in annotating the target labels of task-specific examples. These studies found that advanced\nLLMs show comparable annotation qualities against average crowd human annotators on most tasks with a\n35 lower cost, pointing out the scalability of using machine-emulated annotators. Inspired by these works, some\nstudies (Huang et al., 2023b;a) attempt to leverage advanced LLMs to collect rationales by applying the\nchain-of-thought technique. Specifically, researchers provide several input-rationale-output demonstrations"
    },
    {
        "vector_id": 5619,
        "text": "within the input text to prompt the LLMs to generate rationale and output for an unlabeled input instance.\nThe quality of such annotated rationales largely relies on the in-context learning capabilities of LLMs, lead-\ning to uncontrollable annotation quality on uncommon tasks. Other scholars (Yao et al., 2023a; Chen et al.,\n2023b; Luo et al., 2023) propose a human-in-the-loop LLM-based annotation framework based on theactive-\nlearning architecture. This framework initially collects a small seed dataset with human-annotated rationales\nand labels. This seed dataset is used to train an explainable classifier for this downstream task. Then, each\nunlabeled sample is passed through the trained explainable classifier. This is followed by a selection strategy\nthat chooses representative samples according to metrics such as explanation plausibility, prediction uncer-\ntainty, and sample diversity. Finally, LLMs are leveraged to annotate the rationales and labels of these"
    },
    {
        "vector_id": 5620,
        "text": "selected unlabeled samples. This procedure could be repeated multiple times, and the trained explainable\nclassifier from the latest time is the final output of this framework. Compared with other methods, this\napproach balances the annotation quality and the cost budget in developing explainable models by using\nLLM-emulated annotators.\n11.2 Emulating Human Feedback for Evaluating Explainable Models\nThe explanations generated by the explainable models could be classified into two categories: extractive and\nabstractive (Gurrapu et al., 2023). Extractive explanations derive directly from the input data, exemplified\nby attribution-based methods that emphasize specific segments of the input text. In contrast, abstractive\nexplanations are generated in a free-form text manner, such as chain-of-thought (CoT) responses (Wei et al.,\n2022), offering a more nuanced interpretation. The quality of extractive explanations is typically assessed"
    },
    {
        "vector_id": 5621,
        "text": "through their agreement with annotated rationales (DeYoung et al., 2020), such as accuracy, recall, and\nprecision. However, evaluating abstractive explanations presents a significant challenge, as it is impractical\nto exhaustive all reasonable abstractive results comprehensively. To automatically assess abstractive expla-\nnations, early studies first collect some free-text rationales, and then apply LLMs to estimate the similarity\nbetween the explanation and the rationales (Cheng et al., 2023; Li et al., 2023b). A higher similarity between\nthe abstraction explanation and the annotated rationales indicates a more transparent model. Recently, some\nresearchers directly use LLMs to check the rationality of the model explanations without referring to human-\nannotated rationales (Miao et al., 2023; Bills et al., 2023), emphasizing the potential of emulating human\nfeedback with advanced LLMs.\n11.3 Challenges\n11.3.1 Uncontrollable Credibility of Emulation"
    },
    {
        "vector_id": 5622,
        "text": "11.3.1 Uncontrollable Credibility of Emulation\nWhile LLMs can assist in rationale collection and explanation evaluation, their behaviors of collected results\nmay not always match human annotators, primarily due to hallucinated responses in their unfamiliar do-\nmains (Ji et al., 2023). This issue leads to unreliable annotations or feedback, as LLMs confidently generate\nfactually incorrect conclusions. The quality of data gathered from this process is compromised, impacting\nthe development of XAI systems. To improve the quality of annotations and feedback, future research could\nfocus on incorporating hallucination detection (Dhuliawala et al., 2023) and retrieval augmented genera-\ntion (Ren et al., 2023) techniques. These methods could enhance the reliability of LLM outputs, making\nthem more comparable to human-generated content in the context of XAI development.\n11.3.2 Ethical Considerations in LLM Annotation"
    },
    {
        "vector_id": 5623,
        "text": "11.3.2 Ethical Considerations in LLM Annotation\nWhen LLM annotators keep human annotators away from subjective scenarios, such as hate speech de-\ntection (Huang et al., 2023b), LLMs also have a chance to inject unethical opinions into their annotated\ndatasets. Although most advanced LLMs are fine-tuned to align with human values (Ouyang et al., 2022),\nsuch as being helpful, honest, and harmless, many studies have shown that this protection mechanism can\nbe jailbroken (Wei et al., 2023; Zou et al., 2023b), causing the model to produce values-violating answers.\nEnsuring LLM annotators follow ethical guidelines is worth further exploration.\n36 12 Discussion and Conclusion\nXAI research is undergoing a significant transformation and experiencing rapid expansion in the era of large\nmodels. In previous sections, we have introduced XAI methodologies with an emphasis on their usability."
    },
    {
        "vector_id": 5624,
        "text": "In this final section, we provide a high-level overview of the overarching challenges that persist in the field\nand suggest directions for future endeavors.\n\u2022 Circumvent the interpretability-accuracy tradeoff.The advent of modern LLMs (e.g., ChatGPT)\nhas a significant impact on this tradeoff. Traditionally, in many applications, people are willing to sacrifice\na certain degree of performance for better transparency. A corresponding XAI strategy is to train and\ndeploy an inherently interpretable model that mimics the black-box model (Che et al., 2016). However,\napplying this strategy to LLMs presents a challenge due to the difficulty in identifying an interpretable\nmodel that can match the performance levels of LLMs. This requires the creation of XAI strategies that\ncan circumvent this tradeoff, where enhanced interpretability can contribute to improved accuracy. This\nis consistent with the goal of Usable XAI discussed in this paper."
    },
    {
        "vector_id": 5625,
        "text": "\u2022 Data-driven AI vs. XAI.Data-driven AI refers to developing AI models that operate based on large\nvolumes of training data. This approach often leads to \u201cblack-box\u201d models, as it emphasizes results over\nthe clarity of decision-making pathways. Currently, the development of XAI techniques lags behind the\nadvancement of LLMs because the latter easily scale up with data-driven methods \u2013 they ingest gigantic\namounts of texts from the Internet to train. However, we believe that XAI might still catch up because\nof several opportunities. (1) We may run out of data.It was predicted that \u201cwe will have exhausted\nthe stock of high-quality language data before 2026\u201d*. Should the accumulation of more data cease to\nyield substantial improvements, the focus might shift towards enhancing model interpretability to leverage\nexisting data more effectively.(2) The model is relatively stable.As the Transformer architecture of LLMs"
    },
    {
        "vector_id": 5626,
        "text": "is pretty mature and stable, it will attract more attention to interpret their inner workings.(3) Leveraging\nLLMs for XAI.The advancement of XAI research can be accelerated if it can properly use the knowledge\nand human-like capabilities of LLMs.\n\u2022 The objective matters for explanation. During the transition from the era of classical machine\nlearning (when SVMs and decision trees dominate) to the deep learning era (when convolutional and\nrecurrent neural networks became popular), XAI techniques put a strong emphasis on achieving complete\ntransparency within models, as if \u201canything less than fully transparent is not transparent\u201d. However,\nas LLMs begin to match or even exceed human capabilities across various tasks, the importance of\ncertain XAI problems shifts. For example, when Recurrent Neural Networks (RNNs) were widely adopted\nfor text generation, we are interested in how the output islinguistically derived because RNNs often"
    },
    {
        "vector_id": 5627,
        "text": "produce nonsensical sentences. Nowadays we are less interested in this for LLMs, as they are proficient\nin generating coherent text. Nevertheless, our focus may shift to explaining how LLMs construct output\nwith factual information, as LLMs are prone to producing hallucinations. Similar observations exist in\nhuman cognition, which can be categorized into system-1 and system-2 styles: system-1 handles intuitive\nand unconscious tasks that are less explainable, while system-2 encompasses logical thinking, planning,\nand reasoning (Goyal & Bengio, 2022). Given the vast scale and complexity of LLMs, achieving absolute\ntransparency across all aspects of these models appears increasingly unfeasible in the immediate future.\nThus, prioritizing meaningful and feasible objectives of explanation, customized for specific tasks, becomes\nessential in enhancing the utility of AI systems in practical applications."
    },
    {
        "vector_id": 5628,
        "text": "\u2022 Evaluation remains challenging for XAI in LLMs.Traditional XAI has developed a comprehensive\ntaxonomy of explanation problems and formats, accompanied by clear definitions for each category (Han\net al., 2022; Doshi-Velez & Kim, 2017; Rudin et al., 2022). However, the established taxonomy cannot\nbe simply grafted into the study of LLM because of two reasons. First, certain XAI challenges lose their\nprominence in the context of LLMs, while some approaches become too complex for practical application.\nSecond, whileXAIisbecomingacommonpathwayofsolvingproblemsforLLMs, theexplorationofLLMs\u2019\ninner mechanics has branched into various directions. For instance, there has been a notable trend towards\nleveraging insights from human behavior and limitations to interpret LLMs, such as whether LLMs can\nlie (Azaria & Mitchell, 2023), can LLMs keep secrets (Mireshghallah et al., 2023), the impact of politeness\n*https://www.livemint.com/mint-top-newsletter/techtalk20102023.html"
    },
    {
        "vector_id": 5629,
        "text": "37 in prompts on LLMs (Yin et al., 2024b), and even how they can be \"hypnotized\" (Li et al., 2023e). These\ndiverse approaches have not converged to unified methodologies in interpreting LLM behaviors, which\nmakes it challenging for evaluation. A potential risk is the resultant explanations might give users a\nfalse sense that they accurately understand the model, especially when users attempt to shoehorn certain\nhuman knowledge or concepts to explain LLMs (Schut et al., 2023).\nConclusion. In this paper, we hope to guide readers through a crucial yet frequently underappreciated\naspect of Explainable AI (XAI) \u2013usability. To this end, we present 10 strategies for advancing Usable XAI\nwithin the LLM paradigm, including (1) leveraging explanations to reciprocally enhance LLMs and general\nAI systems, and (2) enriching XAI approaches by integrating LLM capabilities. Unlocking the potential of"
    },
    {
        "vector_id": 5630,
        "text": "XAI\u2019s usability can help address various challenges in LLM such as human alignment. We also provide case\nstudies to several critical topics, aiming to provide resources for interested developers. We further discuss\nopen challenges at the end of each strategy, suggesting directions for future work in this evolving area.\nAcknowledgement\nThe work is, in part, supported by NSF (#IIS-2223768, #IIS-2223769, #IIS-2310261, #DRL-2101104). The\nviews and conclusions in this paper are those of the authors and should not be interpreted as representing\nany funding agencies.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023.\nJulius Adebayo, Melissa Hall, Bowen Yu, and Bobbie Chern. Quantifying and mitigating the impact of label\nerrors on model disparity metrics.arXiv preprint arXiv:2310.02533, 2023."
    },
    {
        "vector_id": 5631,
        "text": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluat-\ning correctness and faithfulness of instruction-following models for question answering.arXiv preprint\narXiv:2307.16877, 2023.\nGustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin L Edelman. Distinguishing the knowable\nfrom the unknowable with language models.arXiv preprint arXiv:2402.03563, 2024.\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\nTowards tracing knowledge in language models back to the training data. InFindings of EMNLP, pp.\n2429\u20132446, December 2022.\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. Where is your evidence: Improving fact-checking by\njustification modeling. InProceedings of the first workshop on fact extraction and verification (FEVER),\npp. 85\u201390, 2018.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,"
    },
    {
        "vector_id": 5632,
        "text": "M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon\nseries of open language models.arXiv preprint arXiv:2311.16867, 2023.\nDiego Matteo Antognini and Boi Faltings. Rationalization through concepts.Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, pp. 761\u2013775, 2021.\nWalter Edwin Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.\nQuarterly of applied mathematics, 9(1):17\u201329, 1951.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of\nword senses, with applications to polysemy.Transactions of the Association for Computational Linguistics,\n6:483\u2013495, 2018.\n38 Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection.arXiv preprint arXiv:2310.11511, 2023."
    },
    {
        "vector_id": 5633,
        "text": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint\narXiv:2304.13734, 2023.\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang, Jing Bai, Jing Yu, and Yunhai Tong. Syntax-bert:\nImproving pre-trained transformers with syntax trees.arXiv preprint arXiv:2103.04350, 2021.\nRandall Balestriero, Romain Cosentino, and Sarath Shekkizhar. Characterizing large language model geom-\netry solves toxicity detection and generation.arXiv preprint arXiv:2312.01648, 2023.\nDavid Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying\ninterpretability of deep visual representations. InProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 6541\u20136549, 2017.\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and\nAntonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In"
    },
    {
        "vector_id": 5634,
        "text": "International Conference on Learning Representations, 2018.\nMohammad Belal, James She, and Simon Wong. Leveraging chatgpt as text annotation tool for sentiment\nanalysis. arXiv preprint arXiv:2306.17177, 2023.\nYonatan Belinkov, Llu\u00eds M\u00e0rquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. Evaluating\nlayers of representation in neural machine translation on part-of-speech and semantic tagging tasks.arXiv\npreprint arXiv:1801.07772, 2018.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving\nelaborate problems with large language models.arXiv preprint arXiv:2308.09687, 2023.\nAmrita Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. Llms as counterfactual explanation\nmodules: Can chatgpt explain black-box text classifiers?arXiv preprint arXiv:2309.13340, 2023."
    },
    {
        "vector_id": 5635,
        "text": "Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.URL\nhttps://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05.\n2023), 2023.\nHolger Boche, Adalbert Fono, and Gitta Kutyniok. Mathematical algorithm design for deep learning\nunder societal and judicial constraints: The algorithmic transparency requirement. arXiv preprint\narXiv:2401.10310, 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206\u20132240. PMLR, 2022.\nTrenton Bricken and Cengiz Pehlevan. Attention approximates sparse distributed memory.Advances in"
    },
    {
        "vector_id": 5636,
        "text": "Neural Information Processing Systems, 34:15301\u201315315, 2021.\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,\nCem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language\nmodels with dictionary learning. transformer circuits thread, 2023, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n39 Bodhisattwa Prasad Majumder1 Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.\nRationale-inspired natural language explanations with commonsense.arXiv preprint arXiv:2106.13876,\n2021.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language\ninference with natural language explanations. Advances in Neural Information Processing Systems, 31,"
    },
    {
        "vector_id": 5637,
        "text": "2018.\nJames Campbell, Richard Ren, and Phillip Guo. Localizing lying in llama: Understanding instructed dishon-\nesty on true-false questions through prompting, probing, and patching.arXiv preprint arXiv:2311.15131,\n2023.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. What to learn, and how: Toward effective learning from\nrationales. arXiv preprint arXiv:2112.00071, 2021.\nWilliam Castillo-Gonz\u00e1lez, Carlos Oscar Lepez, and Mabel Cecilia Bonardi. Chat gpt: a promising tool for\nacademic editing. Data Metadata, 1:23, 2022.\nArjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. Do\nexplanations make vqa models more predictable to a human? InProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pp. 1036\u20131042, 2018.\nZhengping Che, Sanjay Purushotham, Robinder Khemani, and Yan Liu. Interpretable deep models for icu\noutcome prediction. In AMIA annual symposium proceedings, volume 2016, pp. 371. American Medical"
    },
    {
        "vector_id": 5638,
        "text": "Informatics Association, 2016.\nChao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms\u2019\ninternal states retain the power of hallucination detection. InThe Twelfth International Conference on\nLearning Representations, 2023a.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuning\ndesign spaces. InThe Eleventh International Conference on Learning Representations, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code.arXiv preprint arXiv:2107.03374, 2021.\nWei-Lin Chen, An-Zi Yen, Hen-Hsen Huang, Cheng-Kuang Wu, and Hsin-Hsi Chen. Zara: Improving few-\nshot self-rationalization for small language models.arXiv preprint arXiv:2305.07355, 2023b."
    },
    {
        "vector_id": 5639,
        "text": "Yufan Chen, Arjun Arunasalam, and Z Berkay Celik. Can large language models provide security & privacy\nadvice? measuring the ability of llms to refute misconceptions. InProceedings of the 39th Annual Computer\nSecurity Applications Conference, pp. 366\u2013378, 2023c.\nHao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, and Hao Liao. Explainable\nrecommendation with personalized review retrieval and aspect learning.arXiv preprint arXiv:2306.12657,\n2023.\nWei-LinChiang, ZhuohanLi, ZiLin, YingSheng, ZhanghaoWu, HaoZhang, LianminZheng, SiyuanZhuang,\nYonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality.See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\nMunmun De Choudhury, Sachin R. Pendse, and Neha Kumar. Benefits and harms of large language models\nin digital mental health, 2023.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. Chatlaw: Open-source legal large language"
    },
    {
        "vector_id": 5640,
        "text": "model with integrated external knowledge bases.arXiv preprint arXiv:2306.16092, 2023.\nZeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. M6-rec: Generative pretrained\nlanguage models are open-ended recommender systems.arXiv preprint arXiv:2205.08084, 2022.\n40 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find\nhighly interpretable features in language models.arXiv preprint arXiv:2309.08600, 2023.\nGautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a llm. arXiv preprint\narXiv:2308.06391, 2023.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained\ntransformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493\u20138502, 2022.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,"
    },
    {
        "vector_id": 5641,
        "text": "Sheng Li, Dajiang Zhu, et al. Chataug: Leveraging chatgpt for text data augmentation.arXiv preprint\narXiv:2302.13007, 2023.\nZhuyun Daiand Jamie Callan. Deeper text understanding forir with contextualneural languagemodeling. In\nProceedings of the 42nd international ACM SIGIR conference on research and development in information\nretrieval, pp. 985\u2013988, 2019.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained\ntransformer models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 4908\u20134926, 2020.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey\nof the state of explainable ai for natural language processing. InProceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pp. 447\u2013459, 2020."
    },
    {
        "vector_id": 5642,
        "text": "Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In\nAnnual Meeting of the Association for Computational Linguistics, 2023.\nRicheek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. Improving pretraining techniques for code-\nswitched nlp. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1176\u20131191, 2023.\nEmily L Denton et al. Unsupervised learning of disentangled representations from video.Advances in neural\ninformation processing systems, 30, 2017.\nErik Derner, Kristina Batisti\u010d, Jan Zah\u00e1lka, and Robert Babu\u0161ka. A security risk taxonomy for large\nlanguage models. arXiv preprint arXiv:2311.11415, 2023.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and By-\nronCWallace. Eraser: Abenchmarktoevaluaterationalizednlpmodels. arXiv preprint arXiv:1911.03429,\n2019."
    },
    {
        "vector_id": 5643,
        "text": "2019.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp. 4443\u20134458, 2020.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint\narXiv:2309.11495, 2023.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3\na good data annotator?arXiv preprint arXiv:2212.10450, 2022.\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\ngeneration. arXiv preprint arXiv:2311.04254, 2023."
    },
    {
        "vector_id": 5644,
        "text": "41 Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.arXiv\npreprint arXiv:1702.08608, 2017.\nFilip Karlo Do\u0161ilovi\u0107, Mario Br\u010di\u0107, and Nikica Hlupi\u0107. Explainable artificial intelligence: A survey. In2018\n41st International convention on information and communication technology, electronics and microelec-\ntronics (MIPRO), pp. 0210\u20130215. IEEE, 2018.\nMengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning.Communications of\nthe ACM, 63(1):68\u201377, 2019a.\nMengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. Learning credible deep neural networks with rationale\nregularization. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 150\u2013159. IEEE,\n2019b.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models.Pro-"
    },
    {
        "vector_id": 5645,
        "text": "ceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021.\nHanyu Duan, Yi Yang, and Kar Yan Tam. Do llms know about hallucination? an empirical investigation of\nllm\u2019s hidden states.arXiv preprint arXiv:2402.09733, 2024.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text\nclassification. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pp. 31\u201336, 2018.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits.Trans-\nformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac\nHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition.arXiv"
    },
    {
        "vector_id": 5646,
        "text": "preprint arXiv:2209.10652, 2022.\nJoseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language\nmodels. arXiv preprint arXiv:2305.15853, 2023.\nShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Knowl-\nedge card: Filling llms\u2019 knowledge gaps with plug-in specialized language models. InThe Twelfth Inter-\nnational Conference on Learning Representations, 2023.\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt,\nTong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey.\narXiv preprint arXiv:2309.00770, 2023.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint\narXiv:2312.10997, 2023.\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias"
    },
    {
        "vector_id": 5647,
        "text": "Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-\ngence, 2(11):665\u2013673, 2020.\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. Recommendation as language\nprocessing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). InProceedings of the\n16th ACM Conference on Recommender Systems, pp. 299\u2013315, 2022.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value\nmemories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 5484\u20135495, 2021.\n42 Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based expla-\nnations. Advances in neural information processing systems, 32, 2019.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023."
    },
    {
        "vector_id": 5648,
        "text": "tasks. arXiv preprint arXiv:2303.15056, 2023.\nAnirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.Proceedings\nof the Royal Society A, 478(2266):20210068, 2022.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,\nDustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023.\nHanGuo, NazneenRajani, PeterHase, MohitBansal, andCaimingXiong. Fastif: Scalableinfluencefunctions\nfor efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 10333\u201310350, 2021.\nQing Guo, Prashan Wanigasekara, Skyler Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao.\nHow do multimodal llms really fare in classical vision few-shot challenges? a deep dive. 2023."
    },
    {
        "vector_id": 5649,
        "text": "Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, and Feras A Batarseh. Rationalization for\nexplainable nlp: A survey.Frontiers in Artificial Intelligence, 6, 2023.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language\nmodel pre-training. InInternational conference on machine learning, pp. 3929\u20133938. PMLR, 2020.\nTessaHan, SurajSrinivas, andHimabinduLakkaraju. Whichexplanationshouldichoose? afunctionapprox-\nimation perspective to characterizing post hoc explanations.Advances in Neural Information Processing\nSystems, 35:5256\u20135268, 2022.\nXiaochuang Han and Yulia Tsvetkov. Orca: Interpreting prompted language models via locating supporting\ndata evidence in the ocean of pretraining data.arXiv preprint arXiv:2205.12600, 2022.\nXiaochuang Han, Byron C Wallace, and Yulia Tsvetkov. Explaining black box predictions and unveiling\ndata artifacts through influence functions. InProceedings of the 58th Annual Meeting of the Association"
    },
    {
        "vector_id": 5650,
        "text": "for Computational Linguistics, pp. 5553\u20135563, 2020.\nXiaoqi Han, Ru Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan. Improving sequential model\nediting with fact retrieval. InFindings of the Association for Computational Linguistics: EMNLP 2023,\npp. 11209\u201311224, 2023.\nPeter Hase and Mohit Bansal. Evaluating explainable ai: Which algorithmic explanations help users pre-\ndict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 5540\u20135552, 2020.\nPeterHase, MohitBansal, BeenKim, andAsmaGhandeharioun. Doeslocalizationinformediting? surprising\ndifferences in causality-based localization vs. knowledge editing in language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016."
    },
    {
        "vector_id": 5651,
        "text": "Ruining He, Wang-Cheng Kang, and Julian McAuley. Translation-based recommendation. InProceedings of\nthe eleventh ACM conference on recommender systems, pp. 161\u2013169, 2017.\nZexue He, Marco Tulio Ribeiro, and Fereshte Khani. Targeted data generation: Finding and fixing model\nweaknesses. arXiv preprint arXiv:2305.17804, 2023.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector\nmachines. IEEE Intelligent Systems and their applications, 13(4):18\u201328, 1998.\n43 Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, andAlexanderLerchner. beta-vae: Learningbasicvisualconceptswithaconstrainedvariational\nframework. InInternational conference on learning representations, 2016.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071, 2022."
    },
    {
        "vector_id": 5652,
        "text": "preprint arXiv:2212.10071, 2022.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: A visual analysis tool to explore\nlearnedrepresentationsintransformermodels. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 187\u2013196, 2020.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ran-\njay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language\nmodels with less training data and smaller model sizes. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, pp. 8003\u20138017. Association for Computational Linguistics, July 2023. doi:\n10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/2023.findings-acl.507.\nFan Huang, Haewoon Kwak, and Jisun An. Chain of explanation: New prompting method to generate\nquality natural language explanation for implicit hate speech. InCompanion Proceedings of the ACM Web"
    },
    {
        "vector_id": 5653,
        "text": "Conference 2023, pp. 90\u201393, 2023a.\nFan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate speech. InCompanion Proceedings of the ACM Web\nConference 2023, pp. 294\u2013297, 2023b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large\nlanguage models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.arXiv preprint arXiv:2311.05232, 2023c.\nShiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, and Leilani H Gilpin. Can\nlarge language models explain themselves? a study of llm-generated self-explanations. arXiv preprint\narXiv:2310.11207, 2023d."
    },
    {
        "vector_id": 5654,
        "text": "arXiv:2310.11207, 2023d.\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering.arXiv preprint arXiv:2007.01282, 2020.\nSamyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette,\nTim Rockt\u00e4schel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-\ndurally defined tasks.arXiv preprint arXiv:2311.12786, 2023.\nShashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face\nLibrary and Models to Solve Problems, pp. 51\u201367. Springer, 2022.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. What does bert learn about the structure of language?\nIn ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation.ACM Computing"
    },
    {
        "vector_id": 5655,
        "text": "Surveys, 55(12):1\u201338, 2023.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.\narXiv preprint arXiv:2310.06825, 2023.\nPrzemyslaw Joniak and Akiko Aizawa. Gender biases and where to find them: Exploring gender bias in pre-\ntrained transformer-based language models using movement pruning.arXiv preprint arXiv:2207.02463,\n2022.\n44 Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models\nstruggle to learn long-tail knowledge. InInternational Conference on Machine Learning, pp. 15696\u201315707.\nPMLR, 2023.\nVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906, 2020."
    },
    {
        "vector_id": 5656,
        "text": "arXiv:2004.04906, 2020.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond\nthe surface: A challenge set for reading comprehension over multiple sentences. InProceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 252\u2013262, 2018.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. Decomposedprompting: Amodularapproachforsolvingcomplextasks. In The Eleventh International\nConference on Learning Representations, 2022.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and\nJinwoo Shin. Sure: Improving open-domain question answering of llms via summarized retrieval. InThe\nTwelfth International Conference on Learning Representations, 2023a.\nSeungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo."
    },
    {
        "vector_id": 5657,
        "text": "The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought\nfine-tuning. arXiv preprint arXiv:2305.14045, 2023b.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch\u00fctt, Sven D\u00e4hne,\nDumitru Erhan, and Been Kim. The (un) reliability of saliency methods. InExplainable AI: Interpreting,\nExplaining and Visualizing Deep Learning, pp. 267\u2013280.\nPangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. In International conference on machine learning, pp. 5338\u20135348.\nPMLR, 2020.\nEnja Kokalj, Bla\u017e \u0160krlj, Nada Lavra\u010d, Senja Pollak, and Marko Robnik-\u0160ikonja. Bert meets shapley: Ex-\ntending shap explanations to transformer-based classifiers. In Proceedings of the EACL Hackashop on"
    },
    {
        "vector_id": 5658,
        "text": "News Media Content Analysis and Automated Report Generation, pp. 16\u201321, 2021.\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.\nPost hoc explanations of language models can improve language models.arXiv preprint arXiv:2305.11426,\n2023.\nNicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Are large\nlanguage models post hoc explainers?arXiv preprint arXiv:2310.05797, 2023.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,\nDustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-\nthought reasoning.arXiv preprint arXiv:2307.13702, 2023.\nEhsan Latif and Xiaoming Zhai. Fine-tuning chatgpt for automatic scoring. Computers and Education:\nArtificial Intelligence, pp. 100210, 2024.\nAndrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K Kummerfeld, and Rada Mihalcea."
    },
    {
        "vector_id": 5659,
        "text": "A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity.arXiv preprint\narXiv:2401.01967, 2024.\n45 Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf, Matthew Nyaaba, Shuchen Guo,\nZihao Wu, Zhengliang Liu, Hui Wang, et al. Multimodality of ai for education: Towards artificial general\nintelligence. arXiv preprint arXiv:2312.06037, 2023.\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. Recexplainer: Aligning large\nlanguage models for recommendation model interpretability.arXiv preprint arXiv:2311.10947, 2023.\nChak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, and Wenjie Li. Self-detoxifying language models via\ntoxification reversal.arXiv preprint arXiv:2310.09573, 2023.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-"
    },
    {
        "vector_id": 5660,
        "text": "intensive nlp tasks.Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and San-\njiv Kumar. Large language models with controllable working memory.arXiv preprint arXiv:2211.05110,\n2022a.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nJiacheng Li, Zhankui He, Jingbo Shang, and Julian McAuley. Ucepic: Unifying aspect planning and lexical\nconstraints for generating explanations in recommendation. InProceedings of the 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining, pp. 1248\u20131257, 2023b.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\nInProceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 681\u2013691, 2016a."
    },
    {
        "vector_id": 5661,
        "text": "Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure.\narXiv preprint arXiv:1612.08220, 2016b.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale\nhallucination evaluation benchmark for large language models. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pp. 6449\u20136464, 2023c.\nJunyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The\ndawn after the dark: An empirical study on factuality hallucination in large language models. arXiv\npreprint arXiv:2401.03205, 2024a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model.Advances in Neural Information Processing\nSystems, 36, 2024b.\nLei Li, Yongfeng Zhang, and Li Chen. Generate neural template explanations for recommendation. In"
    },
    {
        "vector_id": 5662,
        "text": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pp.\n755\u2013764, 2020.\nLei Li, Yongfeng Zhang, and Li Chen. Personalized prompt learning for explainable recommendation.ACM\nTransactions on Information Systems, 41(4):1\u201326, 2023d.\nQing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. Vqa-e: Explaining, elaborating, and enhancing\nyouranswersforvisualquestions. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 552\u2013567, 2018.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\nPeng, Yi Mao, et al. Explanations from large language models make small reasoners better.arXiv preprint\narXiv:2210.06726, 2022b.\n46 Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora\u2019s box of llms: Jailbreaking llms\nthrough representation engineering.arXiv preprint arXiv:2401.06824, 2024c."
    },
    {
        "vector_id": 5663,
        "text": "Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. Faithfulness in natural lan-\nguage generation: A systematic survey of analysis, evaluation and optimization methods.arXiv preprint\narXiv:2203.05227, 2022c.\nXuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize\nlarge language model to be jailbreaker.arXiv preprint arXiv:2311.03191, 2023e.\nYingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. A survey on fairness in large language\nmodels. arXiv preprint arXiv:2308.10149, 2023f.\nYingji Li, Mengnan Du, Xin Wang, and Ying Wang. Prompt tuning pushes farther, contrastive learning\npulls closer: A two-stage approach to mitigate social biases. InProceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 14254\u201314267, 2023g.\nTom Lieberum, Matthew Rahtz, J\u00e1nos Kram\u00e1r, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does"
    },
    {
        "vector_id": 5664,
        "text": "circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv\npreprint arXiv:2307.09458, 2023.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David S\u00e1nchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation models for\ntext data: State of the art, challenges and future directions. InProceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pp. 4188\u20134203, 2021.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts.Transactions of the Association for\nComputational Linguistics, 12:157\u2013173, 2024.\nNinghao Liu, Hongxia Yang, and Xia Hu. Adversarial detection with model interpretation. InProceedings of\nthe 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1803\u20131811,\n2018."
    },
    {
        "vector_id": 5665,
        "text": "2018.\nNinghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, and Xia Hu. Adversarial attacks and defenses: An\ninterpretation perspective. ACM SIGKDD Explorations Newsletter, 23(1):86\u201399, 2021.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\nComputing Surveys, 55(9):1\u201335, 2023a.\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov,\nMuhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large\nlanguage models\u2019 alignment.arXiv preprint arXiv:2308.05374, 2023b.\nYi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang,\nand Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint\narXiv:2305.13860, 2023c.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen,"
    },
    {
        "vector_id": 5666,
        "text": "Yu Qiao, Jifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs.\narXiv preprint arXiv:2310.17796, 2023d.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2018.\nYin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classification and regression. In\nProceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 150\u2013158, 2012.\n47 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022.\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural\ninformation processing systems, 30, 2017."
    },
    {
        "vector_id": 5667,
        "text": "information processing systems, 30, 2017.\nHaoyan Luo and Lucia Specia. From understanding to utilization: A survey on explainability for large\nlanguage models. arXiv preprint arXiv:2401.12874, 2024.\nYun Luo, Zhen Yang, Fandong Meng, Yingjie Li, Fang Guo, Qinglin Qi, Jie Zhou, and Yue Zhang. Xal:\nExplainableactivelearningmakesclassifiersbetterlow-resourcelearners. arXiv preprint arXiv:2310.05502,\n2023.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris\nCallison-Burch. Faithful chain-of-thought reasoning.arXiv preprint arXiv:2301.13379, 2023.\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun,\nAndrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. Deciphering stereotypes in pre-\ntrained language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 11328\u201311345, 2023."
    },
    {
        "vector_id": 5668,
        "text": "Processing, pp. 11328\u201311345, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching\nsmall language models to reason.arXiv preprint arXiv:2212.08410, 2022.\nAlessio Malizia and Fabio Patern\u00f2. Why is the current xai not meeting the expectations?Communications\nof the ACM, 66(12):20\u201323, 2023.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When\nnot to trust language models: Investigating effectiveness of parametric and non-parametric memories. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 9802\u20139822, 2023.\nSamuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of true/false datasets.arXiv preprint arXiv:2310.06824, 2023.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations"
    },
    {
        "vector_id": 5669,
        "text": "in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022a.\nKevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory\nin a transformer. InThe Eleventh International Conference on Learning Representations, 2022b.\nSachit Menon and Carl Vondrick. Visual classification via description from large language models. InThe\nEleventh International Conference on Learning Representations, 2022.\nNing Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-\nstep reasoning. arXiv preprint arXiv:2308.00436, 2023.\nNiloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, and Yejin\nChoi. Can llms keep a secret? testing privacy implications of language models via contextual integrity\ntheory. InThe Twelfth International Conference on Learning Representations, 2023.\nHosein Mohebbi, Ali Modarressi, and Mohammad Taher Pilehvar. Exploring the role of bert token represen-"
    },
    {
        "vector_id": 5670,
        "text": "tations to explain sentence probing results. InProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 792\u2013806, 2021.\nGr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller.\nExplaining nonlinear classification decisions with deep taylor decomposition. Pattern recognition, 65:\n211\u2013222, 2017.\n48 Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller.\nLayer-wise relevance propagation: an overview.Explainable AI: interpreting, explaining and visualizing\ndeep learning, pp. 193\u2013209, 2019.\nW James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,\nand applications in interpretable machine learning.Proceedings of the National Academy of Sciences, 116\n(44):22071\u201322080, 2019.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,"
    },
    {
        "vector_id": 5671,
        "text": "Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering\nwith human feedback.arXiv preprint arXiv:2112.09332, 2021.\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,\nChristopher A Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. Scalable extraction of\ntraining data from (production) language models.arXiv preprint arXiv:2311.17035, 2023.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.arXiv preprint\narXiv:2209.11895, 2022.\nR OpenAI. Gpt-4 technical report.arXiv, pp. 2303\u201308774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with"
    },
    {
        "vector_id": 5672,
        "text": "human feedback.Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-\nder Miller. Language models as knowledge bases? InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. How context affects language models\u2019 factual predictions.arXiv preprint arXiv:2005.04611,\n2020.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence\nby tracing gradient descent.Advances in Neural Information Processing Systems, 33:19920\u201319930, 2020.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt"
    },
    {
        "vector_id": 5673,
        "text": "a general-purpose natural language processing task solver?arXiv preprint arXiv:2302.06476, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging"
    },
    {
        "vector_id": 5674,
        "text": "language models for commonsense reasoning. InProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4932\u20134942, 2019.\nAishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, and Aman Chadha.\nFrom prejudice to parity: A new approach to debiasing large language model word embeddings.arXiv\npreprint arXiv:2402.11512, 2024.\n49 Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,\nand Haifeng Wang. Investigating the factual knowledge boundary of large language models with retrieval\naugmentation. arXiv preprint arXiv:2307.11019, 2023.\nMarcoTulioRibeiro, SameerSingh, andCarlosGuestrin. \"whyshoulditrustyou?\"explainingthepredictions\nof anyclassifier. InProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135\u20131144, 2016.\nStephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond."
    },
    {
        "vector_id": 5675,
        "text": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842\u2013866, 2021.\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature machine intelligence, 1(5):206\u2013215, 2019.\nCynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable\nmachine learning: Fundamental principles and 10 grand challenges.Statistic Surveys, 16:1\u201385, 2022.\nRachneet Sachdeva, Martin Tutek, and Iryna Gurevych. Catfood: Counterfactual augmented training for\nimproving out-of-domain performance and calibration.arXiv preprint arXiv:2309.07822, 2023.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:"
    },
    {
        "vector_id": 5676,
        "text": "smaller, faster, cheaper and lighter.arXiv preprint arXiv:1910.01108, 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems, 33:20378\u201320389, 2020.\nSoumya Sanyal and Xiang Ren. Discretized integrated gradients for explaining language models. arXiv\npreprint arXiv:2108.13654, 2021.\nAdam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and\ncapacity in neural networks.arXiv preprint arXiv:2210.01892, 2022.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and Artem Sokolov. Scaling up influence functions. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8179\u20138186, 2022.\nLisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the\nhuman-ai knowledge gap: Concept discovery and transfer in alphazero.arXiv preprint arXiv:2310.16410,\n2023."
    },
    {
        "vector_id": 5677,
        "text": "2023.\nSarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas,\nDavid Bau, and Antonio Torralba. Find: A function description benchmark for evaluating interpretability\nmethods. Advances in Neural Information Processing Systems, 36, 2024.\nRamprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and\nDhruv Batra. Grad-cam: Why did you say that?arXiv preprint arXiv:1611.07450, 2016.\nLee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.\nIn AI Alignment Forum, 2022.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face.Advances in Neural Information Processing\nSystems, 36, 2024.\nYucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, and Ninghao Liu. Mededit: Model"
    },
    {
        "vector_id": 5678,
        "text": "editing for medical question answering with external knowledge bases.arXiv preprint arXiv:2309.16035,\n2023.\n50 AvantiShrikumar, PeytonGreenside, andAnshulKundaje. Learningimportantfeaturesthroughpropagating\nactivation differences. InInternational conference on machine learning, pp. 3145\u20133153. PMLR, 2017.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces\nhallucination in conversation.arXiv preprint arXiv:2104.07567, 2021.\nSandipan Sikdar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature\ninteraction attribution for neural nlp models. InProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing\n(Volume 1: Long Papers), pp. 865\u2013878, 2021.\nChandan Singh, Armin Askari, Rich Caruana, and Jianfeng Gao. Augmenting interpretable models with"
    },
    {
        "vector_id": 5679,
        "text": "large language models during training.Nature Communications, 14(1):7913, 2023.\nShamaneSiriwardhana, RivinduWeerasekera, ElliottWen, TharinduKaluarachchi, RajibRana, andSuranga\nNanayakkara. Improving the domain adaptation of retrieval augmented generation (rag) models for open\ndomain question answering.Transactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\nSteven A Sloman. The empirical case for two systems of reasoning.Psychological bulletin, 119(1):3, 1996.\nYan-Yan Song and LU Ying. Decision tree methods: applications for classification and prediction.Shanghai\narchives of psychiatry, 27(2):130, 2015.\nJoe Stacey, Yonatan Belinkov, and Marek Rei. Supervising model attention with human explanations\nfor robust natural language inference. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pp. 11349\u201311357, 2022.\nKeith E Stanovich.Who is rational?: Studies of individual differences in reasoning. Psychology Press, 1999."
    },
    {
        "vector_id": 5680,
        "text": "Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. Api is enough: Conformal prediction for large language\nmodels without logit-access.arXiv preprint arXiv:2403.01216, 2024.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.Advances in neural\ninformation processing systems, 28, 2015.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. InInternational\nconference on machine learning, pp. 3319\u20133328. PMLR, 2017.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Can chatgpt replace\ntraditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family.\nIn International Semantic Web Conference, pp. 348\u2013367. Springer, 2023.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. Does synthetic data generation of llms help\nclinical text mining?arXiv preprint arXiv:2303.04360, 2023a."
    },
    {
        "vector_id": 5681,
        "text": "Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-\nthen-translate: an analysis on improving program translation with self-generated explanations. arXiv\npreprint arXiv:2311.07070, 2023b.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing\nfor sentence structure in contextualized word representations. InInternational Conference on Learning\nRepresentations, 2018.\nHimanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. Lan-\nguage models get a gender makeover: Mitigating gender bias with few-shot data interventions.arXiv\npreprint arXiv:2306.04597, 2023.\nErico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai.IEEE\ntransactions on neural networks and learning systems, 32(11):4793\u20134813, 2020."
    },
    {
        "vector_id": 5682,
        "text": "51 Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function\nvectors in large language models.arXiv preprint arXiv:2310.15213, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say what\nthey think: unfaithful explanations in chain-of-thought prompting.Advances in Neural Information Pro-\ncessing Systems, 36, 2024."
    },
    {
        "vector_id": 5683,
        "text": "cessing Systems, 36, 2024.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine:\nDetecting and mitigating hallucinations of llms by validating low-confidence generation.arXiv preprint\narXiv:2307.03987, 2023.\nJesse Vig. Bertviz: A tool for visualizing multihead self-attention in the bert model. InICLR workshop:\nDebugging machine learning models, volume 23, 2019.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-\nattention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pp. 5797\u20135808, 2019.\nElena Voita, Rico Sennrich, and Ivan Titov. Analyzing the source and target contributions to predictions in\nneural machine translation.arXiv preprint arXiv:2010.10907, 2020.\nSandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the"
    },
    {
        "vector_id": 5684,
        "text": "black box: Automated decisions and the gdpr.Harv. JL & Tech., 31:841, 2017.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. Fact or fiction: Verifying scientific claims. InProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pp. 7534\u20137550, 2020.\nwalkerspider. Dan is my new friend.https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_\nnew_friend/, 2022. [Accessed 27-02-2024].\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nFei Wang, James Y Huang, Tianyi Yan, Wenxuan Zhou, and Muhao Chen. Robust natural language\nunderstanding with residual attention debiasing.arXiv preprint arXiv:2305.17627, 2023a.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabil-"
    },
    {
        "vector_id": 5685,
        "text": "ity in the wild: a circuit for indirect object identification in gpt-2 small. InThe Eleventh International\nConference on Learning Representations, 2022a.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label\nwords are anchors: An information flow perspective for understanding in-context learning.arXiv preprint\narXiv:2305.14160, 2023b.\nQianlong Wang, Keyang Ding, Bin Liang, Min Yang, and Ruifeng Xu. Reducing spurious correlations\nin aspect-based sentiment analysis with explanation from large language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pp. 2930\u20132941, 2023c.\nSong Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large language\nmodels: A survey.arXiv preprint arXiv:2310.16218, 2023d.\n52 Weixuan Wang, Barry Haddow, and Alexandra Birch. Retrieval-augmented multilingual knowledge editing.\narXiv preprint arXiv:2312.13040, 2023e."
    },
    {
        "vector_id": 5686,
        "text": "arXiv preprint arXiv:2312.13040, 2023e.\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding skill neurons\nin pre-trained transformer-based language models. InProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 11132\u201311152, 2022b.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. arXiv preprint\narXiv:2402.10200, 2024a.\nXuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting, 2024b.\nZhao Wang and Aron Culotta. Robustness to spurious correlations in text classification via automatically\ngenerated counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npp. 14024\u201314031, 2021.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.Advances\nin Neural Information Processing Systems, 36, 2024."
    },
    {
        "vector_id": 5687,
        "text": "Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, and Anh Nguyen. Generating valid and natural adversarial\nexamples with large language models.arXiv preprint arXiv:2311.11861, 2023f.\nZiqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, and Xiang\nRen. Learning from explanations with neural execution tree. InInternational Conference on Learning\nRepresentations, 2019.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?arXiv\npreprint arXiv:2307.02483, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information\nProcessing Systems, 35:24824\u201324837, 2022.\nSean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded\nmathematical proof generation with language models.Advances in Neural Information Processing Systems,"
    },
    {
        "vector_id": 5688,
        "text": "35:4913\u20134927, 2022.\nJiaxin Wen, Yeshuang Zhu, Jinchao Zhang, Jie Zhou, and Minlie Huang. Autocad: Automatically generating\ncounterfactuals for mitigating shortcut learning.arXiv preprint arXiv:2211.16202, 2022.\nChenxiWhitehouse, MonojitChoudhury, andAlhamFikriAji. Llm-powereddataaugmentationforenhanced\ncrosslingual performance. arXiv preprint arXiv:2305.14288, 2023.\nJialin Wu, Liyan Chen, and Raymond J Mooney. Improving vqa and its explanations \\\\by comparing\ncompeting explanations. arXiv preprint arXiv:2006.15631, 2020a.\nXuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, and Dong Yu.\nFromlanguagemodelingtoinstructionfollowing: Understandingthebehaviorshiftinllmsafterinstruction\ntuning, 2023.\nZhengxuan Wu and Desmond C Ong. On explaining your explanations of bert: An empirical study with\nsequence classification. arXiv preprint arXiv:2101.00196, 2021.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. Perturbed masking: Parameter-free probing for analyzing"
    },
    {
        "vector_id": 5689,
        "text": "and interpreting bert. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 4166\u20134176, 2020b.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting\ninfluential data for targeted instruction tuning.arXiv preprint arXiv:2402.04333, 2024.\n53 Albert Xu, Xiang Ren, and Robin Jia. Contrastive novelty-augmented learning: Anticipating outliers with\nlarge language models. InProceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 11778\u201311801, 2023a.\nHan Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks\nand defenses in images, graphs and text: A review.International Journal of Automation and Computing,\n17:151\u2013178, 2020.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina"
    },
    {
        "vector_id": 5690,
        "text": "Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\nmodels. arXiv preprint arXiv:2310.03025, 2023b.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J Martindale, and Marine Carpuat. Understanding\nand detecting hallucinations in neural machine translation via model introspection.Transactions of the\nAssociation for Computational Linguistics, 11:546\u2013564, 2023c.\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large\nlanguage models. arXiv preprint arXiv:2401.11817, 2024.\nFan Yang, Mengnan Du, and Xia Hu. Evaluating explanation without ground truth in interpretable machine\nlearning. arXiv preprint arXiv:1907.06831, 2019.\nYi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. Bias a-head? analyzing bias in\ntransformer-based language model attention heads.arXiv preprint arXiv:2311.10395, 2023a."
    },
    {
        "vector_id": 5691,
        "text": "Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.\nLanguage in a bottle: Language model guided concept bottlenecks for interpretable image classification. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19187\u201319197,\n2023b.\nZhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, and\nXiangnan He. Large language model can interpret latent space of sequential recommender.arXiv preprint\narXiv:2310.20487, 2023c.\nBingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank\nSrivastava, Yunyao Li, James Hendler, et al. Beyond labels: Empowering human annotators with natural\nlanguage explanations through a novel active-learning architecture. InFindings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 11629\u201311643, 2023a."
    },
    {
        "vector_id": 5692,
        "text": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\nTreeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv preprint arXiv:2305.10601,\n2023b.\nXi Ye and Greg Durrett. Can explanations be useful for calibrating black box models? InProceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n6199\u20136212, 2022a.\nXi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning.\nAdvances in neural information processing systems, 35:30378\u201330392, 2022b.\nXunjian Yin, Xu Zhang, Jie Ruan, and Xiaojun Wan. Benchmarking knowledge boundary for large language\nmodel: A different perspective on model evaluation.arXiv preprint arXiv:2402.11493, 2024a.\nZiqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, and Satoshi Sekine. Should we respect llms? a cross-"
    },
    {
        "vector_id": 5693,
        "text": "lingual study on the influence of prompt politeness on llm performance.arXiv preprint arXiv:2402.14531,\n2024b.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context.arXiv preprint arXiv:2310.01558, 2023.\n54 Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models\nby partitioning gradients. InFindings of the Association for Computational Linguistics: ACL 2023, pp.\n6032\u20136048, 2023.\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-\nnative representations via the principle of maximal coding rate reduction.Advances in Neural Information\nProcessing Systems, 33:9422\u20139434, 2020.\nHao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic\nsurvey. IEEE transactions on pattern analysis and machine intelligence, 45(5):5782\u20135799, 2022."
    },
    {
        "vector_id": 5694,
        "text": "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. InThe Eleventh\nInternational Conference on Learning Representations, 2022.\nYi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade\nllms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.arXiv preprint\narXiv:2401.06373, 2024.\nQuan-shi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of\nInformation Technology & Electronic Engineering, 19(1):27\u201339, 2018.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-\nthought reasoning in language models.arXiv preprint arXiv:2302.00923, 2023.\nChenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, and Ninghao Liu. Automated natural language expla-\nnation of deep visual neurons with large models.arXiv preprint arXiv:2310.10708, 2023a."
    },
    {
        "vector_id": 5695,
        "text": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,\nand Mengnan Du. Explainability for large language models: A survey.ACM Transactions on Intelligent\nSystems and Technology (TIST), 2023b.\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, and Mengnan Du. Opening the black box of large language\nmodels: Two views on holistic interpretability.arXiv preprint arXiv:2402.10688, 2024.\nXinyan Zhao and VG Vinod Vydiswaran. Lirex: Augmenting language inference with relevant explanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14532\u201314539, 2021.\nZiqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, and Ninghao Liu. Interpretation of time-\nseries deep models: A survey.arXiv preprint arXiv:2305.14582, 2023c.\nZexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, and Danqi Chen. Mquake:\nAssessing knowledge editing in language models via multi-hop questions.arXiv preprint arXiv:2305.14795,\n2023."
    },
    {
        "vector_id": 5696,
        "text": "2023.\nKun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and\nJi-Rong Wen. S3-Rec: Self-supervised learning for sequential recommendation with mutual information\nmaximization. In CIKM, pp. 1893\u20131902, 2020.\nYaochen Zhu, Jing Ma, and Jundong Li. Causal inference in recommender systems: A survey of strategies\nfor bias mitigation, explanation, and generalization.arXiv preprint arXiv:2301.00910, 2023a.\nYaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. Collaborative large language model for\nrecommender systems. InThe Web Conference, 2024.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\nLarge language models can learn rules.arXiv preprint arXiv:2310.07064, 2023b.\nHonglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky, Yonghui Wu, Petr Mitrichev,\nEthanSterling, NathanBell, WalkerRavina, andHai Qian. Interpretableranking withgeneralizedadditive\nmodels. In WSDM, 2021."
    },
    {
        "vector_id": 5697,
        "text": "models. In WSDM, 2021.\n55 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang\nYin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach\nto ai transparency.arXiv preprint arXiv:2310.01405, 2023a.\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks\non aligned language models.arXiv preprint arXiv:2307.15043, 2023b.\n56"
    }
]