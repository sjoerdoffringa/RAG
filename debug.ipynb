{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SjoerdO\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rag--7y4UdRY-py3.13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rag_module.rag import RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2 (this message can be suppressed by setting verbose=0)\n",
      "Loading model FlashRank model ms-marco-MiniLM-L-12-v2...\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(LLM_name=\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 28/28 [00:08<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "rag.reload_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.47it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chunks': [{'filename': 'Rorseth2024.pdf',\n",
       "   'text': 'knowledge sources used during RAG, exposing the in-context\\nlearning behaviors of the LLM. Motivated by our prior work\\nusing counterfactual explanations for information retrieval [3],\\nwe derive provenance counterfactually by identifying minimal\\ncontext perturbations that change an LLM’s output. Our con-\\ntributions are summarized as follows.\\n1) Answer Origin Explainability. We introduce a novel\\nframework to assess the origin of LLM answers, with',\n",
       "   'vector_id': 8,\n",
       "   'distance': np.float32(1.0441858),\n",
       "   'score': np.float32(0.47887334)},\n",
       "  {'filename': 'Anand2022.pdf',\n",
       "   'text': 'to a specific decision, the generated explanation can be a feature attribution . On the one hand,\\nfeature attributions can be soft masks, i.e., real numbers denoting feature importance. On the other\\nhand, they can also be presented as boolean or hard masks where a feature is either present or\\nabsent in the explanation. An explanation is understandable to humans or users based not only\\nif the feature space is understandable but also if the explanation is small. An attribution over a',\n",
       "   'vector_id': 22,\n",
       "   'distance': np.float32(1.1153927),\n",
       "   'score': np.float32(0.45182353)},\n",
       "  {'filename': 'Wu2024.pdf',\n",
       "   'text': 'are accurate and up-to-date. It addresses LLMs’ limitation of relying on fixed and potentially outdated\\nknowledge bases. RAG operates in two steps: (1)Retrieval: It locates and fetches pertinent information\\nfrom an external source based on the user’s query; (2)Generation: It incorporates this information into the\\nmodel’s generated response. Given an input queryx and the desired outputy, the objective function of\\nRAG can be formulated as (Guu et al., 2020):\\nmax\\nϕ,θ\\nlog p(y|x) = max\\nϕ,θ\\nlog\\n∑\\nz∈K',\n",
       "   'vector_id': 242,\n",
       "   'distance': np.float32(0.94648945),\n",
       "   'score': np.float32(0.19402137)}],\n",
       " 'query': 'How would feature attribution apply to a RAG system?',\n",
       " 'answer': \"Feature attribution in a Retrieval-Augmented Generation (RAG) system can be applied to understand how different pieces of retrieved information contribute to the final response generated by the language model (LLM). Here’s how it works in the context of a RAG system:\\n\\n1. **Understanding Influence**: Feature attribution helps identify which specific features (or pieces of information) from the retrieved data significantly influence the output generated by the LLM for a given input query. This can aid users in understanding how the response was formulated.\\n\\n2. **Types of Attributions**: In this framework, feature attributions could manifest as:\\n   - **Soft Masks**: These would indicate the importance of various features through real numbers, helping discern the relative weight each retrieved piece of information holds in generating the final response.\\n   - **Hard Masks**: These would present a more binary view, signifying whether particular features were included or excluded in the explanation provided by the LLM.\\n\\n3. **Explanation Clarity**: For the explanation to be useful to users, it must be comprehensible. This means that the features included in the attribution must be understandable, and the explanation itself should be concise, focusing only on the most relevant information.\\n\\n4. **Improving Transparency**: By providing clear feature attributions, RAG systems can enhance transparency regarding how answers are derived, allowing for easier identification of potentially misleading or irrelevant information.\\n\\n5. **Addressing Limitations**: Feature attributions can address the limitations inherent in LLMs that typically rely on static and possibly outdated knowledge bases by dynamically demonstrating which retrieved information was most relevant for generating the response.\\n\\nIn summary, feature attribution in a RAG system provides a mechanism to clarify and explain how retrieved information impacts the output of the LLM, thereby improving interpretability and trust in the system's responses.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query(\"How would feature attribution apply to a RAG system?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag--7y4UdRY-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
