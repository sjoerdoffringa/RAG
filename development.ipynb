{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pypdf\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Mistral\n",
    "from mistralai import Mistral\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query functions\n",
    "def query_gemini(prompt: str, modelname=\"gemini-2.0-flash\") -> str:\n",
    "    \"\"\"Queries the Gemini API with a prompt and returns the response.\"\"\"\n",
    "    model = genai.GenerativeModel(modelname)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def query_mistral(prompt: str, modelname=\"ministral-3b-latest\") -> str:\n",
    "    \"\"\"Queries the Mistral API with a prompt and returns the response.\"\"\"\n",
    "    chat_response = client.chat.complete(\n",
    "        model = modelname,\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt,}]\n",
    "    )\n",
    "\n",
    "    response = chat_response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "# file load functions\n",
    "def pdf_to_text(filepath) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    reader = pypdf.PdfReader(filepath)\n",
    "    text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()]) # join pages\n",
    "    return text\n",
    "\n",
    "def txt_to_text(filepath: str) -> str:\n",
    "    \"\"\"Read text from a .txt file.\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    \"\"\"Handles file loading, chunking and text embedding using SentenceTransformer and stores embeddings in FAISS & JSON.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.data_path = os.getenv(\"data_path\", \"\")\n",
    "        self.vector_embedding_path = os.getenv(\"embedding_path\", \"\") + 'chunk_vectors.faiss'\n",
    "        self.metadata_path = os.getenv(\"embedding_path\", \"\") + 'chunk_metadata.json'\n",
    "        self.index = None\n",
    "        self.metadata = None\n",
    "\n",
    "        if os.path.exists(self.vector_embedding_path) and os.path.exists(self.metadata_path):\n",
    "                self._load_embeddings()\n",
    "\n",
    "    def _load_embeddings(self):\n",
    "        \"\"\"Loads embeddings and metadata from files.\"\"\"\n",
    "        self.index = self._load_index()\n",
    "        self.metadata = self._load_metadata()\n",
    "\n",
    "    def _delete_embedding_files(self):\n",
    "        \"\"\"Deletes the vector embedding and metadata files.\"\"\"\n",
    "        if os.path.exists(self.vector_embedding_path):\n",
    "            os.remove(self.vector_embedding_path)\n",
    "        if os.path.exists(self.metadata_path):\n",
    "            os.remove(self.metadata_path)\n",
    "    \n",
    "    def _load_documents(self):\n",
    "        \"\"\"Loads text data from the data_path.\"\"\"\n",
    "        docs = []\n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                docs.append({\"filename\": filename, \"text\": pdf_to_text(file_path)})\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                docs.append({\"filename\": filename, \"text\": txt_to_text(file_path)})\n",
    "            elif os.path.isdir(file_path):\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Could not load file {filename}\")\n",
    "        return docs\n",
    "\n",
    "    def _chunk_documents(self, chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"Splits texts into chunks.\"\"\"\n",
    "        docs = self._load_documents()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        chunks = []\n",
    "        for doc in docs:\n",
    "            chunk_texts = splitter.split_text(doc[\"text\"])\n",
    "            filename = doc[\"filename\"]\n",
    "            for chunk_text in chunk_texts:\n",
    "                chunks.append({\"filename\": filename, \"text\": chunk_text})\n",
    "\n",
    "        return chunks \n",
    "\n",
    "    def encode(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"Returns vector embeddings for a list of texts.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True)\n",
    "    \n",
    "    def _create_index(self):\n",
    "        \"\"\"Creates FAISS index and stores it in the vector_embedding_path.\"\"\"\n",
    "        self.index = faiss.IndexFlatL2(self.dim)\n",
    "\n",
    "    def _load_index(self):\n",
    "        \"\"\"Loads FAISS index from the vector_embedding_path.\"\"\"\n",
    "        return faiss.read_index(self.vector_embedding_path)\n",
    "    \n",
    "    def _save_index(self):\n",
    "        \"\"\"Saves FAISS index to the vector_embedding_path.\"\"\"\n",
    "        faiss.write_index(self.index, self.vector_embedding_path)\n",
    "    \n",
    "    def _create_metadata(self):\n",
    "        \"\"\"Creates metadata file if it does not exist.\"\"\"\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            with open(self.metadata_path, \"w\") as f:\n",
    "                json.dump([], f)\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Loads metadata from JSON file if it exists; otherwise, returns an empty list.\"\"\"\n",
    "        try:\n",
    "            with open(self.metadata_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Saves metadata to JSON file.\"\"\"\n",
    "        with open(self.metadata_path, \"w\") as f:\n",
    "            json.dump(self.metadata, f, indent=4)\n",
    "\n",
    "    def _store_embeddings(self, chunks: list[dict]):\n",
    "        \"\"\"Stores vector embeddings in FAISS and saves text-index mapping in JSON.\"\"\"\n",
    "        chunk_vectors = self.encode([chunk[\"text\"] for chunk in chunks])\n",
    "\n",
    "        # Get the starting index for new entries\n",
    "        start_idx = len(self.metadata)\n",
    "\n",
    "        # Store vectors in FAISS\n",
    "        self.index.add(chunk_vectors)\n",
    "        \n",
    "        # Store metadata (text -> FAISS index)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.metadata.append({\n",
    "                \"filename\": chunk[\"filename\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"vector_id\": start_idx + i\n",
    "            })\n",
    "\n",
    "        self._save_index()\n",
    "        self._save_metadata()\n",
    "\n",
    "    def reload_embeddings(self, chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"Reloads embeddings from the data_path.\"\"\"\n",
    "        \n",
    "        # check if data path contains files\n",
    "        if len(os.listdir(self.data_path)) == 0:\n",
    "            print(\"No files found in data_path.\")\n",
    "            return\n",
    "        \n",
    "        self._delete_embedding_files()\n",
    "        self._create_index()\n",
    "        self._create_metadata()\n",
    "        chunks = self._chunk_documents(chunk_size, chunk_overlap)\n",
    "        self._store_embeddings(chunks)\n",
    "\n",
    "    def search(self, query_text, top_k=3):\n",
    "        \"\"\"Searches FAISS and retrieves text metadata from JSON.\"\"\"\n",
    "        query_vector = self.encode([query_text])\n",
    "        distances, indices = self.index.search(query_vector, top_k)\n",
    "        return distances, indices\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, embedding_model_name=\"all-MiniLM-L6-v2\", LLM_name=\"gemini-2.0-flash\"):       \n",
    "        self.data_path = os.getenv(\"data_path\")\n",
    "        self.vector_embedding_path = os.getenv(\"vector_embedding_path\")\n",
    "        self.embedder = Embedder(embedding_model_name) \n",
    "        self.LLM_name = LLM_name\n",
    "\n",
    "    def _retrieve_context(self, query, top_k_chunks):\n",
    "        distances, indices = self.embedder.search(query, top_k=top_k_chunks)\n",
    "        relevant_chunks = [self.embedder.metadata[idx] for idx in indices[0]]\n",
    "        for i, chunk in enumerate(relevant_chunks):\n",
    "            chunk[\"distance\"] = distances[0][i]\n",
    "        return relevant_chunks\n",
    "    \n",
    "    def reload_embeddings(self, chunk_size=500, chunk_overlap=50):\n",
    "        self.embedder.reload_embeddings(chunk_size, chunk_overlap)\n",
    "\n",
    "    def query(self, query, top_k_chunks=3):\n",
    "        relevant_chunks = self._retrieve_context(query, top_k_chunks)\n",
    "        joined_chunks = \" \".join([chunk[\"text\"] for chunk in relevant_chunks])\n",
    "        \n",
    "        # Format the prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant. Use the following retrieved context to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {joined_chunks}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        answer = query_gemini(prompt)\n",
    "\n",
    "        response = {\n",
    "            \"chunks\": relevant_chunks,\n",
    "            \"query\": query,\n",
    "            \"answer\": answer\n",
    "        }\t\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = Embedder(\"all-MiniLM-L6-v2\")\n",
    "em._delete_embedding_files()\n",
    "#em._chunk_documents(500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = Embedder(\"all-MiniLM-L6-v2\")\n",
    "em.reload_embeddings(500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of vectors in faiss\n",
    "em.index.ntotal == len(em.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "rag = RAG(top_k_chunks=5)\n",
    "rag.reload_embeddings()\n",
    "rag.embedder.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGE works by querying external sources and incorporating relevant information into its input context. It considers combinations of retrieved sources, creates prompts based on the user's question for each combination, and retrieves answers from a Large Language Model (LLM). After analyzing the answers, RAGE groups combinations by answer and displays the proportion of each answer. It identifies parts of the input context that, when removed, change the LLM's answer, providing counterfactual explanations. RAGE also includes pruning methods to manage the space of possible explanations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the RAGE tool work?\"\n",
    "response = rag.query(query)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'Rorseth2024.pdf',\n",
       "  'text': '1A video is available at https://vimeo.com/877281038.\\n2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\\narXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\\nSearch\\nCounterfactual\\nSearch\\nRetrieval Model\\n(Pyserini BM25)\\nLlama 2 Chat 7B LLMCounterfactual\\nExplanations\\nAnswers\\nAnalysis\\nLucene\\nIndex\\nUsers RAGE \\nWeb App\\n(Plotly Dash)\\nKnowledge \\nLLM \\nFig. 1. The architecture of RAGE.\\nknowledge about the topic and a provided set of sources. In',\n",
       "  'vector_id': 332,\n",
       "  'distance': np.float32(1.0285647)},\n",
       " {'filename': 'Rorseth2024.pdf',\n",
       "  'text': 'set of combinations, RAGE considers all combinations of the\\nretrieved sources Dq, or draws a fixed-size random sample of s\\ncombinations. Based on the userâ€™s original question, a prompt\\nis created for each selected combination, which is then used to\\nretrieve corresponding answers from the LLM. After analyzing\\nthe answers, RAGE renders a table that groups combinations\\nby answer, along with a pie chart illustrating the proportion\\nof each answer across all combinations. A rule is determined',\n",
       "  'vector_id': 344,\n",
       "  'distance': np.float32(1.1191269)},\n",
       " {'filename': 'Rorseth2024.pdf',\n",
       "  'text': 'retrieval capabilities; i.e., able to query external sources and pull\\nrelevant information into their input context. Our explanations\\nare counterfactual in the sense that they identify parts of the\\ninput context that, when removed, change the answer to the\\nquestion posed to the LLM. RAGE includes pruning methods to\\nnavigate the vast space of possible explanations, allowing users\\nto view the provenance of the produced answers.\\nI. I NTRODUCTION',\n",
       "  'vector_id': 322,\n",
       "  'distance': np.float32(1.1192001)}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449.1808873720137"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(chunk['text']) for chunk in rag.embedder.metadata]) / len(rag.embedder.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self, model=\"gemini\", model_name=\"gemini-2.0-flash\"):\n",
    "        self.data_path = os.getenv(\"data_path\")\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.summary_extension = \".summary.txt\"\n",
    "\n",
    "    def summarize_text(self, text: str) -> str:\n",
    "        prompt = f\"Summarize the following document in one paragraph: \\n{text}\"\n",
    "        if self.model == \"gemini\":\n",
    "            response = query_gemini(prompt)\n",
    "        elif self.model == \"mistral\":\n",
    "            response = query_mistral(prompt)\n",
    "        else:\n",
    "            print(f\"Model {self.model} not found.\")\n",
    "            return None\n",
    "        return response\n",
    "    \n",
    "    def _load_document(self, path: str) -> str:\n",
    "        if path.endswith(\".pdf\"):\n",
    "            return pdf_to_text(path)\n",
    "        elif path.endswith(\".txt\"):\n",
    "            return txt_to_text(path)\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Could not load directory {path}\")\n",
    "        else:\n",
    "            print(f\"Could not load file {path}\")\n",
    "\n",
    "    def _write_file(self, path: str, text: str):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "    \n",
    "    def create_summaries(self, clear=True):\n",
    "        if clear:\n",
    "            for filename in os.listdir(self.data_path):\n",
    "                if filename.endswith(self.summary_extension):\n",
    "                    file_path = os.path.join(self.data_path, filename)\n",
    "                    os.remove(file_path)\n",
    "\n",
    "        summaries = {}\n",
    "\n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            filename = os.path.splitext(filename)[0] # remove file extension\n",
    "            \n",
    "            text = self._load_document(file_path)\n",
    "            summary = self.summarize_text(text)\n",
    "            \n",
    "            summary_path = f\"{self.data_path}/{filename}{self.summary_extension}\"\n",
    "            summaries[summary_path] = summary\n",
    "        \n",
    "        for path, summary in summaries.items():\n",
    "            self._write_file(path, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test summarizer\n",
    "s = Summarizer(model=\"mistral\")\n",
    "s.create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is usable XAI?\"\n",
    "embedding_model = Embedder(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the index\n",
    "index = faiss.read_index(os.getenv(\"vector_embedding_path\"))\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = embedding_model.encode([query])\n",
    "\n",
    "# Search for the most similar chunks\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "\n",
    "# Retrieve the most similar chunks\n",
    "#retrieved_context = [self.chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([375, 726, 402])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(include_summaries=True)\n",
    "rag._create_summaries()\n",
    "\n",
    "#query = \"What is usable XAI?\"\n",
    "#rag.create_embedding()\n",
    "#print(rag.doc_names)\n",
    "#rag.query(query)\n",
    "#rag.create_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "\n",
    "def split_text_into_chunks(texts, chunk_size=1000, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(splitter.split_text(text))\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text_into_chunks(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(texts, chunk_size=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    Splits a list of texts into chunks and returns a list of dictionaries.\n",
    "\n",
    "    :param texts: List of text documents\n",
    "    :param chunk_size: Maximum chunk size\n",
    "    :param overlap: Overlap between chunks\n",
    "    :return: List of dictionaries with 'text' and 'document' keys\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "\n",
    "    for doc_index, text in enumerate(texts):\n",
    "        split_texts = splitter.split_text(text)\n",
    "        for chunk in split_texts:\n",
    "            chunks.append({\"text\": chunk, \"document\": doc_index})  # Track source document\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "chunks = split_text_into_chunks(pdf_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed data\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings_local(text_chunks):\n",
    "    text_chunks = [chunk['text'] for chunk in chunks]\n",
    "    return embedding_model.encode(text_chunks, convert_to_list=True)\n",
    "\n",
    "vector_embeddings = get_embeddings_local(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to a NumPy array\n",
    "embedding_matrix = np.array(vector_embeddings, dtype=np.float32)\n",
    "\n",
    "# Create FAISS index (L2 similarity search)\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save the index for later use\n",
    "faiss.write_index(index, \"vector_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{303: 1.0080444812774658, 310: 1.1675567626953125}\n"
     ]
    }
   ],
   "source": [
    "def search_faiss(query, top_k=2):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Create a dictionary with index as key and distance as value\n",
    "    results = {int(idx): float(dist) for idx, dist in zip(indices[0], distances[0])}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "query = \"What are the advantages of RAG?\"\n",
    "result = search_faiss(query, top_k=2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "doc_origins = []\n",
    "for chunk in result:\n",
    "    doc = chunks[chunk]['document']\n",
    "    doc_origins.append(doc)\n",
    "\n",
    "mode(doc_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top matching texts from FAISS\n",
    "text_chunks = [chunk['text'] for chunk in chunks]\n",
    "retrieved_context = \"\\n\".join([text_chunks[idx] for idx in result.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 3\n",
      "number of chunks: 953\n",
      "average chunk size (characters): 451\n"
     ]
    }
   ],
   "source": [
    "# number of documents\n",
    "n_docs = len(os.listdir('./data'))\n",
    "print(f'number of documents: {n_docs}')\n",
    "\n",
    "# number of chunks\n",
    "n_chunks = len(text_chunks)\n",
    "print(f'number of chunks: {n_chunks}')\n",
    "\n",
    "# average chunk size\n",
    "avg_chunk_size = round(sum([len(text) for text in text_chunks]) / n_chunks)\n",
    "print(f'average chunk size (characters): {avg_chunk_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The length of a banana can vary depending on the variety, but a typical banana is **around 6 to 9 inches (15 to 23 centimeters) long.**\\n',\n",
       " \"The length of a banana can vary quite a bit, depending on the type and how it's grown. However, a typical banana is usually between **6 and 9 inches (15 to 23 cm) long**.\\n\",\n",
       " 'The length of a banana can vary depending on the type and ripeness, but a typical banana is **about 6 to 9 inches (15 to 23 cm) long.**\\n',\n",
       " 'The length of a banana can vary, but on average:\\n\\n*   **Typical banana:** 6-8 inches (15-20 cm)\\n\\nKeep in mind that there are different varieties of bananas, some of which are smaller or larger than average.',\n",
       " 'The length of a banana can vary depending on the variety, but a typical banana is **around 6-9 inches (15-23 cm) long**.\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query LLM n times\n",
    "n = 5\n",
    "query = 'How long is a banana?'\n",
    "\n",
    "responses = []\n",
    "for i in range(n):\n",
    "    response = query_gemini(query)\n",
    "    responses.append(response)\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# Load a local LLM (example: Mistral-7B)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "def query_local_llm(query, retrieved_context):\n",
    "    \"\"\"Query a local LLM using the retrieved context.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the following retrieved context to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {retrieved_context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_pipeline(prompt, max_length=500, do_sample=True)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "response = query_local_llm(query, retrieved_context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
