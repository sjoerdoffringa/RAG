{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pypdf\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Gemini\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Mistral\n",
    "from mistralai import Mistral\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gemini(prompt: str, modelname=\"gemini-2.0-flash\"):\n",
    "    \"\"\"Queries the Gemini API with a prompt and returns the response.\"\"\"\n",
    "    model = genai.GenerativeModel(modelname)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def query_mistral(prompt: str, modelname=\"ministral-3b-latest\"):\n",
    "    \"\"\"Queries the Mistral API with a prompt and returns the response.\"\"\"\n",
    "    chat_response = client.chat.complete(\n",
    "        model = modelname,\n",
    "        messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }]\n",
    "    )\n",
    "\n",
    "    response = chat_response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "def pdf_to_text(filepath) -> str:\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    reader = pypdf.PdfReader(filepath)\n",
    "    text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()]) # join pages\n",
    "    return text\n",
    "\n",
    "def txt_to_text(filepath: str) -> str:\n",
    "    \"\"\"Reads text from a .txt file.\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "class Embedder:\n",
    "    \"\"\"Handles text embedding using SentenceTransformer and stores embeddings in FAISS & JSON.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "        self.vector_embedding_path = os.getenv(\"vector_embedding_path\") + 'chunk_vectors.faiss'\n",
    "        self.metadata_path = os.getenv(\"vector_embedding_path\") + 'chunk_metadata.json'\n",
    "        self.index = self._load_or_create_index()\n",
    "        self.metadata = self._load_metadata()\n",
    "\n",
    "    def _delete_files(self):\n",
    "        \"\"\"Deletes the vector embedding and metadata files.\"\"\"\n",
    "        if os.path.exists(self.vector_embedding_path):\n",
    "            os.remove(self.vector_embedding_path)\n",
    "        if os.path.exists(self.metadata_path):\n",
    "            os.remove(self.metadata)\n",
    "\n",
    "    def encode(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"Returns vector embeddings for a list of texts.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_numpy=True)\n",
    "    \n",
    "    def _load_index(self):\n",
    "        \"\"\"Loads FAISS index if available.\"\"\"\n",
    "        if os.path.exists(self.vector_embedding_path):\n",
    "            return faiss.read_index(self.vector_embedding_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Vector embedding file does not exist.\")\n",
    "    \n",
    "    def _load_or_create_index(self):\n",
    "        \"\"\"Loads FAISS index if available; otherwise, creates a new one.\"\"\"\n",
    "        if os.path.exists(self.vector_embedding_path):\n",
    "            return faiss.read_index(self.vector_embedding_path)\n",
    "        return faiss.IndexFlatL2(self.dim)\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Loads metadata from JSON file if it exists; otherwise, returns an empty list.\"\"\"\n",
    "        if os.path.exists(self.metadata_path):\n",
    "            try:\n",
    "                with open(self.metadata_path, \"r\") as f:\n",
    "                    return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return []\n",
    "\n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Saves metadata to JSON file.\"\"\"\n",
    "        with open(self.metadata_path, \"w\") as f:\n",
    "            json.dump(self.metadata, f, indent=4)\n",
    "\n",
    "    def store_embeddings(self, texts: list[str]):\n",
    "        \"\"\"Stores vector embeddings in FAISS and saves text-index mapping in JSON.\"\"\"\n",
    "        embeddings = self.encode(texts)\n",
    "\n",
    "        # Get the starting index for new entries\n",
    "        start_idx = len(self.metadata)\n",
    "\n",
    "        # Store vectors in FAISS\n",
    "        self.index.add(embeddings)\n",
    "        faiss.write_index(self.index, self.vector_embedding_path)\n",
    "\n",
    "        # Store metadata (text -> FAISS index)\n",
    "        for i, text in enumerate(texts):\n",
    "            self.metadata.append({\"vector_id\": start_idx + i, \"text\": text})\n",
    "\n",
    "        # Save metadata\n",
    "        self._save_metadata()\n",
    "\n",
    "    def search(self, query_text, top_k=3):\n",
    "        \"\"\"Searches FAISS and retrieves text metadata from JSON.\"\"\"\n",
    "        query_vector = self.encode([query_text])\n",
    "        distances, indices = self.index.search(query_vector, top_k)\n",
    "\n",
    "        # Retrieve corresponding texts from metadata\n",
    "        results = [entry[\"text\"] for entry in self.metadata if entry[\"vector_id\"] in indices[0]]\n",
    "        return results\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self,\n",
    "                 chunk_size=500, chunk_overlap=50, top_k_chunks=3,\n",
    "                 embedding_model_name=\"all-MiniLM-L6-v2\", LLM_name=\"gemini-2.0-flash\",\n",
    "                 reload_data=False):\n",
    "        \n",
    "        self.data_path = os.getenv(\"data_path\")\n",
    "        self.vector_embedding_path = os.getenv(\"vector_embedding_path\")\n",
    "        self.doc_names = None\n",
    "        self.doc_texts = None\n",
    "        self.chunks = None\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.top_k_chunks = top_k_chunks\n",
    "        self.embedding_model = Embedder(embedding_model_name) \n",
    "        self.LLM_name = LLM_name\n",
    "        self.reload_data = reload_data\n",
    "\n",
    "        # create embedding\n",
    "        if not os.path.exists(self.vector_embedding_path + 'chunk_vectors.faiss') or reload_data:\n",
    "            self.create_embedding()\n",
    "\n",
    "    def _load_documents(self):\n",
    "        texts = []\n",
    "        filenames = []\n",
    "        \n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                texts.append(pdf_to_text(file_path))\n",
    "                filenames.append(filename)\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                texts.append(txt_to_text(file_path))\n",
    "                filenames.append(filename)\n",
    "            elif os.path.isdir(file_path):\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Could not load file {filename}\")\n",
    "        \n",
    "        self.doc_names=filenames\n",
    "        self.doc_texts=texts\n",
    "\n",
    "    def get_document_texts(self):\n",
    "        if self.doc_texts is None:\n",
    "            self._load_documents()\n",
    "        return self.doc_texts\n",
    "\n",
    "    def _load_chunks(self):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "        chunks = []\n",
    "        for text in self.doc_texts:\n",
    "            chunks.extend(splitter.split_text(text))\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def create_embedding(self):\n",
    "        self._load_documents()\n",
    "        self._load_chunks()\n",
    "        self.embedding_model.store_embeddings(self.chunks)\n",
    "\n",
    "    def _retrieve_context(self, query):\n",
    "        relevant_chunks = self.embedding_model.search(query, top_k=self.top_k_chunks)\n",
    "        return \" \".join(relevant_chunks)\n",
    "\n",
    "    def query(self, query):\n",
    "        relevant_context = self._retrieve_context(query)\n",
    "        \n",
    "        # Format the prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant. Use the following retrieved context to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {relevant_context}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "\n",
    "        print(prompt)\n",
    "\n",
    "        response = query_gemini(prompt)\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self, model=\"gemini\", model_name=\"gemini-2.0-flash\"):\n",
    "        self.data_path = os.getenv(\"data_path\")\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.summary_extension = \".summary.txt\"\n",
    "\n",
    "    def summarize_text(self, text: str) -> str:\n",
    "        prompt = f\"Summarize the following document in one paragraph: \\n{text}\"\n",
    "        if self.model == \"gemini\":\n",
    "            response = query_gemini(prompt)\n",
    "        elif self.model == \"mistral\":\n",
    "            response = query_mistral(prompt)\n",
    "        else:\n",
    "            print(f\"Model {self.model} not found.\")\n",
    "            return None\n",
    "        return response\n",
    "    \n",
    "    def _load_document(self, path: str) -> str:\n",
    "        if path.endswith(\".pdf\"):\n",
    "            return pdf_to_text(path)\n",
    "        elif path.endswith(\".txt\"):\n",
    "            return txt_to_text(path)\n",
    "        elif os.path.isdir(path):\n",
    "            print(f\"Could not load directory {path}\")\n",
    "        else:\n",
    "            print(f\"Could not load file {path}\")\n",
    "\n",
    "    def _write_file(self, path: str, text: str):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "    \n",
    "    def create_summaries(self, clear=True):\n",
    "        if clear:\n",
    "            for filename in os.listdir(self.data_path):\n",
    "                if filename.endswith(self.summary_extension):\n",
    "                    file_path = os.path.join(self.data_path, filename)\n",
    "                    os.remove(file_path)\n",
    "\n",
    "        summaries = {}\n",
    "\n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            filename = os.path.splitext(filename)[0] # remove file extension\n",
    "            \n",
    "            text = self._load_document(file_path)\n",
    "            summary = self.summarize_text(text)\n",
    "            \n",
    "            summary_path = f\"{self.data_path}/{filename}{self.summary_extension}\"\n",
    "            summaries[summary_path] = summary\n",
    "        \n",
    "        for path, summary in summaries.items():\n",
    "            self._write_file(path, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test summarizer\n",
    "s = Summarizer(model=\"mistral\")\n",
    "s.create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "        You are an AI assistant. Use the following retrieved context to answer the question.\n",
      "\n",
      "        Context:\n",
      "        1A video is available at https://vimeo.com/877281038.\n",
      "2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\n",
      "arXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\n",
      "Search\n",
      "Counterfactual\n",
      "Search\n",
      "Retrieval Model\n",
      "(Pyserini BM25)\n",
      "Llama 2 Chat 7B LLMCounterfactual\n",
      "Explanations\n",
      "Answers\n",
      "Analysis\n",
      "Lucene\n",
      "Index\n",
      "Users RAGE \n",
      "Web App\n",
      "(Plotly Dash)\n",
      "Knowledge \n",
      "LLM \n",
      "Fig. 1. The architecture of RAGE.\n",
      "knowledge about the topic and a provided set of sources. In 1A video is available at https://vimeo.com/877281038.\n",
      "2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\n",
      "arXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\n",
      "Search\n",
      "Counterfactual\n",
      "Search\n",
      "Retrieval Model\n",
      "(Pyserini BM25)\n",
      "Llama 2 Chat 7B LLMCounterfactual\n",
      "Explanations\n",
      "Answers\n",
      "Analysis\n",
      "Lucene\n",
      "Index\n",
      "Users RAGE \n",
      "Web App\n",
      "(Plotly Dash)\n",
      "Knowledge \n",
      "LLM \n",
      "Fig. 1. The architecture of RAGE.\n",
      "knowledge about the topic and a provided set of sources. In 1A video is available at https://vimeo.com/877281038.\n",
      "2The tool is available at http://lg-research-2.uwaterloo.ca:8092/rage.\n",
      "arXiv:2405.13000v1  [cs.CL]  11 May 2024 Perturbation\n",
      "Search\n",
      "Counterfactual\n",
      "Search\n",
      "Retrieval Model\n",
      "(Pyserini BM25)\n",
      "Llama 2 Chat 7B LLMCounterfactual\n",
      "Explanations\n",
      "Answers\n",
      "Analysis\n",
      "Lucene\n",
      "Index\n",
      "Users RAGE \n",
      "Web App\n",
      "(Plotly Dash)\n",
      "Knowledge \n",
      "LLM \n",
      "Fig. 1. The architecture of RAGE.\n",
      "knowledge about the topic and a provided set of sources. In\n",
      "\n",
      "        Question:\n",
      "        How does the RAGE tool work?\n",
      "        \n",
      "Based on the provided context, here's how the RAGE tool works:\n",
      "\n",
      "RAGE uses a retrieval model (Pyserini BM25) to search a Lucene index. It then uses the Llama 2 Chat 7B LLM to generate counterfactual explanations, answers, and analysis based on the retrieved knowledge and a set of provided sources. Users interact with RAGE through a web app built with Plotly Dash.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "rag = RAG(chunk_size=1000, chunk_overlap=50, top_k_chunks=3, include_summaries=True, reload_data=True)\n",
    "\n",
    "query = \"How does the RAGE tool work?\"\n",
    "response = rag.query(query)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is usable XAI?\"\n",
    "embedding_model = Embedder(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the index\n",
    "index = faiss.read_index(os.getenv(\"vector_embedding_path\"))\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = embedding_model.encode([query])\n",
    "\n",
    "# Search for the most similar chunks\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "\n",
    "# Retrieve the most similar chunks\n",
    "#retrieved_context = [self.chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([375, 726, 402])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(include_summaries=True)\n",
    "rag._create_summaries()\n",
    "\n",
    "#query = \"What is usable XAI?\"\n",
    "#rag.create_embedding()\n",
    "#print(rag.doc_names)\n",
    "#rag.query(query)\n",
    "#rag.create_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "\n",
    "def split_text_into_chunks(texts, chunk_size=1000, overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(splitter.split_text(text))\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text_into_chunks(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(texts, chunk_size=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    Splits a list of texts into chunks and returns a list of dictionaries.\n",
    "\n",
    "    :param texts: List of text documents\n",
    "    :param chunk_size: Maximum chunk size\n",
    "    :param overlap: Overlap between chunks\n",
    "    :return: List of dictionaries with 'text' and 'document' keys\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    chunks = []\n",
    "\n",
    "    for doc_index, text in enumerate(texts):\n",
    "        split_texts = splitter.split_text(text)\n",
    "        for chunk in split_texts:\n",
    "            chunks.append({\"text\": chunk, \"document\": doc_index})  # Track source document\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "chunks = split_text_into_chunks(pdf_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed data\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings_local(text_chunks):\n",
    "    text_chunks = [chunk['text'] for chunk in chunks]\n",
    "    return embedding_model.encode(text_chunks, convert_to_list=True)\n",
    "\n",
    "vector_embeddings = get_embeddings_local(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to a NumPy array\n",
    "embedding_matrix = np.array(vector_embeddings, dtype=np.float32)\n",
    "\n",
    "# Create FAISS index (L2 similarity search)\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# Save the index for later use\n",
    "faiss.write_index(index, \"vector_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{303: 1.0080444812774658, 310: 1.1675567626953125}\n"
     ]
    }
   ],
   "source": [
    "def search_faiss(query, top_k=2):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Create a dictionary with index as key and distance as value\n",
    "    results = {int(idx): float(dist) for idx, dist in zip(indices[0], distances[0])}\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "query = \"What are the advantages of RAG?\"\n",
    "result = search_faiss(query, top_k=2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "doc_origins = []\n",
    "for chunk in result:\n",
    "    doc = chunks[chunk]['document']\n",
    "    doc_origins.append(doc)\n",
    "\n",
    "mode(doc_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top matching texts from FAISS\n",
    "text_chunks = [chunk['text'] for chunk in chunks]\n",
    "retrieved_context = \"\\n\".join([text_chunks[idx] for idx in result.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 3\n",
      "number of chunks: 953\n",
      "average chunk size (characters): 451\n"
     ]
    }
   ],
   "source": [
    "# number of documents\n",
    "n_docs = len(os.listdir('./data'))\n",
    "print(f'number of documents: {n_docs}')\n",
    "\n",
    "# number of chunks\n",
    "n_chunks = len(text_chunks)\n",
    "print(f'number of chunks: {n_chunks}')\n",
    "\n",
    "# average chunk size\n",
    "avg_chunk_size = round(sum([len(text) for text in text_chunks]) / n_chunks)\n",
    "print(f'average chunk size (characters): {avg_chunk_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The length of a banana can vary depending on the variety, but a typical banana is **around 6 to 9 inches (15 to 23 centimeters) long.**\\n',\n",
       " \"The length of a banana can vary quite a bit, depending on the type and how it's grown. However, a typical banana is usually between **6 and 9 inches (15 to 23 cm) long**.\\n\",\n",
       " 'The length of a banana can vary depending on the type and ripeness, but a typical banana is **about 6 to 9 inches (15 to 23 cm) long.**\\n',\n",
       " 'The length of a banana can vary, but on average:\\n\\n*   **Typical banana:** 6-8 inches (15-20 cm)\\n\\nKeep in mind that there are different varieties of bananas, some of which are smaller or larger than average.',\n",
       " 'The length of a banana can vary depending on the variety, but a typical banana is **around 6-9 inches (15-23 cm) long**.\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query LLM n times\n",
    "n = 5\n",
    "query = 'How long is a banana?'\n",
    "\n",
    "responses = []\n",
    "for i in range(n):\n",
    "    response = query_gemini(query)\n",
    "    responses.append(response)\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "# Load a local LLM (example: Mistral-7B)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "def query_local_llm(query, retrieved_context):\n",
    "    \"\"\"Query a local LLM using the retrieved context.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the following retrieved context to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {retrieved_context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_pipeline(prompt, max_length=500, do_sample=True)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "response = query_local_llm(query, retrieved_context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
