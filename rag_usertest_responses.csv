chunks,query,scope_prediction,scope_probabilities,answer,counterfactual
"[{'filename': '01-power-bi.md', 'text': ""# Power BI  \n## General  \nPower BI is a suite of business analytics tools that deliver insights throughout your organization. Connect to hundreds of data sources, simplify data prep, and drive ad hoc analysis. Produce beautiful reports, then publish them for your organization to consume on the web and across mobile devices.  \n> **Guideline:** Don't implement complex business logic in Power BI / DAX\n> **Why:** Business logic should never be in a presentation layer, where Power BI resides. Business logic should be implemented within the domain where the knowledge of the business lies. Besides, Power BI reports/dashboards tend to get copied, then modified for different user groups. This rapidly leads to duplicated business logic and maintenance headaches as the business logic is duplicated with it.  \n**Tooling**  \n- Power BI Desktop  \n**Platform**  \n- Power BI Service\n- Power BI Report Server\n- Power BI Embedded  \n**Alternatives**  \n- QlikView\n- Tableau\n- Sisense  \n**Training**  \n- [Training Analyzing Data with PowerBI](https://training.infosupport.com/trainingen/msda100/analyzing-data-with-microsoft-power-bi/)\n- [Microsoft Power BI Data Analyst](https://training.infosupport.com/trainingen/pl300/microsoft-power-bi-data-analyst/)\n- [Designing and Implementing Enterprise-Scale Analytics Solutions Using Microsoft Azure and Microsoft Power BI](https://training.infosupport.com/trainingen/dp500/designing-and-implementing-enterprise-scale-analytics-solutions-using-microsoft-azure-and-microsoft-power-bi/)"", 'vector_id': 1580, 'distance': np.float32(0.73016334), 'score': np.float32(0.98731416)}, {'filename': '01-power-bi.md', 'text': '### Power BI service vs (Azure) Analysis Services  \nTabular models are supported within Power BI service, Azure Analysis Service (AAS) and SQL Server Analysis Services (SSAS). SSAS runs on a server and both AAS and Power BI service run in the cloud.\nThe strategy from Microsoft is to make the available features in Power BI Premium a superset of those available in AAS and in SSAS. The only notable remaining difference is that Power BI Premium does not support scale out options (both AAS and SSAS do) and cannot be paused/resumed (AAS can).  \n> **Guideline:** Power BI service is favorable over both Azure Analysis Services (AAS) as well as SQL Server Analysis Services (SSAS) for tabular models. Only use SSAS when you want to fully avoid the cloud or use AAS from a cost perspective.\n> **Why:** Power BI service is the most advanced platform to support tabular models and has many additional features that accelerate the usage of the models by end-users. However, Power BI service will always be part of the cloud, so with a non-cloud requirement SSAS can still be selected. There might be some use-cases in which AAS is more cost-effective than Premium Per User or Premium Capacity that justifies the use of AAS.', 'vector_id': 1593, 'distance': np.float32(1.0227509), 'score': np.float32(0.13362424)}, {'filename': '01-power-bi.md', 'text': ""### Power BI (Premium) vs SQL Server Reporting Services  \nPower BI service is a platform designed by Microsoft to host dashboards, interactive reports and paginated reports that run in the cloud. SQL Server Reporting Services (SSRS) is an older platform also designed by Microsoft to host paginated reports that run on a server.\nPower BI Report Server extends SSRS with features from Power BI and allows you to run Power BI reports on an server. However, the Power BI Report Server has some [limitations](https://docs.microsoft.com/en-us/power-bi/report-server/compare-report-server-service) as opposed to the cloud product. For example, dashboards, dataflows, natural language and advanced analytics can't be used. Power BI Premium capacity and SQL Server Enterprise Edition with Software Assurance include licenses to install Power BI Report Server.\nPower BI service (with Premium) allows you to run paginated reports in the cloud. However, paginated reports in Power BI service have some [limitations](https://docs.microsoft.com/en-us/power-bi/guidance/migrate-ssrs-reports-to-power-bi#assess) as opposed to the paginated reports in SSRS. For example, shared data sources, shared datasets and other resources such as images can't be used.  \n> **Guideline:** Power BI service is favorable over SSRS and Power BI Report Server. Only use SSRS or Power BI Report server when you want to fully avoid the cloud or from a cost perspective.\n> **Why:** SSRS is a legacy product (although still supported) and Power BI Report server is an extension of that. They both lack a lot of features that Power BI service offers and Power BI service is far more future proof. However, Power BI service will always be part of the cloud, so with a non-cloud requirement they can still be selected. In addition, with a cheaper/existing licenses for SSRS (only when exclusively using paginated reports) or with Power BI Report server might be favorable from a cost perspective."", 'vector_id': 1595, 'distance': np.float32(1.0790877), 'score': np.float32(0.07131455)}]",What is the main reason for avoiding the implementation of complex business logic within Power BI?,0,[0.68187171 0.27442005 0.04370824],"The main reason for avoiding the implementation of complex business logic within Power BI lies in the principle of separation of concerns. Complex business logic should never be embedded in a presentation layer like Power BI. Instead, it should be implemented in the domain where the knowledge of the business resides. This approach is important for several reasons:

1. **Duplication and Maintenance Issues**: Power BI reports and dashboards are often duplicated and modified for different user groups, which can lead to duplicated business logic. This redundancy creates significant maintenance headaches as changes must be replicated across multiple reports and dashboards, increasing the risk of errors and inconsistencies [1].

2. **Clarity and Focus**: Keeping the business logic separate enhances clarity and focus in the reporting tools. Power BI should primarily be used for data visualization and analysis, rather than complex calculations and logic that can obfuscate its main purpose [1].

By adhering to these guidelines, organizations can ensure that their data presentation remains efficient, manageable, and less prone to errors over time.",
"[{'filename': 'retrospectives.md', 'text': '# Retrospectives  \n## According to the scrum guide  \nThe Sprint Retrospective is an opportunity for the Scrum Team to inspect itself and create a plan for improvements to be enacted during the next Sprint.  \nThe Sprint Retrospective occurs after the Sprint Review and prior to the next Sprint Planning. This is at most a three-hour meeting for one-month Sprints. For shorter Sprints, the event is usually shorter. The Scrum Master ensures that the event takes place and that attendants understand its purpose.  \nThe Scrum Master ensures that the meeting is positive and productive. The Scrum Master teaches all to keep it within the time-box. The Scrum Master participates as a peer team member in the meeting from the accountability over the Scrum process.  \nThe purpose of the Sprint Retrospective is to:  \n- Inspect how the last Sprint went with regards to people, relationships, process, and tools;\n- Identify and order the major items that went well and potential improvements; and,\n- Create a plan for implementing improvements to the way the Scrum Team does its work.  \nThe Scrum Master encourages the Scrum Team to improve, within the Scrum process framework, its development process and practices to make it more effective and enjoyable for the next Sprint. During each Sprint Retrospective, the Scrum Team plans ways to increase product quality by improving work processes or adapting the definition of ""Done"", if appropriate and not in conflict with product or organizational standards.  \nBy the end of the Sprint Retrospective, the Scrum Team should have identified improvements that it will implement in the next Sprint. Implementing these improvements in the next Sprint is the adaptation to the inspection of the Scrum Team itself. Although improvements may be implemented at any time, the Sprint Retrospective provides a formal opportunity to focus on inspection and adaptation.', 'vector_id': 127, 'distance': np.float32(0.34467834), 'score': np.float32(0.9998248)}, {'filename': 'sprint-review.md', 'text': '# Sprint Review  \n## Definition  \nA Sprint Review is held at the end of the Sprint to inspect the Increment and adapt the Product Backlog if needed. During the Sprint Review, the Scrum Team and stakeholders collaborate about what was done in the Sprint. Based on that and any changes to the Product Backlog during the Sprint, attendees collaborate on the next things that could be done to optimize value. This is an informal meeting, not a status meeting, and the presentation of the Increment is intended to elicit feedback and foster collaboration.  \nThis is at most a four-hour meeting for one-month Sprints. For shorter Sprints, the event is usually shorter. The Scrum Master ensures that the event takes place and that attendees understand its purpose. The Scrum Master teaches everyone involved to keep it within the time-box.', 'vector_id': 140, 'distance': np.float32(0.71554816), 'score': np.float32(0.9921102)}, {'filename': 'sprint-planning.md', 'text': '# Sprint Planning  \n## Definition  \n<!-- definition of the practice -->  \nSprint planning is an event in the Scrum framework where the team determines the product backlog items they will work on during that sprint and discusses their initial plan for completing those product backlog items. The selected items together define the sprint backlog for the specific sprint.', 'vector_id': 135, 'distance': np.float32(0.71686435), 'score': np.float32(0.95881563)}]",What is the purpose of the sprint retrospective in the Scrum process?,0,[0.91010983 0.0866376  0.00325258],"The purpose of the Sprint Retrospective in the Scrum process is to provide a structured opportunity for the Scrum Team to reflect on their past performance and identify areas for improvement. This meeting occurs after the Sprint Review and before the next Sprint Planning, allowing the team to assess how the last Sprint went concerning people, relationships, processes, and tools. Specifically, the goals of the Sprint Retrospective include:

1. **Inspection**: The team inspects how well the last Sprint went, analyzing aspects such as team dynamics and workflow efficiency.
   
2. **Identification of Strengths and Weaknesses**: The team identifies and prioritizes the positive aspects that contributed to success, as well as potential improvements that can be made moving forward.

3. **Improvement Planning**: The Scrum Team formulates a plan for implementing these improvements in the forthcoming Sprint, aiming to enhance their processes and practices to make their work more effective and enjoyable.

Through this formal opportunity, the Scrum Team is encouraged to focus on both inspecting their practices and adapting them accordingly, thereby fostering a culture of continuous improvement within the framework of Scrum [1].",
"[{'filename': '01-code-generation-libraries.md', 'text': ""### Reasons not to use it  \n- By default, the generated code by Lombok will not be seen or reviewed, even though it ends up on production. Developers have to trust in it. Luckily there are ways to `delombok` the annotations to inspect what it will generate. Also, it's fully [open-source](https://github.com/rzwitserloot/lombok)!\n- Even though in most cases, Lombok will work all fine. Sometimes - e.g. in some version, putting certain annotations on inner classes - it can mess up code generation, which might be hard to debug.\n- As the Java language is advancing, some features from Lombok will become obsolete. It might even interfere with new language keywords (like `var`, for example).\n- Some projects prefer to include [Kotlin](../../../02-languages/kotlin/) as language next to Java for reducing boilerplate. Kotlin features a lot which Lombok also offers, but does that in another language. Please be aware of the fact that interop between Kotlin and Java code with Lombok [does not work well](https://stackoverflow.com/a/35530223)."", 'vector_id': 1344, 'distance': np.float32(0.34817648), 'score': np.float32(0.98035324)}, {'filename': '10-fundamentals.md', 'text': '## The programming language Java  \nJava is the default language of the Java Virtual Machine. Oracle is the current maintainer of Java.\nOne of the big benefits of the Java language is that they try to ensure stability over releases as much as possible.\nThis means that it is - in general - not painful to upgrade to newer Java versions.  \n#### Drawbacks  \n- A drawback of Java is that it is **evolving slower than other languages**, and as a result does not contain certain language features alternatives have.\n- Logic expressed in the Java language often results in [bloated code](https://en.wikipedia.org/wiki/Code_bloat). Code generation libraries like [Lombok](../../03-frameworks/02-java/03-libraries/01-code-generation-libraries.md) can improve the situation.', 'vector_id': 1120, 'distance': np.float32(0.7770774), 'score': np.float32(0.9489914)}, {'filename': '01-code-generation-libraries.md', 'text': ""### Advice  \n> **Guideline:** Use Lombok in Java applications.\n> **Why:** Lombok reduces the amount of boilerplate code and reduces bugs while doing so. The learning steep is not steep, and it's a mature project."", 'vector_id': 1345, 'distance': np.float32(0.3413188), 'score': np.float32(0.90613174)}]",What are some potential drawbacks of using Lombok in Java development?,1,[0.21397526 0.6817709  0.10425384],"Some potential drawbacks of using Lombok in Java development include:

1. **Lack of Visibility:** The generated code by Lombok is not visible or reviewed by developers, which means they have to trust the library for production code. While there are methods to `delombok` and inspect the generated code, it adds an additional step.

2. **Debugging Challenges:** Lombok can sometimes cause issues with code generation, especially in specific scenarios (like using certain annotations on inner classes), which may be difficult to debug.

3. **Compatibility with Java Evolution:** As the Java language evolves, certain features provided by Lombok may become obsolete, and they could potentially interfere with new language keywords, such as `var`.

4. **Interop Issues with Kotlin:** If a project incorporates Kotlin alongside Java, it's important to note that interoperability with Lombok may be problematic. While both Kotlin and Lombok aim to reduce boilerplate code, they do so in different ways, which can lead to compatibility issues.

These drawbacks should be weighed against the benefits of using Lombok, such as reduced boilerplate code and fewer bugs.",What are some advantages of using Lombok in Java applications?
"[{'filename': 'azure-managed-databases.md', 'text': '## Graph database  \nFor graph databases there is a single managed offering in Azure.', 'vector_id': 1416, 'distance': np.float32(0.81794834), 'score': np.float32(0.9974981)}, {'filename': 'azure-managed-databases.md', 'text': '### Azure Cosmos DB (Gremlin API)  \nWith the [Gremlin API in Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction) you can create and query large graphs.  \n> **Guidance:** Use **Azure Cosmos DB with the Gremlin API** for graph databases.\n> **Why:** This is the only managed graph database offered in Azure.', 'vector_id': 1417, 'distance': np.float32(1.5836958), 'score': np.float32(0.995131)}, {'filename': 'azure-managed-databases.md', 'text': '### Alternative graph database  \nIf _Cosmos DB_ is too pricy, it is an option to host a Neo4J environment with virtual machines or containers. For enterprise features, additional licenses might be required.  \n- [Neo4J](https://neo4j.com/)', 'vector_id': 1418, 'distance': np.float32(0.97804916), 'score': np.float32(0.92318153)}]",Which database should I use for graphs?,1,[0.03603467 0.53146159 0.43250373],"You should use **Azure Cosmos DB with the Gremlin API** for graph databases, as it is the only managed graph database offering in Azure. If Azure Cosmos DB is too expensive, you could consider hosting a Neo4J environment with virtual machines or containers, although this might require additional licensing for enterprise features.",What is the recommended managed offering for graph databases in Azure?
"[{'filename': 'ADR-012 Logging Strategy for ETL Pipelines with Log Analytics.md', 'text': '## Context and Problem Statement  \nAs we develop and deploy ETL pipelines, we need to establish a logging strategy to monitor pipeline execution, track data transformations, and capture errors or anomalies. We must decide how to set up logging for our ETL pipelines effectively. Log Analytics, a service provided by cloud platforms like Azure, offers a centralized logging solution with advanced querying and analytics capabilities. However, we need to evaluate whether Log Analytics is the most suitable option for our logging requirements.', 'vector_id': 331, 'distance': np.float32(0.8146723), 'score': np.float32(0.97305864)}, {'filename': '03-data-processing.md', 'text': ""## Orchestrating ETL jobs  \nWe recommend using pipelines to combine jobs together into a cohesive data pipeline.\nSynapse, for example, provides excellent ways to define a pipeline in which you can\ncombine Spark jobs with other tasks to process data.  \nKeep the Spark jobs compact so it's easier to test them in your local development\nenvironment."", 'vector_id': 1465, 'distance': np.float32(0.83503234), 'score': np.float32(0.961007)}, {'filename': '05-devops-tools-and-practices.md', 'text': ""## Data pipeline orchestration  \nMany parts of your data solution will rely on ETL processes that are executed\nwithin the data platform. This can lead to a chaotic situation where it's no\nlonger clear which parts interact. Having a good orchestration tool is essential.  \nAlongside a good orchestration tool it's important to understand how to setup a\ndata pipeline when components are contributed from different teams.  \n![Data pipeline](../images/data-pipeline.jpg)  \n- Explain how you can setup source control to combine different data processing\ncomponents into a coherent data pipeline.\n- Explain how to handle the fact that we have data and code moving in a data pipeline.\nEspecially when it comes to testing and deploying changes."", 'vector_id': 1517, 'distance': np.float32(0.8661636), 'score': np.float32(0.9346262)}]",what is an ETL pipeline?,1,[0.03188229 0.52501003 0.44310768],"An ETL pipeline is a data processing framework that involves three main stages: Extract, Transform, and Load. 

1. **Extract**: This stage involves gathering data from various sources, which can include databases, APIs, flat files, or other data storage systems.

2. **Transform**: In this stage, the extracted data is processed and transformed into a suitable format for analysis. This can involve cleansing the data, applying business rules, aggregating information, or converting data types.

3. **Load**: Finally, the transformed data is loaded into a target destination, such as a data warehouse, database, or data lake, where it can be used for reporting and analysis.

ETL pipelines are essential for integrating data from diverse sources, ensuring data quality, and enabling effective data management practices. They are commonly used in data warehousing and analytics initiatives.",How can we effectively monitor the execution of ETL pipelines?
"[{'filename': 'ethics-data.md', 'text': '# Ethics  \n## Definition  \nRespect for human autonomy, prevention of harm, fairness and explicability  \nEnsure that the development, deployment and use of AI systems meets the seven key requirements for\nTrustworthy AI: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data\ngovernance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societal\nwell-being and (7) accountability.  \nThis [framework](https://web.archive.org/web/20210604003015/https://research.ku.edu/sites/research.ku.edu/files/docs/EESE_EthicalDecisionmakingFramework.pdf) can be used as reference.', 'vector_id': 731, 'distance': np.float32(1.0964043), 'score': np.float32(0.12133954)}, {'filename': '05-ethical-ai-principles.md', 'text': ""## 7 key requirements for trustworthy AI  \nFor each requirement, we've summed up the relevant guidance to satisfy or partially satisfy the questions from the [Assessment List for Trustworthy AI][ASSESSMENT_TRUSTWORTHY_AI]. The assessment is best completed involving a multidisciplinary team of people. These could be from within and/or outside your organization with specific competencies or expertise on each of the 7 requirements and related questions. Among the stakeholders you may find for example the following:  \n- AI designers and AI developers of the AI system;\n- data scientists;\n- procurement officers or specialists;\n- front-end staff that will use or work with the AI system;\n- legal/compliance officers;\n- management.  \nThere's an [online assessment tool][ASSESSMENT_TRUSTWORTHY_AI_ONLINE] you can use."", 'vector_id': 1891, 'distance': np.float32(0.95993495), 'score': np.float32(0.080656834)}, {'filename': 'usage-data.md', 'text': '# Usage  \n## Definition  \nApplicable for AI solutions, otherwise this is fully covered by WOW: Customer, use the system.  \nFor AI solutions, focus on how the solution is used in practice and if the solution is fit for purpose.', 'vector_id': 890, 'distance': np.float32(1.1661237), 'score': np.float32(0.033218216)}]",List three use cases for AI in the financial sector.,2,[0.03386008 0.40228941 0.56385051],"Here are three use cases for AI in the financial sector:

1. **Fraud Detection and Prevention**: AI algorithms analyze transaction patterns and user behavior to identify anomalies that may indicate fraudulent activity. By monitoring vast amounts of data in real-time, AI can help financial institutions detect and mitigate fraud more effectively than traditional methods.

2. **Credit Risk Assessment**: AI can assist in evaluating the creditworthiness of individuals or businesses by analyzing a wide range of data sources, including credit scores, transaction histories, and even social media activity. This allows for more accurate risk assessments and can help expand access to credit for underserved populations.

3. **Customer Service and Chatbots**: Financial institutions utilize AI-powered chatbots and virtual assistants to enhance customer service. These AI systems can handle a variety of customer inquiries, provide account information, and assist with transactions 24/7, improving customer satisfaction while reducing operational costs.",
"[{'filename': 'index.md', 'text': '### Structuring your GitOps repository  \nWhen working with GitOps, the Git repository structure is of great importance. It determines how developers commit changes and how these changes propegate through your [DTAP](https://en.wikipedia.org/wiki/Development,_testing,_acceptance_and_production) environments.  \nAn overview of a recommended GitOps structure can be found in the [GitOps repository structure](./gitops-repository-structure.md) article.', 'vector_id': 933, 'distance': np.float32(1.1786672), 'score': np.float32(0.039994836)}, {'filename': '01-diagnostics.md', 'text': ""## Tracking metrics while training models  \nWhen training a machine learning model it's important to record information for\nreproducing the model in the future. We require reproducable runs as part of our\nethical and safe AI efforts.  \n> **Guideline:** Track performance metrics as part of the training run.\n> **Why:** It makes models comparable across various training runs  \n> **Guideline:** Track hyperparameters as part of the training run.\n> **Why:** It makes it possible to train the model again in the future.  \n> **Guideline:** Track the GIT commit as part of the training run.\n> **Why:** It's needed to retrain the model if needed.  \n> **Guideline:** Track the dataset version as part of the training run.\n> **Why:** It's needed to retrain the model if needed.  \n> **Guideline:** Track the model that's produced during the training run.\n> **Why:** It's required for auditing purposes.  \nTracking various pieces of information as part of the training run is also\nimportant in case we need to audit the model.  \nWe recommend using [MLFlow][MLFLOW] or [Azure Machine Learning Service][AZUREML]\nto track training runs."", 'vector_id': 1979, 'distance': np.float32(1.1915478), 'score': np.float32(0.029546447)}, {'filename': 'index.md', 'text': '## Why would you want to store secrets in Git  \nTo follow the [GitOps workflow](../../../../03-architecture-guidance/06-architecture-patterns/gitops/index.md).\nGitOps repositories contain the manifests describing the desired state of environments.\nSealed Secrets offers a safe mechanism for storing secrets in Git.\nThis has multiple benefits over a separate secret-management system, which include:  \n- **Versioning**: Both Kubernetes Secrets and external vaults have some support of versioning, but often work very different from version control systems that are already used in the company. Developers need to understand both ways of versioning and make it work together somehow.\n- **Independence**: Your application is not one contained package of deployment once external secrets are referenced from within. If the secret your application uses has been deleted in your Vault, the package that worked yesterday, doesn’t work anymore today.\n- **One source**: There is just one place where all of your deployment manifests are stored. This makes deployments easier to understand and to execute.\n- **Process**: Application development and secret management are two separate processes often done by separate parties. A developer is not always able to manage secrets himself, so it’s possible that he deploys the application to production, while the required secret is not available on production yet.', 'vector_id': 1875, 'distance': np.float32(1.3226745), 'score': np.float32(0.011177104)}]",What information should I include in a git commit message?,2,[0.02791766 0.35540117 0.61668117],"A good Git commit message should include the following information:

1. **Brief Summary**: A short summary of the changes made in the commit (usually around 50 characters). This should clearly state what has been changed.

2. **Detailed Explanation**: If necessary, provide a more detailed description of the changes in the body of the commit message. This should explain the ""why"" behind the changes, any relevant context, and any impact the changes may have.

3. **Reference Issues or Bugs**: If the commit relates to a specific issue or bug, reference it in the message to provide context.

4. **Performance Metrics (if applicable)**: If you're working in machine learning or a related field, include performance metrics tracked during the training run if the commit relates to model changes.

5. **Hyperparameters and Versioning**: Mention any significant changes to hyperparameters or versioning (if applicable) if the commit involves training a model.

6. **Tracking Information**: Include commitments of dataset version, Git commit, or specific model information if necessary for reproduction or auditing purposes.

By including these elements, your commit messages will be more informative and helpful for you and other developers in the future.",
"[{'filename': '60-data-analysis-and-feature-engineering.md', 'text': ""### Exploratory data analysis  \nWhenever you get a dataset from the customer, it's good to start with\nexploratory data analysis (EDA). The EDA technique uses visuals and benchmarking\ntechniques to gain insight into the dataset's structure.  \nYou can learn more about exploratory data analysis in Python in\n[this article][EDA_PYTHON]  \nYou can use various tools to visualize your data. We recommend using the\nfollowing tools:  \n- [Pyplot][PYPLOT]\n- [Seaborn][SEABORN]\n- [Plotly][PLOTLY]  \n> **Guideline:** Prefer to use notebooks to perform exploratory data analysis.\n> **Why:** Python notebooks provide an excellent interface to render plots for\n> your data analysis. The generated plots are saved with the notebooks, so you\n> don't have to rerun the code to see the outcome of the data analysis you\n> performed.  \n> **Guideline:** Use graphics and plots to visualize the dataset.\n> **Why:** Charts, diagrams, and other visualizations help the customer and\n> your team members understand the data better. Sometimes tables help,\n> but those can be hard to interpret in a glance.  \nYou can use [Pandas Profiling][PANDAS_PROFILING] as tool for getting an overview\nof your dataset quickly. It saves quite a lot of time. However, you sometimes\nhave to perform some additional analysis yourself."", 'vector_id': 1948, 'distance': np.float32(1.297717), 'score': np.float32(0.0038135522)}, {'filename': 'gremlin-api.md', 'text': ""### Partitioning  \nCosmos DB is a database that uses partitions to scale the database.\nThese guidelines help you design the graph across one or more partitions\nso it's fast and cheap to use.  \n> **Guideline:** Use a single partition for small graphs\n> **Why:** When working with a small graph it's cheaper to use a single\n> partition as querying is cheaper. Also, you don't have to think about\n> selecting a partition key, which makes the design simpler.  \n> **Guideline:** Use a partition key that is evenly distributed if you can in very large graphs\n> **Why:** Because the partition key values are evenly distributed, the data will be as well.\n> This improves scalability of the database over the long run as each partition is about the same size.\n> Because the partitions are equal in size, the query operators finish at about the same time as well.  \n> **Guideline:** Use a partition per tenant in a multi-tenancy scenario\n> **Why:** Data is automatically separated per tenant, and since you don't have\n> to query across partitions, this a cheap and fast solution.  \n> **Guideline:** Use a partition key that logically groups data based\n> on your querying needs.\n> **Why:** If you're designing a larger graph, you will want to select a\n> partition key that logically groups data together based on how you query the\n> graph.  \n> **Guideline:** Optimize relationships so you can query in the outgoing direction.\n> **Why:** Every outgoing edge is stored in the same partition as the source vertex.\n> Querying the outgoing edge is faster and cheaper. Incoming edge queries are always\n> executed as a fan-out operation."", 'vector_id': 1429, 'distance': np.float32(1.5897806), 'score': np.float32(0.00033345103)}, {'filename': '05-visualizing-alerting.md', 'text': '# Visualizing and alerting  \nWhen both the [Four Golden Signals](01-application-monitoring.md) and [USE methods](03-infrastructure-monitoring.md) are applied to monitor the infrastructure and application stack, it is possible to visualize this cohesively.  \nDashboarding and alerting are an essential part of informing development teams and operators to unhealthy deviations in metrics of the system so that an appropriate action can be taken.  \nWith regard to dashboarding, both direct numerical insights (current state) trends of these numerical insights over time (graphs) are required.  \nThe chosen monitoring topology should easily provide insight into these metrics and trends per application, information system or service.  \nIn the field of alerting, it is important to receive notifications when there is a deviation (above-standard or below-standard deviation) from configured threshold.', 'vector_id': 1674, 'distance': np.float32(1.4400303), 'score': np.float32(0.0003198296)}]",Which plot should I use to visualize six distributions on the same scale?,2,[0.01629234 0.27718508 0.70652258],"To visualize six distributions on the same scale, you should consider using a **combined box plot** or a **violin plot**. 

- **Box Plot**: A box plot can show the median, quartiles, and potential outliers of each distribution, allowing you to easily compare the distributions side by side on the same scale.

- **Violin Plot**: A violin plot not only displays the median and quartiles but also shows the density of the distributions, giving a more comprehensive view of the distribution shapes.

Both types of plots can effectively display multiple distributions together, making it easy to compare them visually. You can create these plots using libraries like Seaborn or Matplotlib's Pyplot in Python, as suggested in the context provided.",
